{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c033961d-e588-4913-930c-a467f746985b",
   "metadata": {},
   "source": [
    "# Explain what does PyTorch 2.0 do to your code\n",
    "\n",
    "PyTorch 2.0 looks like magic to many researchers, as they dynamically translate the bytecode for you. Many people don't know Python bytecode, so they don't know how do the translated code look like. Fortunately, with `depyf`, we can clearly illustrate it for you.\n",
    "\n",
    "# Runtime requirements\n",
    "\n",
    "Please use **Jupyter Lab**, not **Jupyter Notebook**.\n",
    "\n",
    "If you didn't install the following dependencies, please **restart the jupyter lab server after installing these dependencies.**\n",
    "\n",
    "If you have already installed them, skip the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f397e05-1857-4dad-8950-5369a7c5365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies and enable widgets for display\n",
    "!pip install ipywidgets\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "!jupyter labextension enable widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66dc808-699f-4db6-be39-103a504103b4",
   "metadata": {},
   "source": [
    "# Run some code with `torch.compile`, and use `eager` backend\n",
    "\n",
    "Note that we use `eager` backend so that the compiled subgraph runs in eager mode, and then we can easily get the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a95337-ef1a-40be-9d8b-2ba1a5e7cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "@torch.compile(backend=\"eager\")\n",
    "def toy_example(a, b):\n",
    "    x = a / (torch.abs(a) + 1)\n",
    "    __temp_2 = b.sum()\n",
    "    if __temp_2 < 0:\n",
    "        b = b * -1\n",
    "    return x * b\n",
    "\n",
    "for _ in range(100):\n",
    "    toy_example(torch.randn(10), torch.randn(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4ea8e-6dfa-434d-b020-bbba8d65dcbb",
   "metadata": {},
   "source": [
    "# Import the library for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451390c8-3a96-4c3f-9b0a-024f739f0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run lib_explain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aedd01-78ba-4393-8b26-79a9ba210796",
   "metadata": {},
   "source": [
    "# Explain everything you are curious about, with the magical `explain` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd1080f-6852-43d4-972a-0aa26fb0e1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# transformed source code:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "compiled_code_entries": [
        {
         "compiled_code": "cache_code:0",
         "compiled_subgraph": "__compiled_fn_0",
         "guard": [
          "___guarded_code.valid",
          "___check_global_state()",
          "hasattr(L['a'], '_dynamo_dynamic_indices') == False",
          "hasattr(L['b'], '_dynamo_dynamic_indices') == False",
          "utils_device.CURRENT_DEVICE == None",
          "___skip_backend_check() or ___current_backend() == ___lookup_backend(6093935504)",
          "___check_tensors(L['a'], L['b'], tensor_check_names=tensor_check_names)"
         ],
         "referenced_global_functions": {
          "__resume_at_34_1": {
           "compiled_code_entries": [
            {
             "compiled_code": "cache_code:1",
             "compiled_subgraph": "__compiled_fn_4",
             "guard": [
              "___guarded_code.valid",
              "___check_global_state()",
              "hasattr(L['b'], '_dynamo_dynamic_indices') == False",
              "hasattr(L['x'], '_dynamo_dynamic_indices') == False",
              "utils_device.CURRENT_DEVICE == None",
              "___skip_backend_check() or ___current_backend() == ___lookup_backend(6093935504)",
              "___check_tensors(L['b'], L['x'], tensor_check_names=tensor_check_names)"
             ],
             "referenced_global_functions": {}
            }
           ],
           "name": "__resume_at_34_1",
           "source_code": "__resume_at_34_1"
          },
          "__resume_at_42_2": {
           "compiled_code_entries": [
            {
             "compiled_code": "cache_code:2",
             "compiled_subgraph": "__compiled_fn_3",
             "guard": [
              "___guarded_code.valid",
              "___check_global_state()",
              "hasattr(L['b'], '_dynamo_dynamic_indices') == False",
              "hasattr(L['x'], '_dynamo_dynamic_indices') == False",
              "utils_device.CURRENT_DEVICE == None",
              "___skip_backend_check() or ___current_backend() == ___lookup_backend(6093935504)",
              "___check_tensors(L['b'], L['x'], tensor_check_names=tensor_check_names)"
             ],
             "referenced_global_functions": {}
            }
           ],
           "name": "__resume_at_42_2",
           "source_code": "__resume_at_42_2"
          }
         }
        }
       ],
       "name": "toy_example",
       "source_code": "toy_example"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# source code of referenced function:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<details>\n",
       "  <summary>__resume_at_34_1</summary>\n",
       "\n",
       "  ```python\n",
       "  def <resume in toy_example>(b, x):\n",
       "      b = b * -1\n",
       "      return x * b\n",
       "\n",
       "  ```\n",
       "</details>\n",
       "<details>\n",
       "  <summary>__compiled_fn_3</summary>\n",
       "\n",
       "  ```python\n",
       "  def forward(self, L_b_ : torch.Tensor, L_x_ : torch.Tensor):\n",
       "      l_b_ = L_b_\n",
       "      l_x_ = L_x_\n",
       "      mul = l_x_ * l_b_;  l_x_ = l_b_ = None\n",
       "      return (mul,)\n",
       "\n",
       "  ```\n",
       "</details>\n",
       "<details>\n",
       "  <summary>__compiled_fn_0</summary>\n",
       "\n",
       "  ```python\n",
       "  def forward(self, L_a_ : torch.Tensor, L_b_ : torch.Tensor):\n",
       "      l_a_ = L_a_\n",
       "      l_b_ = L_b_\n",
       "      abs_1 = torch.abs(l_a_)\n",
       "      add = abs_1 + 1;  abs_1 = None\n",
       "      x = l_a_ / add;  l_a_ = add = None\n",
       "      __temp_2 = l_b_.sum();  l_b_ = None\n",
       "      lt = __temp_2 < 0;  __temp_2 = None\n",
       "      return (x, lt)\n",
       "\n",
       "  ```\n",
       "</details>\n",
       "<details>\n",
       "  <summary>cache_code:0</summary>\n",
       "\n",
       "  ```python\n",
       "  def toy_example(a, b):\n",
       "      __temp_22 = __compiled_fn_0(a, b)\n",
       "      x = __temp_22[0]\n",
       "      if __temp_22[1]:\n",
       "          return __resume_at_34_1(b, x)\n",
       "      return __resume_at_42_2(b, x)\n",
       "\n",
       "  ```\n",
       "</details>\n",
       "<details>\n",
       "  <summary>cache_code:2</summary>\n",
       "\n",
       "  ```python\n",
       "  def <resume in toy_example>(b, x):\n",
       "      return __compiled_fn_3(b, x)[0]\n",
       "\n",
       "  ```\n",
       "</details>\n",
       "<details>\n",
       "  <summary>toy_example</summary>\n",
       "\n",
       "  ```python\n",
       "  def toy_example(a, b):\n",
       "      x = a / (torch.abs(a) + 1)\n",
       "      if b.sum() < 0:\n",
       "          b = b * -1\n",
       "      return x * b\n",
       "\n",
       "  ```\n",
       "</details>\n",
       "<details>\n",
       "  <summary>cache_code:1</summary>\n",
       "\n",
       "  ```python\n",
       "  def <resume in toy_example>(b, x):\n",
       "      return __compiled_fn_4(b, x)[0]\n",
       "\n",
       "  ```\n",
       "</details>\n",
       "<details>\n",
       "  <summary>__resume_at_42_2</summary>\n",
       "\n",
       "  ```python\n",
       "  def <resume in toy_example>(b, x):\n",
       "      return x * b\n",
       "\n",
       "  ```\n",
       "</details>\n",
       "<details>\n",
       "  <summary>__compiled_fn_4</summary>\n",
       "\n",
       "  ```python\n",
       "  def forward(self, L_b_ : torch.Tensor, L_x_ : torch.Tensor):\n",
       "      l_b_ = L_b_\n",
       "      l_x_ = L_x_\n",
       "      b = l_b_ * -1;  l_b_ = None\n",
       "      mul_1 = l_x_ * b;  l_x_ = b = None\n",
       "      return (mul_1,)\n",
       "\n",
       "  ```\n",
       "</details>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explain(toy_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de1fd6-6fd4-4d0f-a27f-b6bb87761b50",
   "metadata": {},
   "source": [
    "# Going further, we can directly explain referenced functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eecd046a-6671-4ea3-ad54-7836193e861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# transformed source code:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "compiled_code_entries": [
        {
         "compiled_code": "cache_code:3",
         "compiled_subgraph": "__compiled_fn_3",
         "guard": [
          "___guarded_code.valid",
          "___check_global_state()",
          "hasattr(L['b'], '_dynamo_dynamic_indices') == False",
          "hasattr(L['x'], '_dynamo_dynamic_indices') == False",
          "utils_device.CURRENT_DEVICE == None",
          "___skip_backend_check() or ___current_backend() == ___lookup_backend(6093935504)",
          "___check_tensors(L['b'], L['x'], tensor_check_names=tensor_check_names)"
         ],
         "referenced_global_functions": {}
        }
       ],
       "name": "__resume_at_42_2",
       "source_code": "__resume_at_42_2"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# source code of referenced function:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<details>\n",
       "  <summary>__compiled_fn_3</summary>\n",
       "\n",
       "  ```python\n",
       "  def forward(self, L_b_ : torch.Tensor, L_x_ : torch.Tensor):\n",
       "      l_b_ = L_b_\n",
       "      l_x_ = L_x_\n",
       "      mul = l_x_ * l_b_;  l_x_ = l_b_ = None\n",
       "      return (mul,)\n",
       "\n",
       "  ```\n",
       "</details>\n",
       "<details>\n",
       "  <summary>cache_code:3</summary>\n",
       "\n",
       "  ```python\n",
       "  def <resume in toy_example>(b, x):\n",
       "      return __compiled_fn_3(b, x)[0]\n",
       "\n",
       "  ```\n",
       "</details>\n",
       "<details>\n",
       "  <summary>__resume_at_42_2</summary>\n",
       "\n",
       "  ```python\n",
       "  def <resume in toy_example>(b, x):\n",
       "      return x * b\n",
       "\n",
       "  ```\n",
       "</details>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explain(__resume_at_42_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3958aa8-6763-440a-bab8-378132973e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# transformed source code:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "compiled_code_entries": [],
       "name": "forward",
       "source_code": "forward"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# source code of referenced function:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<details>\n",
       "  <summary>forward</summary>\n",
       "\n",
       "  ```python\n",
       "  def forward(self, L_b_, L_x_):\n",
       "      l_b_ = L_b_\n",
       "      l_x_ = L_x_\n",
       "      b = l_b_ * -1\n",
       "      l_b_ = None\n",
       "      mul_1 = l_x_ * b\n",
       "      l_x_ = None\n",
       "      b = None\n",
       "      return mul_1,\n",
       "\n",
       "  ```\n",
       "</details>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explain(__compiled_fn_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c8499-4ce0-49f0-85c0-eb2c75ab96a3",
   "metadata": {},
   "source": [
    "# General notice\n",
    "\n",
    "If you find `compiled_code_entries` is not empty for a function, it probably means PyTorch does not execute its source code. PyTorch will tries to examine the code entries, and execute the code whose guarding conditions are satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5596f7b-c550-41fd-9c29-e9dc66de88bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
