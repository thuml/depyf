["MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             14 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             2\n             18 STORE_FAST             634 (graph_out_0)\n             20 EXTENDED_ARG             2\n             22 LOAD_FAST              634 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.MaskedLMOutput'>)\n             32 EXTENDED_ARG             2\n             34 LOAD_FAST              634 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             2\n             42 LOAD_FAST              634 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 LOAD_CONST               0 (None)\n             50 LOAD_CONST               0 (None)\n             52 LOAD_CONST               7 (('loss', 'logits', 'hidden_states', 'attentions'))\n             54 CALL_FUNCTION_KW         4\n             56 EXTENDED_ARG             2\n             58 LOAD_FAST              634 (graph_out_0)\n             60 LOAD_CONST               4 (0)\n             62 BINARY_SUBSCR\n             64 STORE_FAST               7 (loss)\n             66 STORE_FAST               6 (pred)\n\n559          68 CALL_FUNCTION            0\n             70 LOAD_GLOBAL             15 (__resume_at_100_4)\n             72 ROT_TWO\n             74 LOAD_FAST                1 (self)\n             76 LOAD_FAST                2 (mod)\n             78 LOAD_FAST                3 (collect_outputs)\n             80 LOAD_FAST                4 (cloned_inputs)\n             82 LOAD_FAST                6 (pred)\n             84 LOAD_FAST                7 (loss)\n             86 CALL_FUNCTION            7\n             88 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             14 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('start_positions')\n             12 BINARY_SUBSCR\n             14 LOAD_FAST                4 (cloned_inputs)\n             16 LOAD_CONST               4 ('end_positions')\n             18 BINARY_SUBSCR\n             20 CALL_FUNCTION            3\n             22 EXTENDED_ARG             2\n             24 STORE_FAST             638 (graph_out_0)\n             26 EXTENDED_ARG             2\n             28 LOAD_FAST              638 (graph_out_0)\n             30 LOAD_CONST               5 (0)\n             32 BINARY_SUBSCR\n             34 LOAD_ATTR                6 (backward)\n             36 LOAD_CONST               6 (<class 'transformers.modeling_outputs.QuestionAnsweringModelOutput'>)\n             38 EXTENDED_ARG             2\n             40 LOAD_FAST              638 (graph_out_0)\n             42 LOAD_CONST               5 (0)\n             44 BINARY_SUBSCR\n             46 EXTENDED_ARG             2\n             48 LOAD_FAST              638 (graph_out_0)\n             50 LOAD_CONST               7 (1)\n             52 BINARY_SUBSCR\n             54 EXTENDED_ARG             2\n             56 LOAD_FAST              638 (graph_out_0)\n             58 LOAD_CONST               8 (2)\n             60 BINARY_SUBSCR\n             62 LOAD_CONST               0 (None)\n             64 LOAD_CONST               0 (None)\n             66 LOAD_CONST               9 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n             68 CALL_FUNCTION_KW         5\n             70 EXTENDED_ARG             2\n             72 LOAD_FAST              638 (graph_out_0)\n             74 LOAD_CONST               5 (0)\n             76 BINARY_SUBSCR\n             78 STORE_FAST               7 (loss)\n             80 STORE_FAST               6 (pred)\n\n559          82 CALL_FUNCTION            0\n             84 LOAD_GLOBAL             15 (__resume_at_100_4)\n             86 ROT_TWO\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 LOAD_FAST                6 (pred)\n             98 LOAD_FAST                7 (loss)\n            100 CALL_FUNCTION            7\n            102 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             12 (__import_contextlib)\n             28 LOAD_ATTR               13 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              53 (___context_manager_0_3)\n             34 LOAD_FAST               53 (___context_manager_0_3)\n             36 LOAD_METHOD             14 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               53 (___context_manager_0_3)\n             50 LOAD_METHOD             15 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               53 (___context_manager_0_3)\n             68 LOAD_METHOD             15 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             16 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py line 1789 \n1789           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                2 (longformer)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                3 (global_attention_mask)\n              10 LOAD_FAST                4 (head_mask)\n              12 LOAD_FAST                5 (token_type_ids)\n              14 LOAD_FAST                6 (position_ids)\n              16 LOAD_FAST                7 (inputs_embeds)\n              18 LOAD_FAST                9 (output_attentions)\n              20 LOAD_FAST               10 (output_hidden_states)\n              22 LOAD_FAST                0 (self)\n              24 LOAD_ATTR                0 (config)\n              26 LOAD_ATTR                1 (use_return_dict)\n              28 LOAD_CONST               2 (('attention_mask', 'global_attention_mask', 'head_mask', 'token_type_ids', 'position_ids', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              30 LOAD_FAST                0 (self)\n              32 LOAD_ATTR                0 (config)\n              34 LOAD_ATTR                1 (use_return_dict)\n              36 STORE_FAST              11 (return_dict)\n\n1843          38 CALL_FUNCTION_KW        10\n              40 LOAD_GLOBAL             15 (__resume_at_48_5)\n              42 ROT_TWO\n              44 LOAD_FAST                0 (self)\n              46 LOAD_FAST                8 (labels)\n              48 LOAD_FAST               11 (return_dict)\n              50 CALL_FUNCTION            4\n              52 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py line 1647 \n1647           0 LOAD_GLOBAL             24 (__compiled_fn_6)\n               2 LOAD_FAST                1 (input_ids)\n               4 CALL_FUNCTION            1\n               6 STORE_FAST              60 (graph_out_0)\n               8 LOAD_FAST                0 (self)\n              10 LOAD_ATTR               16 (encoder)\n              12 LOAD_FAST               60 (graph_out_0)\n              14 LOAD_CONST               8 (0)\n              16 BINARY_SUBSCR\n              18 LOAD_FAST               60 (graph_out_0)\n              20 LOAD_CONST              11 (1)\n              22 BINARY_SUBSCR\n              24 LOAD_FAST                4 (head_mask)\n              26 LOAD_CONST               8 (0)\n              28 LOAD_FAST                0 (self)\n              30 LOAD_ATTR                0 (config)\n              32 LOAD_ATTR                1 (output_attentions)\n              34 LOAD_FAST                0 (self)\n              36 LOAD_ATTR                0 (config)\n              38 LOAD_ATTR                2 (output_hidden_states)\n              40 LOAD_FAST               10 (return_dict)\n              42 LOAD_CONST              10 (('attention_mask', 'head_mask', 'padding_len', 'output_attentions', 'output_hidden_states', 'return_dict'))\n\n1746          44 CALL_FUNCTION_KW         7\n              46 LOAD_GLOBAL             25 (__resume_at_334_7)\n              48 ROT_TWO\n              50 LOAD_FAST                0 (self)\n              52 LOAD_FAST               10 (return_dict)\n              54 CALL_FUNCTION            3\n              56 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py line 1277 \n1277           0 LOAD_GLOBAL             16 (__compiled_fn_8)\n               2 LOAD_FAST                2 (attention_mask)\n               4 CALL_FUNCTION            1\n               6 STORE_FAST              19 (graph_out_0)\n               8 LOAD_FAST               19 (graph_out_0)\n              10 LOAD_CONST               1 (0)\n              12 BINARY_SUBSCR\n              14 LOAD_ATTR                2 (item)\n              16 LOAD_FAST               19 (graph_out_0)\n              18 LOAD_CONST               9 (1)\n              20 BINARY_SUBSCR\n              22 LOAD_FAST               19 (graph_out_0)\n              24 LOAD_CONST              10 (2)\n              26 BINARY_SUBSCR\n              28 STORE_FAST               9 (is_index_global_attn)\n              30 STORE_FAST               8 (is_index_masked)\n\n1291          32 CALL_FUNCTION            0\n              34 LOAD_CLOSURE             0 (is_global_attn)\n              36 LOAD_CLOSURE             1 (output_attentions)\n              38 LOAD_CLOSURE             2 (padding_len)\n              40 BUILD_TUPLE              3\n              42 LOAD_CONST              18 (<code object <resume in forward> at 0x7f192ab552c0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1291>)\n              44 LOAD_CONST              19 ('__resume_at_30_9')\n              46 MAKE_FUNCTION            8 (closure)\n              48 ROT_TWO\n              50 LOAD_FAST                0 (self)\n              52 LOAD_FAST                1 (hidden_states)\n              54 LOAD_FAST                2 (attention_mask)\n              56 LOAD_FAST                3 (head_mask)\n              58 LOAD_FAST                6 (output_hidden_states)\n              60 LOAD_FAST                7 (return_dict)\n              62 LOAD_FAST                8 (is_index_masked)\n              64 LOAD_FAST                9 (is_index_global_attn)\n              66 CALL_FUNCTION            9\n              68 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py line 1291 \n1291           0 LOAD_GLOBAL             18 (__compiled_fn_10)\n               2 LOAD_FAST                2 (hidden_states)\n               4 LOAD_FAST                3 (attention_mask)\n               6 LOAD_FAST                7 (is_index_masked)\n               8 CALL_FUNCTION            3\n              10 EXTENDED_ARG             6\n              12 STORE_FAST            1607 (graph_out_0)\n              14 LOAD_CONST              18 (<class 'transformers.models.longformer.modeling_longformer.LongformerBaseModelOutput'>)\n              16 EXTENDED_ARG             6\n              18 LOAD_FAST             1607 (graph_out_0)\n              20 LOAD_CONST               1 (0)\n              22 BINARY_SUBSCR\n              24 LOAD_CONST               0 (None)\n              26 LOAD_CONST               0 (None)\n              28 LOAD_CONST               0 (None)\n              30 LOAD_CONST              19 (('last_hidden_state', 'hidden_states', 'attentions', 'global_attentions'))\n              32 CALL_FUNCTION_KW         4\n              34 LOAD_FAST                0 (___stack0)\n              36 STORE_DEREF              0 (is_global_attn)\n              38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py line 1843 \n1843           0 LOAD_GLOBAL             14 (__compiled_fn_11)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               15 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              32 (graph_out_0)\n              12 LOAD_CONST               7 (<class 'transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput'>)\n              14 LOAD_FAST               32 (graph_out_0)\n              16 LOAD_CONST               3 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               32 (graph_out_0)\n              22 LOAD_CONST               8 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR               10 (hidden_states)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR               11 (attentions)\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               12 (global_attentions)\n              38 LOAD_CONST               9 (('loss', 'logits', 'hidden_states', 'attentions', 'global_attentions'))\n              40 CALL_FUNCTION_KW         5\n              42 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_12)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_13)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_14)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             12 (__import_contextlib)\n             28 LOAD_ATTR               13 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              49 (___context_manager_0_3)\n             34 LOAD_FAST               49 (___context_manager_0_3)\n             36 LOAD_METHOD             14 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               49 (___context_manager_0_3)\n             50 LOAD_METHOD             15 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               49 (___context_manager_0_3)\n             68 LOAD_METHOD             15 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             16 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 1775 \n1775           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                4 (model)\n               4 LOAD_ATTR                5 (decoder)\n               6 LOAD_FAST                1 (input_ids)\n               8 LOAD_FAST                2 (attention_mask)\n              10 LOAD_FAST                3 (encoder_hidden_states)\n              12 LOAD_FAST                4 (encoder_attention_mask)\n              14 LOAD_FAST                5 (head_mask)\n              16 LOAD_FAST                6 (cross_attn_head_mask)\n              18 LOAD_FAST                7 (past_key_values)\n              20 LOAD_FAST                8 (inputs_embeds)\n              22 LOAD_FAST               10 (use_cache)\n              24 LOAD_FAST                0 (self)\n              26 LOAD_ATTR                0 (config)\n              28 LOAD_ATTR                1 (output_attentions)\n              30 LOAD_FAST                0 (self)\n              32 LOAD_ATTR                0 (config)\n              34 LOAD_ATTR                2 (output_hidden_states)\n              36 LOAD_FAST                0 (self)\n              38 LOAD_ATTR                0 (config)\n              40 LOAD_ATTR                3 (use_return_dict)\n              42 LOAD_CONST               2 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                3 (use_return_dict)\n              50 STORE_FAST              13 (return_dict)\n\n1884          52 CALL_FUNCTION_KW        12\n              54 LOAD_GLOBAL             19 (__resume_at_94_5)\n              56 ROT_TWO\n              58 LOAD_FAST                0 (self)\n              60 LOAD_FAST                9 (labels)\n              62 LOAD_FAST               13 (return_dict)\n              64 CALL_FUNCTION            4\n              66 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 924 \n924           0 LOAD_GLOBAL              5 (__compiled_fn_6)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 131 \n131           0 LOAD_GLOBAL             11 (__compiled_fn_7)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_8)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_15)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_16)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_17)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_18)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_19)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_20)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              6 (__resume_at_14_21)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fcacc058450, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_22')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fcacbabd790, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_23')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fcacc058450, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_24')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fcacbabd790, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_25')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 1884 \n1884           0 LOAD_GLOBAL             17 (__compiled_fn_26)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               18 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              26 (graph_out_0)\n              12 LOAD_CONST               7 (<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>)\n              14 LOAD_FAST               26 (graph_out_0)\n              16 LOAD_CONST               3 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               26 (graph_out_0)\n              22 LOAD_CONST               5 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR               13 (past_key_values)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR               14 (hidden_states)\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               15 (attentions)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               16 (cross_attentions)\n              42 LOAD_CONST               8 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n              44 CALL_FUNCTION_KW         6\n              46 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_27)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_28)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_29)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             12 (__import_contextlib)\n             28 LOAD_ATTR               13 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              40 (___context_manager_0_3)\n             34 LOAD_FAST               40 (___context_manager_0_3)\n             36 LOAD_METHOD             14 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               40 (___context_manager_0_3)\n             50 LOAD_METHOD             15 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               40 (___context_manager_0_3)\n             68 LOAD_METHOD             15 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             16 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 1339 \n1339           0 LOAD_GLOBAL             25 (__compiled_fn_5)\n               2 LOAD_FAST               12 (labels)\n               4 CALL_FUNCTION            1\n               6 STORE_FAST              52 (graph_out_0)\n               8 LOAD_FAST                0 (self)\n              10 LOAD_ATTR                7 (model)\n              12 LOAD_FAST                1 (input_ids)\n              14 LOAD_FAST                2 (attention_mask)\n              16 LOAD_FAST               52 (graph_out_0)\n              18 LOAD_CONST               5 (0)\n              20 BINARY_SUBSCR\n              22 LOAD_FAST                8 (encoder_outputs)\n              24 LOAD_FAST                4 (decoder_attention_mask)\n              26 LOAD_FAST                5 (head_mask)\n              28 LOAD_FAST                6 (decoder_head_mask)\n              30 LOAD_FAST                7 (cross_attn_head_mask)\n              32 LOAD_FAST                9 (past_key_values)\n              34 LOAD_FAST               10 (inputs_embeds)\n              36 LOAD_FAST               11 (decoder_inputs_embeds)\n              38 LOAD_CONST               3 (False)\n              40 LOAD_FAST               14 (output_attentions)\n              42 LOAD_FAST               15 (output_hidden_states)\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                1 (use_return_dict)\n              50 LOAD_CONST               4 (('attention_mask', 'decoder_input_ids', 'encoder_outputs', 'decoder_attention_mask', 'head_mask', 'decoder_head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'decoder_inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              52 LOAD_FAST                0 (self)\n              54 LOAD_ATTR                0 (config)\n              56 LOAD_ATTR                1 (use_return_dict)\n              58 STORE_FAST              16 (return_dict)\n\n1380          60 CALL_FUNCTION_KW        15\n              62 LOAD_GLOBAL             26 (__resume_at_120_6)\n              64 ROT_TWO\n              66 LOAD_FAST                0 (self)\n              68 LOAD_FAST               12 (labels)\n              70 LOAD_FAST               16 (return_dict)\n              72 CALL_FUNCTION            4\n              74 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 1201 \n1201           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                9 (encoder)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                5 (head_mask)\n              10 LOAD_FAST               10 (inputs_embeds)\n              12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                2 (config)\n              16 LOAD_ATTR                5 (output_attentions)\n              18 LOAD_FAST                0 (self)\n              20 LOAD_ATTR                2 (config)\n              22 LOAD_ATTR                6 (output_hidden_states)\n              24 LOAD_FAST               15 (return_dict)\n              26 LOAD_CONST               2 (('input_ids', 'attention_mask', 'head_mask', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              28 LOAD_FAST                0 (self)\n              30 LOAD_ATTR                2 (config)\n              32 LOAD_ATTR                5 (output_attentions)\n              34 LOAD_FAST                0 (self)\n              36 LOAD_ATTR                2 (config)\n              38 LOAD_ATTR                6 (output_hidden_states)\n              40 STORE_FAST              14 (output_hidden_states)\n              42 STORE_FAST              13 (output_attentions)\n\n1248          44 CALL_FUNCTION_KW         7\n              46 LOAD_GLOBAL             22 (__resume_at_162_7)\n              48 ROT_TWO\n              50 LOAD_FAST                0 (self)\n              52 LOAD_FAST                2 (attention_mask)\n              54 LOAD_FAST                3 (decoder_input_ids)\n              56 LOAD_FAST                4 (decoder_attention_mask)\n              58 LOAD_FAST                6 (decoder_head_mask)\n              60 LOAD_FAST                7 (cross_attn_head_mask)\n              62 LOAD_FAST                9 (past_key_values)\n              64 LOAD_FAST               11 (decoder_inputs_embeds)\n              66 LOAD_FAST               12 (use_cache)\n              68 LOAD_FAST               13 (output_attentions)\n              70 LOAD_FAST               14 (output_hidden_states)\n              72 LOAD_FAST               15 (return_dict)\n              74 CALL_FUNCTION           13\n              76 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 131 \n131           0 LOAD_GLOBAL             11 (__compiled_fn_8)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 313 \n313           0 LOAD_GLOBAL             21 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 313 \n313           0 LOAD_GLOBAL             21 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 313 \n313           0 LOAD_GLOBAL             21 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 313 \n313           0 LOAD_GLOBAL             21 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 313 \n313           0 LOAD_GLOBAL             21 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 313 \n313           0 LOAD_GLOBAL             21 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 313 \n313           0 LOAD_GLOBAL             21 (__compiled_fn_15)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 313 \n313           0 LOAD_GLOBAL             21 (__compiled_fn_16)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 313 \n313           0 LOAD_GLOBAL             21 (__compiled_fn_17)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 313 \n313           0 LOAD_GLOBAL             21 (__compiled_fn_18)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 313 \n313           0 LOAD_GLOBAL             21 (__compiled_fn_19)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 313 \n313           0 LOAD_GLOBAL             21 (__compiled_fn_20)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              4 (__resume_at_6_21)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f426b0df520, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_22')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f426b0defa0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_23')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 1248 \n1248           0 LOAD_FAST                1 (self)\n               2 LOAD_ATTR               13 (decoder)\n               4 LOAD_FAST                3 (decoder_input_ids)\n               6 LOAD_FAST                4 (decoder_attention_mask)\n               8 LOAD_FAST                0 (___stack0)\n              10 LOAD_ATTR               15 (last_hidden_state)\n              12 LOAD_FAST                2 (attention_mask)\n              14 LOAD_FAST                5 (decoder_head_mask)\n              16 LOAD_FAST                6 (cross_attn_head_mask)\n              18 LOAD_FAST                7 (past_key_values)\n              20 LOAD_FAST                8 (decoder_inputs_embeds)\n              22 LOAD_FAST                9 (use_cache)\n              24 LOAD_FAST               10 (output_attentions)\n              26 LOAD_FAST               11 (output_hidden_states)\n              28 LOAD_FAST               12 (return_dict)\n              30 LOAD_CONST               7 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              32 LOAD_FAST                0 (___stack0)\n              34 STORE_FAST              15 (encoder_outputs)\n\n1266          36 CALL_FUNCTION_KW        12\n              38 LOAD_GLOBAL             22 (__resume_at_278_24)\n              40 ROT_TWO\n              42 LOAD_FAST               12 (return_dict)\n              44 LOAD_FAST               15 (encoder_outputs)\n              46 CALL_FUNCTION            3\n              48 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 924 \n924           0 LOAD_GLOBAL              5 (__compiled_fn_25)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 131 \n131           0 LOAD_GLOBAL             11 (__compiled_fn_26)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_27)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_28)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_29)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_30)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_31)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_32)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_33)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_34)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_35)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_36)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_37)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 391 \n391           0 LOAD_GLOBAL             14 (__compiled_fn_38)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_39)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f426b0df520, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_40')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f426b0defa0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_41')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              9 (__resume_at_6_42)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (decoder_hidden_states)\n             14 LOAD_FAST                4 (decoder_attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 LOAD_FAST                6 (encoder_last_hidden_state)\n             20 LOAD_FAST                7 (encoder_hidden_states)\n             22 LOAD_FAST                8 (encoder_attentions)\n             24 CALL_FUNCTION            8\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              9 (__resume_at_14_43)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (decoder_hidden_states)\n             12 LOAD_FAST                3 (decoder_attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 LOAD_FAST                5 (encoder_last_hidden_state)\n             18 LOAD_FAST                6 (encoder_hidden_states)\n             20 LOAD_FAST                7 (encoder_attentions)\n             22 CALL_FUNCTION            7\n             24 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 4 \n  4           0 LOAD_FAST                1 (decoder_hidden_states)\n              2 LOAD_FAST                0 (self)\n\n  5           4 STORE_ATTR               2 (decoder_hidden_states)\n              6 LOAD_GLOBAL              9 (__resume_at_20_44)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (decoder_attentions)\n             12 LOAD_FAST                3 (cross_attentions)\n             14 LOAD_FAST                4 (encoder_last_hidden_state)\n             16 LOAD_FAST                5 (encoder_hidden_states)\n             18 LOAD_FAST                6 (encoder_attentions)\n             20 CALL_FUNCTION            6\n             22 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 5 \n  5           0 LOAD_FAST                1 (decoder_attentions)\n              2 LOAD_FAST                0 (self)\n\n  6           4 STORE_ATTR               3 (decoder_attentions)\n              6 LOAD_GLOBAL              9 (__resume_at_26_45)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (cross_attentions)\n             12 LOAD_FAST                3 (encoder_last_hidden_state)\n             14 LOAD_FAST                4 (encoder_hidden_states)\n             16 LOAD_FAST                5 (encoder_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 6 \n  6           0 LOAD_FAST                1 (cross_attentions)\n              2 LOAD_FAST                0 (self)\n\n  7           4 STORE_ATTR               4 (cross_attentions)\n              6 LOAD_GLOBAL              9 (__resume_at_32_46)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (encoder_last_hidden_state)\n             12 LOAD_FAST                3 (encoder_hidden_states)\n             14 LOAD_FAST                4 (encoder_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 7 \n  7           0 LOAD_FAST                1 (encoder_last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  8           4 STORE_ATTR               5 (encoder_last_hidden_state)\n              6 LOAD_GLOBAL              9 (__resume_at_38_47)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (encoder_hidden_states)\n             12 LOAD_FAST                3 (encoder_attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f426b0df520, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_48')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f426b0defa0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_49')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (encoder_last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f426b0df520, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_50')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (encoder_last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f426b0defa0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_51')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py line 1380 \n1380           0 LOAD_GLOBAL             23 (__compiled_fn_52)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               24 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              30 (graph_out_0)\n              12 LOAD_CONST               9 (<class 'transformers.modeling_outputs.Seq2SeqLMOutput'>)\n              14 LOAD_FAST               30 (graph_out_0)\n              16 LOAD_CONST               5 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               30 (graph_out_0)\n              22 LOAD_CONST               7 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR               16 (past_key_values)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR               17 (decoder_hidden_states)\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               18 (decoder_attentions)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               19 (cross_attentions)\n              42 LOAD_FAST                0 (___stack0)\n              44 LOAD_ATTR               20 (encoder_last_hidden_state)\n              46 LOAD_FAST                0 (___stack0)\n              48 LOAD_ATTR               21 (encoder_hidden_states)\n              50 LOAD_FAST                0 (___stack0)\n              52 LOAD_ATTR               22 (encoder_attentions)\n              54 LOAD_CONST              10 (('loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'))\n              56 CALL_FUNCTION_KW         9\n              58 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_53)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_54)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_55)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             10 (__import_contextlib)\n              2 LOAD_ATTR               11 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             10 (__import_contextlib)\n             28 LOAD_ATTR               11 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              17 (___context_manager_0_3)\n             34 LOAD_FAST               17 (___context_manager_0_3)\n             36 LOAD_METHOD             12 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               17 (___context_manager_0_3)\n             50 LOAD_METHOD             13 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               17 (___context_manager_0_3)\n             68 LOAD_METHOD             13 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             14 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 1326 \n1326           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                2 (bert)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                3 (token_type_ids)\n              10 LOAD_FAST                4 (position_ids)\n              12 LOAD_FAST                5 (head_mask)\n              14 LOAD_FAST                6 (inputs_embeds)\n              16 LOAD_FAST                7 (encoder_hidden_states)\n              18 LOAD_FAST                8 (encoder_attention_mask)\n              20 LOAD_FAST               10 (output_attentions)\n              22 LOAD_FAST               11 (output_hidden_states)\n              24 LOAD_FAST                0 (self)\n              26 LOAD_ATTR                0 (config)\n              28 LOAD_ATTR                1 (use_return_dict)\n              30 LOAD_CONST               2 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'encoder_hidden_states', 'encoder_attention_mask', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                0 (config)\n              36 LOAD_ATTR                1 (use_return_dict)\n              38 STORE_FAST              12 (return_dict)\n\n1358          40 CALL_FUNCTION_KW        11\n              42 LOAD_GLOBAL             11 (__resume_at_50_5)\n              44 ROT_TWO\n              46 LOAD_FAST                0 (self)\n              48 LOAD_FAST                9 (labels)\n              50 LOAD_FAST               12 (return_dict)\n              52 CALL_FUNCTION            4\n              54 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 913 \n913           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                8 (warn_if_padding_and_no_attention_mask)\n              4 LOAD_FAST                1 (input_ids)\n              6 LOAD_FAST                2 (attention_mask)\n              8 LOAD_CONST               2 (False)\n             10 LOAD_FAST                0 (self)\n             12 LOAD_ATTR                0 (config)\n             14 LOAD_ATTR                1 (output_attentions)\n             16 LOAD_FAST                0 (self)\n             18 LOAD_ATTR                0 (config)\n             20 LOAD_ATTR                2 (output_hidden_states)\n             22 LOAD_GLOBAL             30 (__import_torch)\n             24 LOAD_ATTR               31 (Size)\n             26 LOAD_CONST              15 (16)\n             28 LOAD_CONST              16 (512)\n             30 BUILD_TUPLE              2\n             32 CALL_FUNCTION            1\n             34 STORE_FAST              14 (input_shape)\n             36 STORE_FAST              12 (output_hidden_states)\n             38 STORE_FAST              11 (output_attentions)\n             40 STORE_FAST              10 (use_cache)\n\n970          42 CALL_FUNCTION            2\n             44 LOAD_GLOBAL             32 (__resume_at_144_6)\n             46 ROT_TWO\n             48 LOAD_FAST                0 (self)\n             50 LOAD_FAST                1 (input_ids)\n             52 LOAD_FAST                2 (attention_mask)\n             54 LOAD_FAST                3 (token_type_ids)\n             56 LOAD_FAST                4 (position_ids)\n             58 LOAD_FAST                5 (head_mask)\n             60 LOAD_FAST                6 (inputs_embeds)\n             62 LOAD_FAST                7 (encoder_hidden_states)\n             64 LOAD_FAST                8 (encoder_attention_mask)\n             66 LOAD_FAST                9 (past_key_values)\n             68 LOAD_FAST               10 (use_cache)\n             70 LOAD_FAST               11 (output_attentions)\n             72 LOAD_FAST               12 (output_hidden_states)\n             74 LOAD_FAST               13 (return_dict)\n             76 LOAD_FAST               14 (input_shape)\n             78 CALL_FUNCTION           16\n             80 RETURN_VALUE\n\n", "MODIFIED BYTECODE warn_if_padding_and_no_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/modeling_utils.py line 3482 \n3490           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (config)\n               4 LOAD_ATTR                1 (pad_token_id)\n               6 LOAD_FAST                1 (input_ids)\n               8 LOAD_CONST               1 (None)\n              10 LOAD_CONST               1 (None)\n              12 BUILD_SLICE              2\n              14 LOAD_CONST               2 (-1)\n              16 LOAD_CONST               3 (0)\n              18 BUILD_LIST               2\n              20 BUILD_TUPLE              2\n              22 BINARY_SUBSCR\n              24 CONTAINS_OP              0\n              26 POP_JUMP_IF_FALSE       90 (to 180)\n\n3492          28 LOAD_CONST               4 ('We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.')\n\n3491          30 STORE_FAST               3 (warn_string)\n\n3500          32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                0 (config)\n              36 LOAD_ATTR                2 (bos_token_id)\n              38 LOAD_CONST               1 (None)\n              40 IS_OP                    1\n              42 POP_JUMP_IF_FALSE       30 (to 60)\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                2 (bos_token_id)\n              50 LOAD_FAST                0 (self)\n              52 LOAD_ATTR                0 (config)\n              54 LOAD_ATTR                1 (pad_token_id)\n              56 COMPARE_OP               2 (==)\n              58 POP_JUMP_IF_TRUE        58 (to 116)\n\n3501     >>   60 LOAD_FAST                0 (self)\n              62 LOAD_ATTR                0 (config)\n              64 LOAD_ATTR                3 (eos_token_id)\n              66 LOAD_CONST               1 (None)\n              68 IS_OP                    1\n              70 POP_JUMP_IF_FALSE       44 (to 88)\n              72 LOAD_FAST                0 (self)\n              74 LOAD_ATTR                0 (config)\n              76 LOAD_ATTR                3 (eos_token_id)\n              78 LOAD_FAST                0 (self)\n              80 LOAD_ATTR                0 (config)\n              82 LOAD_ATTR                1 (pad_token_id)\n              84 COMPARE_OP               2 (==)\n              86 POP_JUMP_IF_TRUE        58 (to 116)\n\n3502     >>   88 LOAD_FAST                0 (self)\n              90 LOAD_ATTR                0 (config)\n              92 LOAD_ATTR                4 (sep_token_id)\n              94 LOAD_CONST               1 (None)\n              96 IS_OP                    1\n              98 POP_JUMP_IF_FALSE       83 (to 166)\n             100 LOAD_FAST                0 (self)\n             102 LOAD_ATTR                0 (config)\n             104 LOAD_ATTR                4 (sep_token_id)\n             106 LOAD_FAST                0 (self)\n             108 LOAD_ATTR                0 (config)\n             110 LOAD_ATTR                1 (pad_token_id)\n             112 COMPARE_OP               2 (==)\n             114 POP_JUMP_IF_FALSE       83 (to 166)\n\n3504     >>  116 LOAD_FAST                3 (warn_string)\n\n3505         118 LOAD_CONST               5 ('\\nYou may ignore this warning if your `pad_token_id` (')\n             120 LOAD_FAST                0 (self)\n             122 LOAD_ATTR                0 (config)\n             124 LOAD_ATTR                1 (pad_token_id)\n             126 FORMAT_VALUE             0\n             128 LOAD_CONST               6 (') is identical to the `bos_token_id` (')\n\n3506         130 LOAD_FAST                0 (self)\n             132 LOAD_ATTR                0 (config)\n             134 LOAD_ATTR                2 (bos_token_id)\n\n3505         136 FORMAT_VALUE             0\n             138 LOAD_CONST               7 ('), `eos_token_id` (')\n\n3506         140 LOAD_FAST                0 (self)\n             142 LOAD_ATTR                0 (config)\n             144 LOAD_ATTR                3 (eos_token_id)\n\n3505         146 FORMAT_VALUE             0\n             148 LOAD_CONST               8 ('), or the `sep_token_id` (')\n\n3507         150 LOAD_FAST                0 (self)\n             152 LOAD_ATTR                0 (config)\n             154 LOAD_ATTR                4 (sep_token_id)\n\n3505         156 FORMAT_VALUE             0\n             158 LOAD_CONST               9 ('), and your input is not padded.')\n             160 BUILD_STRING             9\n\n3504         162 INPLACE_ADD\n             164 STORE_FAST               3 (warn_string)\n\n3510     >>  166 LOAD_GLOBAL              5 (logger)\n             168 LOAD_ATTR                6 (warning_once)\n             170 LOAD_FAST                3 (warn_string)\n             172 CALL_FUNCTION            1\n             174 POP_TOP\n             176 LOAD_CONST               1 (None)\n             178 RETURN_VALUE\n\n3490     >>  180 LOAD_CONST               1 (None)\n             182 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 970 \n970           0 LOAD_GLOBAL             33 (__compiled_fn_7)\n              2 LOAD_FAST                2 (input_ids)\n              4 CALL_FUNCTION            1\n              6 EXTENDED_ARG             2\n              8 STORE_FAST             745 (graph_out_0)\n             10 LOAD_CONST              15 (<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>)\n             12 EXTENDED_ARG             2\n             14 LOAD_FAST              745 (graph_out_0)\n             16 LOAD_CONST               6 (0)\n             18 BINARY_SUBSCR\n             20 LOAD_CONST               1 (None)\n             22 LOAD_CONST               1 (None)\n             24 LOAD_CONST               1 (None)\n             26 LOAD_CONST               1 (None)\n             28 LOAD_CONST               1 (None)\n             30 LOAD_CONST              16 (('last_hidden_state', 'pooler_output', 'hidden_states', 'past_key_values', 'attentions', 'cross_attentions'))\n             32 CALL_FUNCTION_KW         6\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 1358 \n1358           0 LOAD_GLOBAL             11 (__compiled_fn_8)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               12 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              36 (graph_out_0)\n              12 LOAD_CONST               7 (<class 'transformers.modeling_outputs.MaskedLMOutput'>)\n              14 LOAD_FAST               36 (graph_out_0)\n              16 LOAD_CONST               3 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               36 (graph_out_0)\n              22 LOAD_CONST               8 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR                8 (hidden_states)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR                9 (attentions)\n              34 LOAD_CONST               9 (('loss', 'logits', 'hidden_states', 'attentions'))\n              36 CALL_FUNCTION_KW         4\n              38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_9)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_10)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_11)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             10 (__import_contextlib)\n              2 LOAD_ATTR               11 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('start_positions')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('start_positions')\n             22 BINARY_SUBSCR\n             24 LOAD_CONST               4 ('end_positions')\n             26 LOAD_FAST                4 (cloned_inputs)\n             28 LOAD_CONST               4 ('end_positions')\n             30 BINARY_SUBSCR\n             32 BUILD_MAP                3\n             34 LOAD_GLOBAL             10 (__import_contextlib)\n             36 LOAD_ATTR               11 (nullcontext)\n             38 CALL_FUNCTION            0\n             40 STORE_FAST              17 (___context_manager_0_3)\n             42 LOAD_FAST               17 (___context_manager_0_3)\n             44 LOAD_METHOD             12 (__enter__)\n             46 CALL_METHOD              0\n             48 POP_TOP\n             50 SETUP_FINALLY           10 (to 72)\n\n557          52 CALL_FUNCTION_EX         1\n             54 POP_BLOCK\n             56 LOAD_FAST               17 (___context_manager_0_3)\n             58 LOAD_METHOD             13 (__exit__)\n             60 LOAD_CONST               0 (None)\n             62 DUP_TOP\n             64 DUP_TOP\n             66 CALL_METHOD              3\n             68 POP_TOP\n             70 JUMP_FORWARD             9 (to 90)\n        >>   72 NOP\n             74 LOAD_FAST               17 (___context_manager_0_3)\n             76 LOAD_METHOD             13 (__exit__)\n             78 LOAD_CONST               0 (None)\n             80 DUP_TOP\n             82 DUP_TOP\n             84 CALL_METHOD              3\n             86 POP_TOP\n             88 RERAISE                  0\n        >>   90 NOP\n             92 LOAD_GLOBAL             14 (__resume_at_44_4)\n             94 ROT_THREE\n             96 LOAD_FAST                1 (self)\n             98 LOAD_FAST                2 (mod)\n            100 LOAD_FAST                3 (collect_outputs)\n            102 LOAD_FAST                4 (cloned_inputs)\n            104 CALL_FUNCTION            6\n            106 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 1808 \n1808           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                2 (bert)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                3 (token_type_ids)\n              10 LOAD_FAST                4 (position_ids)\n              12 LOAD_FAST                5 (head_mask)\n              14 LOAD_FAST                6 (inputs_embeds)\n              16 LOAD_FAST                9 (output_attentions)\n              18 LOAD_FAST               10 (output_hidden_states)\n              20 LOAD_FAST                0 (self)\n              22 LOAD_ATTR                0 (config)\n              24 LOAD_ATTR                1 (use_return_dict)\n              26 LOAD_CONST               2 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              28 LOAD_FAST                0 (self)\n              30 LOAD_ATTR                0 (config)\n              32 LOAD_ATTR                1 (use_return_dict)\n              34 STORE_FAST              11 (return_dict)\n\n1844          36 CALL_FUNCTION_KW         9\n              38 LOAD_GLOBAL             15 (__resume_at_46_5)\n              40 ROT_TWO\n              42 LOAD_FAST                0 (self)\n              44 LOAD_FAST                7 (start_positions)\n              46 LOAD_FAST                8 (end_positions)\n              48 LOAD_FAST               11 (return_dict)\n              50 CALL_FUNCTION            5\n              52 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 913 \n913           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                8 (warn_if_padding_and_no_attention_mask)\n              4 LOAD_FAST                1 (input_ids)\n              6 LOAD_FAST                2 (attention_mask)\n              8 LOAD_CONST               2 (False)\n             10 LOAD_FAST                0 (self)\n             12 LOAD_ATTR                0 (config)\n             14 LOAD_ATTR                1 (output_attentions)\n             16 LOAD_FAST                0 (self)\n             18 LOAD_ATTR                0 (config)\n             20 LOAD_ATTR                2 (output_hidden_states)\n             22 LOAD_GLOBAL             30 (__import_torch)\n             24 LOAD_ATTR               31 (Size)\n             26 LOAD_CONST              15 (16)\n             28 LOAD_CONST              16 (512)\n             30 BUILD_TUPLE              2\n             32 CALL_FUNCTION            1\n             34 STORE_FAST              14 (input_shape)\n             36 STORE_FAST              12 (output_hidden_states)\n             38 STORE_FAST              11 (output_attentions)\n             40 STORE_FAST              10 (use_cache)\n\n970          42 CALL_FUNCTION            2\n             44 LOAD_GLOBAL             32 (__resume_at_144_6)\n             46 ROT_TWO\n             48 LOAD_FAST                0 (self)\n             50 LOAD_FAST                1 (input_ids)\n             52 LOAD_FAST                2 (attention_mask)\n             54 LOAD_FAST                3 (token_type_ids)\n             56 LOAD_FAST                4 (position_ids)\n             58 LOAD_FAST                5 (head_mask)\n             60 LOAD_FAST                6 (inputs_embeds)\n             62 LOAD_FAST                7 (encoder_hidden_states)\n             64 LOAD_FAST                8 (encoder_attention_mask)\n             66 LOAD_FAST                9 (past_key_values)\n             68 LOAD_FAST               10 (use_cache)\n             70 LOAD_FAST               11 (output_attentions)\n             72 LOAD_FAST               12 (output_hidden_states)\n             74 LOAD_FAST               13 (return_dict)\n             76 LOAD_FAST               14 (input_shape)\n             78 CALL_FUNCTION           16\n             80 RETURN_VALUE\n\n", "MODIFIED BYTECODE warn_if_padding_and_no_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/modeling_utils.py line 3482 \n3490           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (config)\n               4 LOAD_ATTR                1 (pad_token_id)\n               6 LOAD_FAST                1 (input_ids)\n               8 LOAD_CONST               1 (None)\n              10 LOAD_CONST               1 (None)\n              12 BUILD_SLICE              2\n              14 LOAD_CONST               2 (-1)\n              16 LOAD_CONST               3 (0)\n              18 BUILD_LIST               2\n              20 BUILD_TUPLE              2\n              22 BINARY_SUBSCR\n              24 CONTAINS_OP              0\n              26 POP_JUMP_IF_FALSE       90 (to 180)\n\n3492          28 LOAD_CONST               4 ('We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.')\n\n3491          30 STORE_FAST               3 (warn_string)\n\n3500          32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                0 (config)\n              36 LOAD_ATTR                2 (bos_token_id)\n              38 LOAD_CONST               1 (None)\n              40 IS_OP                    1\n              42 POP_JUMP_IF_FALSE       30 (to 60)\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                2 (bos_token_id)\n              50 LOAD_FAST                0 (self)\n              52 LOAD_ATTR                0 (config)\n              54 LOAD_ATTR                1 (pad_token_id)\n              56 COMPARE_OP               2 (==)\n              58 POP_JUMP_IF_TRUE        58 (to 116)\n\n3501     >>   60 LOAD_FAST                0 (self)\n              62 LOAD_ATTR                0 (config)\n              64 LOAD_ATTR                3 (eos_token_id)\n              66 LOAD_CONST               1 (None)\n              68 IS_OP                    1\n              70 POP_JUMP_IF_FALSE       44 (to 88)\n              72 LOAD_FAST                0 (self)\n              74 LOAD_ATTR                0 (config)\n              76 LOAD_ATTR                3 (eos_token_id)\n              78 LOAD_FAST                0 (self)\n              80 LOAD_ATTR                0 (config)\n              82 LOAD_ATTR                1 (pad_token_id)\n              84 COMPARE_OP               2 (==)\n              86 POP_JUMP_IF_TRUE        58 (to 116)\n\n3502     >>   88 LOAD_FAST                0 (self)\n              90 LOAD_ATTR                0 (config)\n              92 LOAD_ATTR                4 (sep_token_id)\n              94 LOAD_CONST               1 (None)\n              96 IS_OP                    1\n              98 POP_JUMP_IF_FALSE       83 (to 166)\n             100 LOAD_FAST                0 (self)\n             102 LOAD_ATTR                0 (config)\n             104 LOAD_ATTR                4 (sep_token_id)\n             106 LOAD_FAST                0 (self)\n             108 LOAD_ATTR                0 (config)\n             110 LOAD_ATTR                1 (pad_token_id)\n             112 COMPARE_OP               2 (==)\n             114 POP_JUMP_IF_FALSE       83 (to 166)\n\n3504     >>  116 LOAD_FAST                3 (warn_string)\n\n3505         118 LOAD_CONST               5 ('\\nYou may ignore this warning if your `pad_token_id` (')\n             120 LOAD_FAST                0 (self)\n             122 LOAD_ATTR                0 (config)\n             124 LOAD_ATTR                1 (pad_token_id)\n             126 FORMAT_VALUE             0\n             128 LOAD_CONST               6 (') is identical to the `bos_token_id` (')\n\n3506         130 LOAD_FAST                0 (self)\n             132 LOAD_ATTR                0 (config)\n             134 LOAD_ATTR                2 (bos_token_id)\n\n3505         136 FORMAT_VALUE             0\n             138 LOAD_CONST               7 ('), `eos_token_id` (')\n\n3506         140 LOAD_FAST                0 (self)\n             142 LOAD_ATTR                0 (config)\n             144 LOAD_ATTR                3 (eos_token_id)\n\n3505         146 FORMAT_VALUE             0\n             148 LOAD_CONST               8 ('), or the `sep_token_id` (')\n\n3507         150 LOAD_FAST                0 (self)\n             152 LOAD_ATTR                0 (config)\n             154 LOAD_ATTR                4 (sep_token_id)\n\n3505         156 FORMAT_VALUE             0\n             158 LOAD_CONST               9 ('), and your input is not padded.')\n             160 BUILD_STRING             9\n\n3504         162 INPLACE_ADD\n             164 STORE_FAST               3 (warn_string)\n\n3510     >>  166 LOAD_GLOBAL              5 (logger)\n             168 LOAD_ATTR                6 (warning_once)\n             170 LOAD_FAST                3 (warn_string)\n             172 CALL_FUNCTION            1\n             174 POP_TOP\n             176 LOAD_CONST               1 (None)\n             178 RETURN_VALUE\n\n3490     >>  180 LOAD_CONST               1 (None)\n             182 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 970 \n970           0 LOAD_GLOBAL             33 (__compiled_fn_7)\n              2 LOAD_FAST                2 (input_ids)\n              4 CALL_FUNCTION            1\n              6 EXTENDED_ARG             2\n              8 STORE_FAST             745 (graph_out_0)\n             10 LOAD_CONST              15 (<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>)\n             12 EXTENDED_ARG             2\n             14 LOAD_FAST              745 (graph_out_0)\n             16 LOAD_CONST               6 (0)\n             18 BINARY_SUBSCR\n             20 LOAD_CONST               1 (None)\n             22 LOAD_CONST               1 (None)\n             24 LOAD_CONST               1 (None)\n             26 LOAD_CONST               1 (None)\n             28 LOAD_CONST               1 (None)\n             30 LOAD_CONST              16 (('last_hidden_state', 'pooler_output', 'hidden_states', 'past_key_values', 'attentions', 'cross_attentions'))\n             32 CALL_FUNCTION_KW         6\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 1844 \n1844           0 LOAD_GLOBAL             14 (__compiled_fn_8)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               15 (last_hidden_state)\n               6 LOAD_FAST                2 (start_positions)\n               8 LOAD_FAST                3 (end_positions)\n              10 CALL_FUNCTION            3\n              12 STORE_FAST              39 (graph_out_0)\n              14 LOAD_CONST              10 (<class 'transformers.modeling_outputs.QuestionAnsweringModelOutput'>)\n              16 LOAD_FAST               39 (graph_out_0)\n              18 LOAD_CONST               3 (0)\n              20 BINARY_SUBSCR\n              22 LOAD_FAST               39 (graph_out_0)\n              24 LOAD_CONST               4 (1)\n              26 BINARY_SUBSCR\n              28 LOAD_FAST               39 (graph_out_0)\n              30 LOAD_CONST               8 (2)\n              32 BINARY_SUBSCR\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               12 (hidden_states)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               13 (attentions)\n              42 LOAD_CONST              11 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n              44 CALL_FUNCTION_KW         5\n              46 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_9)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_10)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_11)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             12 (__import_contextlib)\n             28 LOAD_ATTR               13 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              47 (___context_manager_0_3)\n             34 LOAD_FAST               47 (___context_manager_0_3)\n             36 LOAD_METHOD             14 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               47 (___context_manager_0_3)\n             50 LOAD_METHOD             15 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               47 (___context_manager_0_3)\n             68 LOAD_METHOD             15 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             16 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 1435 \n1435           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                4 (model)\n               4 LOAD_ATTR                5 (decoder)\n               6 LOAD_FAST                1 (input_ids)\n               8 LOAD_FAST                2 (attention_mask)\n              10 LOAD_FAST                3 (encoder_hidden_states)\n              12 LOAD_FAST                4 (encoder_attention_mask)\n              14 LOAD_FAST                5 (head_mask)\n              16 LOAD_FAST                6 (cross_attn_head_mask)\n              18 LOAD_FAST                7 (past_key_values)\n              20 LOAD_FAST                8 (inputs_embeds)\n              22 LOAD_FAST               10 (use_cache)\n              24 LOAD_FAST                0 (self)\n              26 LOAD_ATTR                0 (config)\n              28 LOAD_ATTR                1 (output_attentions)\n              30 LOAD_FAST                0 (self)\n              32 LOAD_ATTR                0 (config)\n              34 LOAD_ATTR                2 (output_hidden_states)\n              36 LOAD_FAST                0 (self)\n              38 LOAD_ATTR                0 (config)\n              40 LOAD_ATTR                3 (use_return_dict)\n              42 LOAD_CONST               2 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                3 (use_return_dict)\n              50 STORE_FAST              13 (return_dict)\n\n1546          52 CALL_FUNCTION_KW        12\n              54 LOAD_GLOBAL             19 (__resume_at_94_5)\n              56 ROT_TWO\n              58 LOAD_FAST                0 (self)\n              60 LOAD_FAST                9 (labels)\n              62 LOAD_FAST               13 (return_dict)\n              64 CALL_FUNCTION            4\n              66 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 853 \n853           0 LOAD_GLOBAL              5 (__compiled_fn_6)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 116 \n116           0 LOAD_GLOBAL              8 (__compiled_fn_7)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_8)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_15)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_16)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              6 (__resume_at_14_17)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fd1e1f79370, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_18')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fd1e2164870, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_19')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fd1e1f79370, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_20')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fd1e2164870, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_21')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 1546 \n1546           0 LOAD_GLOBAL             17 (__compiled_fn_22)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               18 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              26 (graph_out_0)\n              12 LOAD_CONST               7 (<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>)\n              14 LOAD_FAST               26 (graph_out_0)\n              16 LOAD_CONST               3 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               26 (graph_out_0)\n              22 LOAD_CONST               5 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR               13 (past_key_values)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR               14 (hidden_states)\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               15 (attentions)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               16 (cross_attentions)\n              42 LOAD_CONST               8 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n              44 CALL_FUNCTION_KW         6\n              46 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_23)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_24)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_25)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('decoder_input_ids')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('decoder_input_ids')\n             22 BINARY_SUBSCR\n             24 LOAD_CONST               4 ('labels')\n             26 LOAD_FAST                4 (cloned_inputs)\n             28 LOAD_CONST               4 ('labels')\n             30 BINARY_SUBSCR\n             32 BUILD_MAP                3\n             34 LOAD_GLOBAL             12 (__import_contextlib)\n             36 LOAD_ATTR               13 (nullcontext)\n             38 CALL_FUNCTION            0\n             40 STORE_FAST              37 (___context_manager_0_3)\n             42 LOAD_FAST               37 (___context_manager_0_3)\n             44 LOAD_METHOD             14 (__enter__)\n             46 CALL_METHOD              0\n             48 POP_TOP\n             50 SETUP_FINALLY           10 (to 72)\n\n557          52 CALL_FUNCTION_EX         1\n             54 POP_BLOCK\n             56 LOAD_FAST               37 (___context_manager_0_3)\n             58 LOAD_METHOD             15 (__exit__)\n             60 LOAD_CONST               0 (None)\n             62 DUP_TOP\n             64 DUP_TOP\n             66 CALL_METHOD              3\n             68 POP_TOP\n             70 JUMP_FORWARD             9 (to 90)\n        >>   72 NOP\n             74 LOAD_FAST               37 (___context_manager_0_3)\n             76 LOAD_METHOD             15 (__exit__)\n             78 LOAD_CONST               0 (None)\n             80 DUP_TOP\n             82 DUP_TOP\n             84 CALL_METHOD              3\n             86 POP_TOP\n             88 RERAISE                  0\n        >>   90 NOP\n             92 LOAD_GLOBAL             16 (__resume_at_44_4)\n             94 ROT_THREE\n             96 LOAD_FAST                1 (self)\n             98 LOAD_FAST                2 (mod)\n            100 LOAD_FAST                3 (collect_outputs)\n            102 LOAD_FAST                4 (cloned_inputs)\n            104 CALL_FUNCTION            6\n            106 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 1266 \n1266           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                7 (model)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                3 (decoder_input_ids)\n              10 LOAD_FAST                8 (encoder_outputs)\n              12 LOAD_FAST                4 (decoder_attention_mask)\n              14 LOAD_FAST                5 (head_mask)\n              16 LOAD_FAST                6 (decoder_head_mask)\n              18 LOAD_FAST                7 (cross_attn_head_mask)\n              20 LOAD_FAST                9 (past_key_values)\n              22 LOAD_FAST               10 (inputs_embeds)\n              24 LOAD_FAST               11 (decoder_inputs_embeds)\n              26 LOAD_CONST               3 (False)\n              28 LOAD_FAST               14 (output_attentions)\n              30 LOAD_FAST               15 (output_hidden_states)\n              32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                0 (config)\n              36 LOAD_ATTR                1 (use_return_dict)\n              38 LOAD_CONST               4 (('attention_mask', 'decoder_input_ids', 'encoder_outputs', 'decoder_attention_mask', 'head_mask', 'decoder_head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'decoder_inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              40 LOAD_FAST                0 (self)\n              42 LOAD_ATTR                0 (config)\n              44 LOAD_ATTR                1 (use_return_dict)\n              46 STORE_FAST              16 (return_dict)\n\n1307          48 CALL_FUNCTION_KW        15\n              50 LOAD_GLOBAL             23 (__resume_at_120_5)\n              52 ROT_TWO\n              54 LOAD_FAST                0 (self)\n              56 LOAD_FAST               12 (labels)\n              58 LOAD_FAST               16 (return_dict)\n              60 CALL_FUNCTION            4\n              62 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 1127 \n1127           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                5 (encoder)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                5 (head_mask)\n              10 LOAD_FAST               10 (inputs_embeds)\n              12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (output_attentions)\n              18 LOAD_FAST                0 (self)\n              20 LOAD_ATTR                0 (config)\n              22 LOAD_ATTR                2 (output_hidden_states)\n              24 LOAD_FAST               15 (return_dict)\n              26 LOAD_CONST               2 (('input_ids', 'attention_mask', 'head_mask', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              28 LOAD_FAST                0 (self)\n              30 LOAD_ATTR                0 (config)\n              32 LOAD_ATTR                1 (output_attentions)\n              34 LOAD_FAST                0 (self)\n              36 LOAD_ATTR                0 (config)\n              38 LOAD_ATTR                2 (output_hidden_states)\n              40 STORE_FAST              14 (output_hidden_states)\n              42 STORE_FAST              13 (output_attentions)\n\n1174          44 CALL_FUNCTION_KW         7\n              46 LOAD_GLOBAL             18 (__resume_at_110_6)\n              48 ROT_TWO\n              50 LOAD_FAST                0 (self)\n              52 LOAD_FAST                2 (attention_mask)\n              54 LOAD_FAST                3 (decoder_input_ids)\n              56 LOAD_FAST                4 (decoder_attention_mask)\n              58 LOAD_FAST                6 (decoder_head_mask)\n              60 LOAD_FAST                7 (cross_attn_head_mask)\n              62 LOAD_FAST                9 (past_key_values)\n              64 LOAD_FAST               11 (decoder_inputs_embeds)\n              66 LOAD_FAST               12 (use_cache)\n              68 LOAD_FAST               13 (output_attentions)\n              70 LOAD_FAST               14 (output_hidden_states)\n              72 LOAD_FAST               15 (return_dict)\n              74 CALL_FUNCTION           13\n              76 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 116 \n116           0 LOAD_GLOBAL              8 (__compiled_fn_7)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 298 \n298           0 LOAD_GLOBAL             21 (__compiled_fn_8)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 298 \n298           0 LOAD_GLOBAL             21 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 298 \n298           0 LOAD_GLOBAL             21 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 298 \n298           0 LOAD_GLOBAL             21 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 298 \n298           0 LOAD_GLOBAL             21 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 298 \n298           0 LOAD_GLOBAL             21 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 298 \n298           0 LOAD_GLOBAL             21 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 298 \n298           0 LOAD_GLOBAL             21 (__compiled_fn_15)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              4 (__resume_at_6_16)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f3d31698c90, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_17')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f3d3442b940, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_18')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 1174 \n1174           0 LOAD_FAST                1 (self)\n               2 LOAD_ATTR                9 (decoder)\n               4 LOAD_FAST                3 (decoder_input_ids)\n               6 LOAD_FAST                4 (decoder_attention_mask)\n               8 LOAD_FAST                0 (___stack0)\n              10 LOAD_ATTR               11 (last_hidden_state)\n              12 LOAD_FAST                2 (attention_mask)\n              14 LOAD_FAST                5 (decoder_head_mask)\n              16 LOAD_FAST                6 (cross_attn_head_mask)\n              18 LOAD_FAST                7 (past_key_values)\n              20 LOAD_FAST                8 (decoder_inputs_embeds)\n              22 LOAD_FAST                9 (use_cache)\n              24 LOAD_FAST               10 (output_attentions)\n              26 LOAD_FAST               11 (output_hidden_states)\n              28 LOAD_FAST               12 (return_dict)\n              30 LOAD_CONST               7 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              32 LOAD_FAST                0 (___stack0)\n              34 STORE_FAST              15 (encoder_outputs)\n\n1192          36 CALL_FUNCTION_KW        12\n              38 LOAD_GLOBAL             18 (__resume_at_226_19)\n              40 ROT_TWO\n              42 LOAD_FAST               12 (return_dict)\n              44 LOAD_FAST               15 (encoder_outputs)\n              46 CALL_FUNCTION            3\n              48 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 853 \n853           0 LOAD_GLOBAL              5 (__compiled_fn_20)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 116 \n116           0 LOAD_GLOBAL              8 (__compiled_fn_21)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_22)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_23)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_24)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_25)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_26)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_27)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_28)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 377 \n377           0 LOAD_GLOBAL             14 (__compiled_fn_29)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_30)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f3d31698c90, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_31')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f3d3442b940, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_32')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              9 (__resume_at_6_33)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (decoder_hidden_states)\n             14 LOAD_FAST                4 (decoder_attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 LOAD_FAST                6 (encoder_last_hidden_state)\n             20 LOAD_FAST                7 (encoder_hidden_states)\n             22 LOAD_FAST                8 (encoder_attentions)\n             24 CALL_FUNCTION            8\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              9 (__resume_at_14_34)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (decoder_hidden_states)\n             12 LOAD_FAST                3 (decoder_attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 LOAD_FAST                5 (encoder_last_hidden_state)\n             18 LOAD_FAST                6 (encoder_hidden_states)\n             20 LOAD_FAST                7 (encoder_attentions)\n             22 CALL_FUNCTION            7\n             24 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 4 \n  4           0 LOAD_FAST                1 (decoder_hidden_states)\n              2 LOAD_FAST                0 (self)\n\n  5           4 STORE_ATTR               2 (decoder_hidden_states)\n              6 LOAD_GLOBAL              9 (__resume_at_20_35)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (decoder_attentions)\n             12 LOAD_FAST                3 (cross_attentions)\n             14 LOAD_FAST                4 (encoder_last_hidden_state)\n             16 LOAD_FAST                5 (encoder_hidden_states)\n             18 LOAD_FAST                6 (encoder_attentions)\n             20 CALL_FUNCTION            6\n             22 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 5 \n  5           0 LOAD_FAST                1 (decoder_attentions)\n              2 LOAD_FAST                0 (self)\n\n  6           4 STORE_ATTR               3 (decoder_attentions)\n              6 LOAD_GLOBAL              9 (__resume_at_26_36)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (cross_attentions)\n             12 LOAD_FAST                3 (encoder_last_hidden_state)\n             14 LOAD_FAST                4 (encoder_hidden_states)\n             16 LOAD_FAST                5 (encoder_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 6 \n  6           0 LOAD_FAST                1 (cross_attentions)\n              2 LOAD_FAST                0 (self)\n\n  7           4 STORE_ATTR               4 (cross_attentions)\n              6 LOAD_GLOBAL              9 (__resume_at_32_37)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (encoder_last_hidden_state)\n             12 LOAD_FAST                3 (encoder_hidden_states)\n             14 LOAD_FAST                4 (encoder_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 7 \n  7           0 LOAD_FAST                1 (encoder_last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  8           4 STORE_ATTR               5 (encoder_last_hidden_state)\n              6 LOAD_GLOBAL              9 (__resume_at_38_38)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (encoder_hidden_states)\n             12 LOAD_FAST                3 (encoder_attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f3d31698c90, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_39')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f3d3442b940, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_40')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (encoder_last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f3d31698c90, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_41')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (encoder_last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f3d3442b940, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_42')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py line 1307 \n1307           0 LOAD_GLOBAL             21 (__compiled_fn_43)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               22 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              28 (graph_out_0)\n              12 LOAD_CONST               9 (<class 'transformers.modeling_outputs.Seq2SeqLMOutput'>)\n              14 LOAD_FAST               28 (graph_out_0)\n              16 LOAD_CONST               5 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               28 (graph_out_0)\n              22 LOAD_CONST               7 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR               14 (past_key_values)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR               15 (decoder_hidden_states)\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               16 (decoder_attentions)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               17 (cross_attentions)\n              42 LOAD_FAST                0 (___stack0)\n              44 LOAD_ATTR               18 (encoder_last_hidden_state)\n              46 LOAD_FAST                0 (___stack0)\n              48 LOAD_ATTR               19 (encoder_hidden_states)\n              50 LOAD_FAST                0 (___stack0)\n              52 LOAD_ATTR               20 (encoder_attentions)\n              54 LOAD_CONST              10 (('loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'))\n              56 CALL_FUNCTION_KW         9\n              58 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_44)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_45)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_46)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             10 (__import_contextlib)\n              2 LOAD_ATTR               11 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             10 (__import_contextlib)\n             28 LOAD_ATTR               11 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              17 (___context_manager_0_3)\n             34 LOAD_FAST               17 (___context_manager_0_3)\n             36 LOAD_METHOD             12 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               17 (___context_manager_0_3)\n             50 LOAD_METHOD             13 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               17 (___context_manager_0_3)\n             68 LOAD_METHOD             13 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             14 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py line 954 \n954           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                2 (roberta)\n              4 LOAD_FAST                1 (input_ids)\n              6 LOAD_FAST                2 (attention_mask)\n              8 LOAD_FAST                3 (token_type_ids)\n             10 LOAD_FAST                4 (position_ids)\n             12 LOAD_FAST                5 (head_mask)\n             14 LOAD_FAST                6 (inputs_embeds)\n             16 LOAD_FAST                7 (encoder_hidden_states)\n             18 LOAD_FAST                8 (encoder_attention_mask)\n             20 LOAD_FAST               10 (output_attentions)\n             22 LOAD_FAST               11 (output_hidden_states)\n             24 LOAD_FAST                0 (self)\n             26 LOAD_ATTR                0 (config)\n             28 LOAD_ATTR                1 (use_return_dict)\n             30 LOAD_CONST               2 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'encoder_hidden_states', 'encoder_attention_mask', 'output_attentions', 'output_hidden_states', 'return_dict'))\n             32 LOAD_FAST                0 (self)\n             34 LOAD_ATTR                0 (config)\n             36 LOAD_ATTR                1 (use_return_dict)\n             38 STORE_FAST              12 (return_dict)\n\n988          40 CALL_FUNCTION_KW        11\n             42 LOAD_GLOBAL             13 (__resume_at_50_5)\n             44 ROT_TWO\n             46 LOAD_FAST                0 (self)\n             48 LOAD_FAST                9 (labels)\n             50 LOAD_FAST               12 (return_dict)\n             52 CALL_FUNCTION            4\n             54 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py line 787 \n787           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                8 (warn_if_padding_and_no_attention_mask)\n              4 LOAD_FAST                1 (input_ids)\n              6 LOAD_FAST                2 (attention_mask)\n              8 LOAD_CONST               2 (False)\n             10 LOAD_FAST                0 (self)\n             12 LOAD_ATTR                0 (config)\n             14 LOAD_ATTR                1 (output_attentions)\n             16 LOAD_FAST                0 (self)\n             18 LOAD_ATTR                0 (config)\n             20 LOAD_ATTR                2 (output_hidden_states)\n             22 LOAD_GLOBAL             30 (__import_torch)\n             24 LOAD_ATTR               31 (Size)\n             26 LOAD_CONST              15 (16)\n             28 LOAD_CONST              16 (512)\n             30 BUILD_TUPLE              2\n             32 CALL_FUNCTION            1\n             34 STORE_FAST              14 (input_shape)\n             36 STORE_FAST              12 (output_hidden_states)\n             38 STORE_FAST              11 (output_attentions)\n             40 STORE_FAST              10 (use_cache)\n\n845          42 CALL_FUNCTION            2\n             44 LOAD_GLOBAL             32 (__resume_at_144_6)\n             46 ROT_TWO\n             48 LOAD_FAST                0 (self)\n             50 LOAD_FAST                1 (input_ids)\n             52 LOAD_FAST                2 (attention_mask)\n             54 LOAD_FAST                3 (token_type_ids)\n             56 LOAD_FAST                4 (position_ids)\n             58 LOAD_FAST                5 (head_mask)\n             60 LOAD_FAST                6 (inputs_embeds)\n             62 LOAD_FAST                7 (encoder_hidden_states)\n             64 LOAD_FAST                8 (encoder_attention_mask)\n             66 LOAD_FAST                9 (past_key_values)\n             68 LOAD_FAST               10 (use_cache)\n             70 LOAD_FAST               11 (output_attentions)\n             72 LOAD_FAST               12 (output_hidden_states)\n             74 LOAD_FAST               13 (return_dict)\n             76 LOAD_FAST               14 (input_shape)\n             78 CALL_FUNCTION           16\n             80 RETURN_VALUE\n\n", "MODIFIED BYTECODE warn_if_padding_and_no_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/modeling_utils.py line 3482 \n3490           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (config)\n               4 LOAD_ATTR                1 (pad_token_id)\n               6 LOAD_FAST                1 (input_ids)\n               8 LOAD_CONST               1 (None)\n              10 LOAD_CONST               1 (None)\n              12 BUILD_SLICE              2\n              14 LOAD_CONST               2 (-1)\n              16 LOAD_CONST               3 (0)\n              18 BUILD_LIST               2\n              20 BUILD_TUPLE              2\n              22 BINARY_SUBSCR\n              24 CONTAINS_OP              0\n              26 POP_JUMP_IF_FALSE       90 (to 180)\n\n3492          28 LOAD_CONST               4 ('We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.')\n\n3491          30 STORE_FAST               3 (warn_string)\n\n3500          32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                0 (config)\n              36 LOAD_ATTR                2 (bos_token_id)\n              38 LOAD_CONST               1 (None)\n              40 IS_OP                    1\n              42 POP_JUMP_IF_FALSE       30 (to 60)\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                2 (bos_token_id)\n              50 LOAD_FAST                0 (self)\n              52 LOAD_ATTR                0 (config)\n              54 LOAD_ATTR                1 (pad_token_id)\n              56 COMPARE_OP               2 (==)\n              58 POP_JUMP_IF_TRUE        58 (to 116)\n\n3501     >>   60 LOAD_FAST                0 (self)\n              62 LOAD_ATTR                0 (config)\n              64 LOAD_ATTR                3 (eos_token_id)\n              66 LOAD_CONST               1 (None)\n              68 IS_OP                    1\n              70 POP_JUMP_IF_FALSE       44 (to 88)\n              72 LOAD_FAST                0 (self)\n              74 LOAD_ATTR                0 (config)\n              76 LOAD_ATTR                3 (eos_token_id)\n              78 LOAD_FAST                0 (self)\n              80 LOAD_ATTR                0 (config)\n              82 LOAD_ATTR                1 (pad_token_id)\n              84 COMPARE_OP               2 (==)\n              86 POP_JUMP_IF_TRUE        58 (to 116)\n\n3502     >>   88 LOAD_FAST                0 (self)\n              90 LOAD_ATTR                0 (config)\n              92 LOAD_ATTR                4 (sep_token_id)\n              94 LOAD_CONST               1 (None)\n              96 IS_OP                    1\n              98 POP_JUMP_IF_FALSE       83 (to 166)\n             100 LOAD_FAST                0 (self)\n             102 LOAD_ATTR                0 (config)\n             104 LOAD_ATTR                4 (sep_token_id)\n             106 LOAD_FAST                0 (self)\n             108 LOAD_ATTR                0 (config)\n             110 LOAD_ATTR                1 (pad_token_id)\n             112 COMPARE_OP               2 (==)\n             114 POP_JUMP_IF_FALSE       83 (to 166)\n\n3504     >>  116 LOAD_FAST                3 (warn_string)\n\n3505         118 LOAD_CONST               5 ('\\nYou may ignore this warning if your `pad_token_id` (')\n             120 LOAD_FAST                0 (self)\n             122 LOAD_ATTR                0 (config)\n             124 LOAD_ATTR                1 (pad_token_id)\n             126 FORMAT_VALUE             0\n             128 LOAD_CONST               6 (') is identical to the `bos_token_id` (')\n\n3506         130 LOAD_FAST                0 (self)\n             132 LOAD_ATTR                0 (config)\n             134 LOAD_ATTR                2 (bos_token_id)\n\n3505         136 FORMAT_VALUE             0\n             138 LOAD_CONST               7 ('), `eos_token_id` (')\n\n3506         140 LOAD_FAST                0 (self)\n             142 LOAD_ATTR                0 (config)\n             144 LOAD_ATTR                3 (eos_token_id)\n\n3505         146 FORMAT_VALUE             0\n             148 LOAD_CONST               8 ('), or the `sep_token_id` (')\n\n3507         150 LOAD_FAST                0 (self)\n             152 LOAD_ATTR                0 (config)\n             154 LOAD_ATTR                4 (sep_token_id)\n\n3505         156 FORMAT_VALUE             0\n             158 LOAD_CONST               9 ('), and your input is not padded.')\n             160 BUILD_STRING             9\n\n3504         162 INPLACE_ADD\n             164 STORE_FAST               3 (warn_string)\n\n3510     >>  166 LOAD_GLOBAL              5 (logger)\n             168 LOAD_ATTR                6 (warning_once)\n             170 LOAD_FAST                3 (warn_string)\n             172 CALL_FUNCTION            1\n             174 POP_TOP\n             176 LOAD_CONST               1 (None)\n             178 RETURN_VALUE\n\n3490     >>  180 LOAD_CONST               1 (None)\n             182 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py line 845 \n845           0 LOAD_GLOBAL             33 (__compiled_fn_7)\n              2 LOAD_FAST                2 (input_ids)\n              4 CALL_FUNCTION            1\n              6 EXTENDED_ARG             2\n              8 STORE_FAST             747 (graph_out_0)\n             10 LOAD_CONST              15 (<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>)\n             12 EXTENDED_ARG             2\n             14 LOAD_FAST              747 (graph_out_0)\n             16 LOAD_CONST               6 (0)\n             18 BINARY_SUBSCR\n             20 LOAD_CONST               1 (None)\n             22 LOAD_CONST               1 (None)\n             24 LOAD_CONST               1 (None)\n             26 LOAD_CONST               1 (None)\n             28 LOAD_CONST               1 (None)\n             30 LOAD_CONST              16 (('last_hidden_state', 'pooler_output', 'hidden_states', 'past_key_values', 'attentions', 'cross_attentions'))\n             32 CALL_FUNCTION_KW         6\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py line 988 \n988           0 LOAD_GLOBAL             13 (__compiled_fn_8)\n              2 LOAD_FAST                0 (___stack0)\n              4 LOAD_ATTR               14 (last_hidden_state)\n              6 LOAD_FAST                2 (labels)\n              8 CALL_FUNCTION            2\n             10 STORE_FAST              33 (graph_out_0)\n             12 LOAD_CONST               7 (<class 'transformers.modeling_outputs.MaskedLMOutput'>)\n             14 LOAD_FAST               33 (graph_out_0)\n             16 LOAD_CONST               3 (0)\n             18 BINARY_SUBSCR\n             20 LOAD_FAST               33 (graph_out_0)\n             22 LOAD_CONST               8 (1)\n             24 BINARY_SUBSCR\n             26 LOAD_FAST                0 (___stack0)\n             28 LOAD_ATTR               10 (hidden_states)\n             30 LOAD_FAST                0 (___stack0)\n             32 LOAD_ATTR               11 (attentions)\n             34 LOAD_CONST               9 (('loss', 'logits', 'hidden_states', 'attentions'))\n             36 CALL_FUNCTION_KW         4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_9)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_10)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_11)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             4\n             18 STORE_FAST            1206 (graph_out_0)\n             20 EXTENDED_ARG             4\n             22 LOAD_FAST             1206 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.MaskedLMOutput'>)\n             32 EXTENDED_ARG             4\n             34 LOAD_FAST             1206 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             4\n             42 LOAD_FAST             1206 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 LOAD_CONST               0 (None)\n             50 LOAD_CONST               0 (None)\n             52 LOAD_CONST               7 (('loss', 'logits', 'hidden_states', 'attentions'))\n             54 CALL_FUNCTION_KW         4\n             56 EXTENDED_ARG             4\n             58 LOAD_FAST             1206 (graph_out_0)\n             60 LOAD_CONST               4 (0)\n             62 BINARY_SUBSCR\n             64 STORE_FAST               7 (loss)\n             66 STORE_FAST               6 (pred)\n\n559          68 CALL_FUNCTION            0\n             70 LOAD_GLOBAL             14 (__resume_at_100_4)\n             72 ROT_TWO\n             74 LOAD_FAST                1 (self)\n             76 LOAD_FAST                2 (mod)\n             78 LOAD_FAST                3 (collect_outputs)\n             80 LOAD_FAST                4 (cloned_inputs)\n             82 LOAD_FAST                6 (pred)\n             84 LOAD_FAST                7 (loss)\n             86 CALL_FUNCTION            7\n             88 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('start_positions')\n             12 BINARY_SUBSCR\n             14 LOAD_FAST                4 (cloned_inputs)\n             16 LOAD_CONST               4 ('end_positions')\n             18 BINARY_SUBSCR\n             20 CALL_FUNCTION            3\n             22 EXTENDED_ARG             4\n             24 STORE_FAST            1207 (graph_out_0)\n             26 EXTENDED_ARG             4\n             28 LOAD_FAST             1207 (graph_out_0)\n             30 LOAD_CONST               5 (0)\n             32 BINARY_SUBSCR\n             34 LOAD_ATTR                6 (backward)\n             36 LOAD_CONST               6 (<class 'transformers.modeling_outputs.QuestionAnsweringModelOutput'>)\n             38 EXTENDED_ARG             4\n             40 LOAD_FAST             1207 (graph_out_0)\n             42 LOAD_CONST               5 (0)\n             44 BINARY_SUBSCR\n             46 EXTENDED_ARG             4\n             48 LOAD_FAST             1207 (graph_out_0)\n             50 LOAD_CONST               7 (1)\n             52 BINARY_SUBSCR\n             54 EXTENDED_ARG             4\n             56 LOAD_FAST             1207 (graph_out_0)\n             58 LOAD_CONST               8 (2)\n             60 BINARY_SUBSCR\n             62 LOAD_CONST               0 (None)\n             64 LOAD_CONST               0 (None)\n             66 LOAD_CONST               9 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n             68 CALL_FUNCTION_KW         5\n             70 EXTENDED_ARG             4\n             72 LOAD_FAST             1207 (graph_out_0)\n             74 LOAD_CONST               5 (0)\n             76 BINARY_SUBSCR\n             78 STORE_FAST               7 (loss)\n             80 STORE_FAST               6 (pred)\n\n559          82 CALL_FUNCTION            0\n             84 LOAD_GLOBAL             14 (__resume_at_100_4)\n             86 ROT_TWO\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 LOAD_FAST                6 (pred)\n             98 LOAD_FAST                7 (loss)\n            100 CALL_FUNCTION            7\n            102 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             7\n             18 STORE_FAST            1896 (graph_out_0)\n             20 EXTENDED_ARG             7\n             22 LOAD_FAST             1896 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.MaskedLMOutput'>)\n             32 EXTENDED_ARG             7\n             34 LOAD_FAST             1896 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             7\n             42 LOAD_FAST             1896 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 LOAD_CONST               0 (None)\n             50 LOAD_CONST               0 (None)\n             52 LOAD_CONST               7 (('loss', 'logits', 'hidden_states', 'attentions'))\n             54 CALL_FUNCTION_KW         4\n             56 EXTENDED_ARG             7\n             58 LOAD_FAST             1896 (graph_out_0)\n             60 LOAD_CONST               4 (0)\n             62 BINARY_SUBSCR\n             64 STORE_FAST               7 (loss)\n             66 STORE_FAST               6 (pred)\n\n559          68 CALL_FUNCTION            0\n             70 LOAD_GLOBAL             14 (__resume_at_100_4)\n             72 ROT_TWO\n             74 LOAD_FAST                1 (self)\n             76 LOAD_FAST                2 (mod)\n             78 LOAD_FAST                3 (collect_outputs)\n             80 LOAD_FAST                4 (cloned_inputs)\n             82 LOAD_FAST                6 (pred)\n             84 LOAD_FAST                7 (loss)\n             86 CALL_FUNCTION            7\n             88 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('start_positions')\n             12 BINARY_SUBSCR\n             14 LOAD_FAST                4 (cloned_inputs)\n             16 LOAD_CONST               4 ('end_positions')\n             18 BINARY_SUBSCR\n             20 CALL_FUNCTION            3\n             22 EXTENDED_ARG             7\n             24 STORE_FAST            1897 (graph_out_0)\n             26 EXTENDED_ARG             7\n             28 LOAD_FAST             1897 (graph_out_0)\n             30 LOAD_CONST               5 (0)\n             32 BINARY_SUBSCR\n             34 LOAD_ATTR                6 (backward)\n             36 LOAD_CONST               6 (<class 'transformers.modeling_outputs.QuestionAnsweringModelOutput'>)\n             38 EXTENDED_ARG             7\n             40 LOAD_FAST             1897 (graph_out_0)\n             42 LOAD_CONST               5 (0)\n             44 BINARY_SUBSCR\n             46 EXTENDED_ARG             7\n             48 LOAD_FAST             1897 (graph_out_0)\n             50 LOAD_CONST               7 (1)\n             52 BINARY_SUBSCR\n             54 EXTENDED_ARG             7\n             56 LOAD_FAST             1897 (graph_out_0)\n             58 LOAD_CONST               8 (2)\n             60 BINARY_SUBSCR\n             62 LOAD_CONST               0 (None)\n             64 LOAD_CONST               0 (None)\n             66 LOAD_CONST               9 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n             68 CALL_FUNCTION_KW         5\n             70 EXTENDED_ARG             7\n             72 LOAD_FAST             1897 (graph_out_0)\n             74 LOAD_CONST               5 (0)\n             76 BINARY_SUBSCR\n             78 STORE_FAST               7 (loss)\n             80 STORE_FAST               6 (pred)\n\n559          82 CALL_FUNCTION            0\n             84 LOAD_GLOBAL             14 (__resume_at_100_4)\n             86 ROT_TWO\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 LOAD_FAST                6 (pred)\n             98 LOAD_FAST                7 (loss)\n            100 CALL_FUNCTION            7\n            102 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             1\n             18 STORE_FAST             281 (graph_out_0)\n             20 EXTENDED_ARG             1\n             22 LOAD_FAST              281 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.MaskedLMOutput'>)\n             32 EXTENDED_ARG             1\n             34 LOAD_FAST              281 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             1\n             42 LOAD_FAST              281 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 LOAD_CONST               0 (None)\n             50 LOAD_CONST               0 (None)\n             52 LOAD_CONST               7 (('loss', 'logits', 'hidden_states', 'attentions'))\n             54 CALL_FUNCTION_KW         4\n             56 EXTENDED_ARG             1\n             58 LOAD_FAST              281 (graph_out_0)\n             60 LOAD_CONST               4 (0)\n             62 BINARY_SUBSCR\n             64 STORE_FAST               7 (loss)\n             66 STORE_FAST               6 (pred)\n\n559          68 CALL_FUNCTION            0\n             70 LOAD_GLOBAL             13 (__resume_at_100_4)\n             72 ROT_TWO\n             74 LOAD_FAST                1 (self)\n             76 LOAD_FAST                2 (mod)\n             78 LOAD_FAST                3 (collect_outputs)\n             80 LOAD_FAST                4 (cloned_inputs)\n             82 LOAD_FAST                6 (pred)\n             84 LOAD_FAST                7 (loss)\n             86 CALL_FUNCTION            7\n             88 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('start_positions')\n             12 BINARY_SUBSCR\n             14 LOAD_FAST                4 (cloned_inputs)\n             16 LOAD_CONST               4 ('end_positions')\n             18 BINARY_SUBSCR\n             20 CALL_FUNCTION            3\n             22 EXTENDED_ARG             1\n             24 STORE_FAST             290 (graph_out_0)\n             26 EXTENDED_ARG             1\n             28 LOAD_FAST              290 (graph_out_0)\n             30 LOAD_CONST               5 (0)\n             32 BINARY_SUBSCR\n             34 LOAD_ATTR                6 (backward)\n             36 LOAD_CONST               6 (<class 'transformers.modeling_outputs.QuestionAnsweringModelOutput'>)\n             38 EXTENDED_ARG             1\n             40 LOAD_FAST              290 (graph_out_0)\n             42 LOAD_CONST               5 (0)\n             44 BINARY_SUBSCR\n             46 EXTENDED_ARG             1\n             48 LOAD_FAST              290 (graph_out_0)\n             50 LOAD_CONST               7 (1)\n             52 BINARY_SUBSCR\n             54 EXTENDED_ARG             1\n             56 LOAD_FAST              290 (graph_out_0)\n             58 LOAD_CONST               8 (2)\n             60 BINARY_SUBSCR\n             62 LOAD_CONST               0 (None)\n             64 LOAD_CONST               0 (None)\n             66 LOAD_CONST               9 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n             68 CALL_FUNCTION_KW         5\n             70 EXTENDED_ARG             1\n             72 LOAD_FAST              290 (graph_out_0)\n             74 LOAD_CONST               5 (0)\n             76 BINARY_SUBSCR\n             78 STORE_FAST               7 (loss)\n             80 STORE_FAST               6 (pred)\n\n559          82 CALL_FUNCTION            0\n             84 LOAD_GLOBAL             13 (__resume_at_100_4)\n             86 ROT_TWO\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 LOAD_FAST                6 (pred)\n             98 LOAD_FAST                7 (loss)\n            100 CALL_FUNCTION            7\n            102 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             1\n             18 STORE_FAST             506 (graph_out_0)\n             20 EXTENDED_ARG             1\n             22 LOAD_FAST              506 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>)\n             32 EXTENDED_ARG             1\n             34 LOAD_FAST              506 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             1\n             42 LOAD_FAST              506 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 EXTENDED_ARG             1\n             50 LOAD_FAST              506 (graph_out_0)\n             52 LOAD_CONST               7 (2)\n             54 BINARY_SUBSCR\n             56 EXTENDED_ARG             1\n             58 LOAD_FAST              506 (graph_out_0)\n             60 LOAD_CONST               8 (3)\n             62 BINARY_SUBSCR\n             64 BUILD_TUPLE              2\n             66 EXTENDED_ARG             1\n             68 LOAD_FAST              506 (graph_out_0)\n             70 LOAD_CONST               9 (4)\n             72 BINARY_SUBSCR\n             74 EXTENDED_ARG             1\n             76 LOAD_FAST              506 (graph_out_0)\n             78 LOAD_CONST              10 (5)\n             80 BINARY_SUBSCR\n             82 BUILD_TUPLE              2\n             84 EXTENDED_ARG             1\n             86 LOAD_FAST              506 (graph_out_0)\n             88 LOAD_CONST              11 (6)\n             90 BINARY_SUBSCR\n             92 EXTENDED_ARG             1\n             94 LOAD_FAST              506 (graph_out_0)\n             96 LOAD_CONST              12 (7)\n             98 BINARY_SUBSCR\n            100 BUILD_TUPLE              2\n            102 EXTENDED_ARG             1\n            104 LOAD_FAST              506 (graph_out_0)\n            106 LOAD_CONST              13 (8)\n            108 BINARY_SUBSCR\n            110 EXTENDED_ARG             1\n            112 LOAD_FAST              506 (graph_out_0)\n            114 LOAD_CONST              14 (9)\n            116 BINARY_SUBSCR\n            118 BUILD_TUPLE              2\n            120 EXTENDED_ARG             1\n            122 LOAD_FAST              506 (graph_out_0)\n            124 LOAD_CONST              15 (10)\n            126 BINARY_SUBSCR\n            128 EXTENDED_ARG             1\n            130 LOAD_FAST              506 (graph_out_0)\n            132 LOAD_CONST              16 (11)\n            134 BINARY_SUBSCR\n            136 BUILD_TUPLE              2\n            138 EXTENDED_ARG             1\n            140 LOAD_FAST              506 (graph_out_0)\n            142 LOAD_CONST              17 (12)\n            144 BINARY_SUBSCR\n            146 EXTENDED_ARG             1\n            148 LOAD_FAST              506 (graph_out_0)\n            150 LOAD_CONST              18 (13)\n            152 BINARY_SUBSCR\n            154 BUILD_TUPLE              2\n            156 BUILD_TUPLE              6\n            158 LOAD_CONST               0 (None)\n            160 LOAD_CONST               0 (None)\n            162 LOAD_CONST               0 (None)\n            164 LOAD_CONST              19 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n            166 CALL_FUNCTION_KW         6\n            168 EXTENDED_ARG             1\n            170 LOAD_FAST              506 (graph_out_0)\n            172 LOAD_CONST               4 (0)\n            174 BINARY_SUBSCR\n            176 STORE_FAST               7 (loss)\n            178 STORE_FAST               6 (pred)\n\n559         180 CALL_FUNCTION            0\n            182 LOAD_GLOBAL             14 (__resume_at_100_4)\n            184 ROT_TWO\n            186 LOAD_FAST                1 (self)\n            188 LOAD_FAST                2 (mod)\n            190 LOAD_FAST                3 (collect_outputs)\n            192 LOAD_FAST                4 (cloned_inputs)\n            194 LOAD_FAST                6 (pred)\n            196 LOAD_FAST                7 (loss)\n            198 CALL_FUNCTION            7\n            200 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             14 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             2\n             18 STORE_FAST             758 (graph_out_0)\n             20 EXTENDED_ARG             2\n             22 LOAD_FAST              758 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>)\n             32 EXTENDED_ARG             2\n             34 LOAD_FAST              758 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             2\n             42 LOAD_FAST              758 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 LOAD_CONST               0 (None)\n             50 LOAD_CONST               0 (None)\n             52 LOAD_CONST               0 (None)\n             54 LOAD_CONST               0 (None)\n             56 LOAD_CONST               7 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n             58 CALL_FUNCTION_KW         6\n             60 EXTENDED_ARG             2\n             62 LOAD_FAST              758 (graph_out_0)\n             64 LOAD_CONST               4 (0)\n             66 BINARY_SUBSCR\n             68 STORE_FAST               7 (loss)\n             70 STORE_FAST               6 (pred)\n\n559          72 CALL_FUNCTION            0\n             74 LOAD_GLOBAL             15 (__resume_at_100_4)\n             76 ROT_TWO\n             78 LOAD_FAST                1 (self)\n             80 LOAD_FAST                2 (mod)\n             82 LOAD_FAST                3 (collect_outputs)\n             84 LOAD_FAST                4 (cloned_inputs)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_FAST                7 (loss)\n             90 CALL_FUNCTION            7\n             92 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('start_positions')\n             12 BINARY_SUBSCR\n             14 LOAD_FAST                4 (cloned_inputs)\n             16 LOAD_CONST               4 ('end_positions')\n             18 BINARY_SUBSCR\n             20 CALL_FUNCTION            3\n             22 EXTENDED_ARG             2\n             24 STORE_FAST             752 (graph_out_0)\n             26 EXTENDED_ARG             2\n             28 LOAD_FAST              752 (graph_out_0)\n             30 LOAD_CONST               5 (0)\n             32 BINARY_SUBSCR\n             34 LOAD_ATTR                6 (backward)\n             36 LOAD_CONST               6 (<class 'transformers.modeling_outputs.QuestionAnsweringModelOutput'>)\n             38 EXTENDED_ARG             2\n             40 LOAD_FAST              752 (graph_out_0)\n             42 LOAD_CONST               5 (0)\n             44 BINARY_SUBSCR\n             46 EXTENDED_ARG             2\n             48 LOAD_FAST              752 (graph_out_0)\n             50 LOAD_CONST               7 (1)\n             52 BINARY_SUBSCR\n             54 EXTENDED_ARG             2\n             56 LOAD_FAST              752 (graph_out_0)\n             58 LOAD_CONST               8 (2)\n             60 BINARY_SUBSCR\n             62 LOAD_CONST               0 (None)\n             64 LOAD_CONST               0 (None)\n             66 LOAD_CONST               9 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n             68 CALL_FUNCTION_KW         5\n             70 EXTENDED_ARG             2\n             72 LOAD_FAST              752 (graph_out_0)\n             74 LOAD_CONST               5 (0)\n             76 BINARY_SUBSCR\n             78 STORE_FAST               7 (loss)\n             80 STORE_FAST               6 (pred)\n\n559          82 CALL_FUNCTION            0\n             84 LOAD_GLOBAL             14 (__resume_at_100_4)\n             86 ROT_TWO\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 LOAD_FAST                6 (pred)\n             98 LOAD_FAST                7 (loss)\n            100 CALL_FUNCTION            7\n            102 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             3\n             18 STORE_FAST             942 (graph_out_0)\n             20 EXTENDED_ARG             3\n             22 LOAD_FAST              942 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.SequenceClassifierOutputWithPast'>)\n             32 EXTENDED_ARG             3\n             34 LOAD_FAST              942 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             3\n             42 LOAD_FAST              942 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 EXTENDED_ARG             3\n             50 LOAD_FAST              942 (graph_out_0)\n             52 LOAD_CONST               7 (2)\n             54 BINARY_SUBSCR\n             56 EXTENDED_ARG             3\n             58 LOAD_FAST              942 (graph_out_0)\n             60 LOAD_CONST               8 (3)\n             62 BINARY_SUBSCR\n             64 BUILD_TUPLE              2\n             66 EXTENDED_ARG             3\n             68 LOAD_FAST              942 (graph_out_0)\n             70 LOAD_CONST               9 (4)\n             72 BINARY_SUBSCR\n             74 EXTENDED_ARG             3\n             76 LOAD_FAST              942 (graph_out_0)\n             78 LOAD_CONST              10 (5)\n             80 BINARY_SUBSCR\n             82 BUILD_TUPLE              2\n             84 EXTENDED_ARG             3\n             86 LOAD_FAST              942 (graph_out_0)\n             88 LOAD_CONST              11 (6)\n             90 BINARY_SUBSCR\n             92 EXTENDED_ARG             3\n             94 LOAD_FAST              942 (graph_out_0)\n             96 LOAD_CONST              12 (7)\n             98 BINARY_SUBSCR\n            100 BUILD_TUPLE              2\n            102 EXTENDED_ARG             3\n            104 LOAD_FAST              942 (graph_out_0)\n            106 LOAD_CONST              13 (8)\n            108 BINARY_SUBSCR\n            110 EXTENDED_ARG             3\n            112 LOAD_FAST              942 (graph_out_0)\n            114 LOAD_CONST              14 (9)\n            116 BINARY_SUBSCR\n            118 BUILD_TUPLE              2\n            120 EXTENDED_ARG             3\n            122 LOAD_FAST              942 (graph_out_0)\n            124 LOAD_CONST              15 (10)\n            126 BINARY_SUBSCR\n            128 EXTENDED_ARG             3\n            130 LOAD_FAST              942 (graph_out_0)\n            132 LOAD_CONST              16 (11)\n            134 BINARY_SUBSCR\n            136 BUILD_TUPLE              2\n            138 EXTENDED_ARG             3\n            140 LOAD_FAST              942 (graph_out_0)\n            142 LOAD_CONST              17 (12)\n            144 BINARY_SUBSCR\n            146 EXTENDED_ARG             3\n            148 LOAD_FAST              942 (graph_out_0)\n            150 LOAD_CONST              18 (13)\n            152 BINARY_SUBSCR\n            154 BUILD_TUPLE              2\n            156 EXTENDED_ARG             3\n            158 LOAD_FAST              942 (graph_out_0)\n            160 LOAD_CONST              19 (14)\n            162 BINARY_SUBSCR\n            164 EXTENDED_ARG             3\n            166 LOAD_FAST              942 (graph_out_0)\n            168 LOAD_CONST              20 (15)\n            170 BINARY_SUBSCR\n            172 BUILD_TUPLE              2\n            174 EXTENDED_ARG             3\n            176 LOAD_FAST              942 (graph_out_0)\n            178 LOAD_CONST              21 (16)\n            180 BINARY_SUBSCR\n            182 EXTENDED_ARG             3\n            184 LOAD_FAST              942 (graph_out_0)\n            186 LOAD_CONST              22 (17)\n            188 BINARY_SUBSCR\n            190 BUILD_TUPLE              2\n            192 EXTENDED_ARG             3\n            194 LOAD_FAST              942 (graph_out_0)\n            196 LOAD_CONST              23 (18)\n            198 BINARY_SUBSCR\n            200 EXTENDED_ARG             3\n            202 LOAD_FAST              942 (graph_out_0)\n            204 LOAD_CONST              24 (19)\n            206 BINARY_SUBSCR\n            208 BUILD_TUPLE              2\n            210 EXTENDED_ARG             3\n            212 LOAD_FAST              942 (graph_out_0)\n            214 LOAD_CONST              25 (20)\n            216 BINARY_SUBSCR\n            218 EXTENDED_ARG             3\n            220 LOAD_FAST              942 (graph_out_0)\n            222 LOAD_CONST              26 (21)\n            224 BINARY_SUBSCR\n            226 BUILD_TUPLE              2\n            228 EXTENDED_ARG             3\n            230 LOAD_FAST              942 (graph_out_0)\n            232 LOAD_CONST              27 (22)\n            234 BINARY_SUBSCR\n            236 EXTENDED_ARG             3\n            238 LOAD_FAST              942 (graph_out_0)\n            240 LOAD_CONST              28 (23)\n            242 BINARY_SUBSCR\n            244 BUILD_TUPLE              2\n            246 EXTENDED_ARG             3\n            248 LOAD_FAST              942 (graph_out_0)\n            250 LOAD_CONST              29 (24)\n            252 BINARY_SUBSCR\n            254 EXTENDED_ARG             3\n            256 LOAD_FAST              942 (graph_out_0)\n            258 LOAD_CONST              30 (25)\n            260 BINARY_SUBSCR\n            262 BUILD_TUPLE              2\n            264 BUILD_TUPLE             12\n            266 LOAD_CONST               0 (None)\n            268 LOAD_CONST               0 (None)\n            270 LOAD_CONST              31 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions'))\n            272 CALL_FUNCTION_KW         5\n            274 EXTENDED_ARG             3\n            276 LOAD_FAST              942 (graph_out_0)\n            278 LOAD_CONST               4 (0)\n            280 BINARY_SUBSCR\n            282 STORE_FAST               7 (loss)\n            284 STORE_FAST               6 (pred)\n\n559         286 CALL_FUNCTION            0\n            288 LOAD_GLOBAL             14 (__resume_at_100_4)\n            290 ROT_TWO\n            292 LOAD_FAST                1 (self)\n            294 LOAD_FAST                2 (mod)\n            296 LOAD_FAST                3 (collect_outputs)\n            298 LOAD_FAST                4 (cloned_inputs)\n            300 LOAD_FAST                6 (pred)\n            302 LOAD_FAST                7 (loss)\n            304 CALL_FUNCTION            7\n            306 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             1\n             18 STORE_FAST             420 (graph_out_0)\n             20 EXTENDED_ARG             1\n             22 LOAD_FAST              420 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.MaskedLMOutput'>)\n             32 EXTENDED_ARG             1\n             34 LOAD_FAST              420 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             1\n             42 LOAD_FAST              420 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 LOAD_CONST               0 (None)\n             50 LOAD_CONST               0 (None)\n             52 LOAD_CONST               7 (('loss', 'logits', 'hidden_states', 'attentions'))\n             54 CALL_FUNCTION_KW         4\n             56 EXTENDED_ARG             1\n             58 LOAD_FAST              420 (graph_out_0)\n             60 LOAD_CONST               4 (0)\n             62 BINARY_SUBSCR\n             64 STORE_FAST               7 (loss)\n             66 STORE_FAST               6 (pred)\n\n559          68 CALL_FUNCTION            0\n             70 LOAD_GLOBAL             14 (__resume_at_100_4)\n             72 ROT_TWO\n             74 LOAD_FAST                1 (self)\n             76 LOAD_FAST                2 (mod)\n             78 LOAD_FAST                3 (collect_outputs)\n             80 LOAD_FAST                4 (cloned_inputs)\n             82 LOAD_FAST                6 (pred)\n             84 LOAD_FAST                7 (loss)\n             86 CALL_FUNCTION            7\n             88 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             3\n             18 STORE_FAST             773 (graph_out_0)\n             20 EXTENDED_ARG             3\n             22 LOAD_FAST              773 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.MaskedLMOutput'>)\n             32 EXTENDED_ARG             3\n             34 LOAD_FAST              773 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             3\n             42 LOAD_FAST              773 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 LOAD_CONST               0 (None)\n             50 LOAD_CONST               0 (None)\n             52 LOAD_CONST               7 (('loss', 'logits', 'hidden_states', 'attentions'))\n             54 CALL_FUNCTION_KW         4\n             56 EXTENDED_ARG             3\n             58 LOAD_FAST              773 (graph_out_0)\n             60 LOAD_CONST               4 (0)\n             62 BINARY_SUBSCR\n             64 STORE_FAST               7 (loss)\n             66 STORE_FAST               6 (pred)\n\n559          68 CALL_FUNCTION            0\n             70 LOAD_GLOBAL             14 (__resume_at_100_4)\n             72 ROT_TWO\n             74 LOAD_FAST                1 (self)\n             76 LOAD_FAST                2 (mod)\n             78 LOAD_FAST                3 (collect_outputs)\n             80 LOAD_FAST                4 (cloned_inputs)\n             82 LOAD_FAST                6 (pred)\n             84 LOAD_FAST                7 (loss)\n             86 CALL_FUNCTION            7\n             88 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             2\n             18 STORE_FAST             764 (graph_out_0)\n             20 EXTENDED_ARG             2\n             22 LOAD_FAST              764 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.SequenceClassifierOutput'>)\n             32 EXTENDED_ARG             2\n             34 LOAD_FAST              764 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             2\n             42 LOAD_FAST              764 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 LOAD_CONST               0 (None)\n             50 LOAD_CONST               0 (None)\n             52 LOAD_CONST               7 (('loss', 'logits', 'hidden_states', 'attentions'))\n             54 CALL_FUNCTION_KW         4\n             56 EXTENDED_ARG             2\n             58 LOAD_FAST              764 (graph_out_0)\n             60 LOAD_CONST               4 (0)\n             62 BINARY_SUBSCR\n             64 STORE_FAST               7 (loss)\n             66 STORE_FAST               6 (pred)\n\n559          68 CALL_FUNCTION            0\n             70 LOAD_GLOBAL             14 (__resume_at_100_4)\n             72 ROT_TWO\n             74 LOAD_FAST                1 (self)\n             76 LOAD_FAST                2 (mod)\n             78 LOAD_FAST                3 (collect_outputs)\n             80 LOAD_FAST                4 (cloned_inputs)\n             82 LOAD_FAST                6 (pred)\n             84 LOAD_FAST                7 (loss)\n             86 CALL_FUNCTION            7\n             88 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('decoder_input_ids')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('decoder_input_ids')\n             22 BINARY_SUBSCR\n             24 LOAD_CONST               4 ('labels')\n             26 LOAD_FAST                4 (cloned_inputs)\n             28 LOAD_CONST               4 ('labels')\n             30 BINARY_SUBSCR\n             32 BUILD_MAP                3\n             34 LOAD_GLOBAL             12 (__import_contextlib)\n             36 LOAD_ATTR               13 (nullcontext)\n             38 CALL_FUNCTION            0\n             40 STORE_FAST              39 (___context_manager_0_3)\n             42 LOAD_FAST               39 (___context_manager_0_3)\n             44 LOAD_METHOD             14 (__enter__)\n             46 CALL_METHOD              0\n             48 POP_TOP\n             50 SETUP_FINALLY           10 (to 72)\n\n557          52 CALL_FUNCTION_EX         1\n             54 POP_BLOCK\n             56 LOAD_FAST               39 (___context_manager_0_3)\n             58 LOAD_METHOD             15 (__exit__)\n             60 LOAD_CONST               0 (None)\n             62 DUP_TOP\n             64 DUP_TOP\n             66 CALL_METHOD              3\n             68 POP_TOP\n             70 JUMP_FORWARD             9 (to 90)\n        >>   72 NOP\n             74 LOAD_FAST               39 (___context_manager_0_3)\n             76 LOAD_METHOD             15 (__exit__)\n             78 LOAD_CONST               0 (None)\n             80 DUP_TOP\n             82 DUP_TOP\n             84 CALL_METHOD              3\n             86 POP_TOP\n             88 RERAISE                  0\n        >>   90 NOP\n             92 LOAD_GLOBAL             16 (__resume_at_44_4)\n             94 ROT_THREE\n             96 LOAD_FAST                1 (self)\n             98 LOAD_FAST                2 (mod)\n            100 LOAD_FAST                3 (collect_outputs)\n            102 LOAD_FAST                4 (cloned_inputs)\n            104 CALL_FUNCTION            6\n            106 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 1279 \n1279           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                5 (model)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                3 (decoder_input_ids)\n              10 LOAD_FAST                8 (encoder_outputs)\n              12 LOAD_FAST                4 (decoder_attention_mask)\n              14 LOAD_FAST                5 (head_mask)\n              16 LOAD_FAST                6 (decoder_head_mask)\n              18 LOAD_FAST                7 (cross_attn_head_mask)\n              20 LOAD_FAST                9 (past_key_values)\n              22 LOAD_FAST               10 (inputs_embeds)\n              24 LOAD_FAST               11 (decoder_inputs_embeds)\n              26 LOAD_FAST               13 (use_cache)\n              28 LOAD_FAST               14 (output_attentions)\n              30 LOAD_FAST               15 (output_hidden_states)\n              32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                0 (config)\n              36 LOAD_ATTR                1 (use_return_dict)\n              38 LOAD_CONST               2 (('attention_mask', 'decoder_input_ids', 'encoder_outputs', 'decoder_attention_mask', 'head_mask', 'decoder_head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'decoder_inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              40 LOAD_FAST                0 (self)\n              42 LOAD_ATTR                0 (config)\n              44 LOAD_ATTR                1 (use_return_dict)\n              46 STORE_FAST              16 (return_dict)\n\n1317          48 CALL_FUNCTION_KW        15\n              50 LOAD_GLOBAL             22 (__resume_at_94_5)\n              52 ROT_TWO\n              54 LOAD_FAST                0 (self)\n              56 LOAD_FAST               12 (labels)\n              58 LOAD_FAST               16 (return_dict)\n              60 CALL_FUNCTION            4\n              62 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 1168 \n1168           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                5 (encoder)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                5 (head_mask)\n              10 LOAD_FAST               10 (inputs_embeds)\n              12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (output_attentions)\n              18 LOAD_FAST                0 (self)\n              20 LOAD_ATTR                0 (config)\n              22 LOAD_ATTR                2 (output_hidden_states)\n              24 LOAD_FAST               15 (return_dict)\n              26 LOAD_CONST               1 (('input_ids', 'attention_mask', 'head_mask', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              28 LOAD_FAST                0 (self)\n              30 LOAD_ATTR                0 (config)\n              32 LOAD_ATTR                3 (use_cache)\n              34 LOAD_FAST                0 (self)\n              36 LOAD_ATTR                0 (config)\n              38 LOAD_ATTR                1 (output_attentions)\n              40 LOAD_FAST                0 (self)\n              42 LOAD_ATTR                0 (config)\n              44 LOAD_ATTR                2 (output_hidden_states)\n              46 STORE_FAST              14 (output_hidden_states)\n              48 STORE_FAST              13 (output_attentions)\n              50 STORE_FAST              12 (use_cache)\n\n1200          52 CALL_FUNCTION_KW         7\n              54 LOAD_GLOBAL             18 (__resume_at_110_6)\n              56 ROT_TWO\n              58 LOAD_FAST                0 (self)\n              60 LOAD_FAST                2 (attention_mask)\n              62 LOAD_FAST                3 (decoder_input_ids)\n              64 LOAD_FAST                4 (decoder_attention_mask)\n              66 LOAD_FAST                6 (decoder_head_mask)\n              68 LOAD_FAST                7 (cross_attn_head_mask)\n              70 LOAD_FAST                9 (past_key_values)\n              72 LOAD_FAST               11 (decoder_inputs_embeds)\n              74 LOAD_FAST               12 (use_cache)\n              76 LOAD_FAST               13 (output_attentions)\n              78 LOAD_FAST               14 (output_hidden_states)\n              80 LOAD_FAST               15 (return_dict)\n              82 CALL_FUNCTION           13\n              84 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 157 \n157           0 LOAD_GLOBAL             14 (__compiled_fn_7)\n              2 LOAD_FAST                1 (input_ids)\n              4 CALL_FUNCTION            1\n              6 UNPACK_SEQUENCE          1\n              8 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 369 \n369           0 LOAD_GLOBAL             21 (__compiled_fn_8)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              47 (graph_out_0)\n              8 LOAD_FAST               47 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 369 \n369           0 LOAD_GLOBAL             21 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              47 (graph_out_0)\n              8 LOAD_FAST               47 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 369 \n369           0 LOAD_GLOBAL             21 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              47 (graph_out_0)\n              8 LOAD_FAST               47 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 369 \n369           0 LOAD_GLOBAL             21 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              47 (graph_out_0)\n              8 LOAD_FAST               47 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 369 \n369           0 LOAD_GLOBAL             21 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              47 (graph_out_0)\n              8 LOAD_FAST               47 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 369 \n369           0 LOAD_GLOBAL             21 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              47 (graph_out_0)\n              8 LOAD_FAST               47 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 369 \n369           0 LOAD_GLOBAL             21 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              47 (graph_out_0)\n              8 LOAD_FAST               47 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 369 \n369           0 LOAD_GLOBAL             21 (__compiled_fn_15)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              47 (graph_out_0)\n              8 LOAD_FAST               47 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 369 \n369           0 LOAD_GLOBAL             21 (__compiled_fn_16)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              47 (graph_out_0)\n              8 LOAD_FAST               47 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 369 \n369           0 LOAD_GLOBAL             21 (__compiled_fn_17)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              47 (graph_out_0)\n              8 LOAD_FAST               47 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 369 \n369           0 LOAD_GLOBAL             21 (__compiled_fn_18)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              47 (graph_out_0)\n              8 LOAD_FAST               47 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 369 \n369           0 LOAD_GLOBAL             21 (__compiled_fn_19)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              47 (graph_out_0)\n              8 LOAD_FAST               47 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              4 (__resume_at_6_20)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7ff64a74aef0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_21')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7ff5befb2600, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_22')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 1200 \n1200           0 LOAD_FAST                1 (self)\n               2 LOAD_ATTR                9 (decoder)\n               4 LOAD_FAST                3 (decoder_input_ids)\n               6 LOAD_FAST                4 (decoder_attention_mask)\n               8 LOAD_FAST                0 (___stack0)\n              10 LOAD_ATTR               11 (last_hidden_state)\n              12 LOAD_FAST                2 (attention_mask)\n              14 LOAD_FAST                5 (decoder_head_mask)\n              16 LOAD_FAST                6 (cross_attn_head_mask)\n              18 LOAD_FAST                7 (past_key_values)\n              20 LOAD_FAST                8 (decoder_inputs_embeds)\n              22 LOAD_FAST                9 (use_cache)\n              24 LOAD_FAST               10 (output_attentions)\n              26 LOAD_FAST               11 (output_hidden_states)\n              28 LOAD_FAST               12 (return_dict)\n              30 LOAD_CONST               6 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              32 LOAD_FAST                0 (___stack0)\n              34 STORE_FAST              15 (encoder_outputs)\n\n1218          36 CALL_FUNCTION_KW        12\n              38 LOAD_GLOBAL             18 (__resume_at_226_23)\n              40 ROT_TWO\n              42 LOAD_FAST               12 (return_dict)\n              44 LOAD_FAST               15 (encoder_outputs)\n              46 CALL_FUNCTION            3\n              48 RETURN_VALUE\n\n", "MODIFIED BYTECODE _make_causal_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 75 \n 75           0 LOAD_GLOBAL             12 (__compiled_fn_24)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 157 \n157           0 LOAD_GLOBAL             14 (__compiled_fn_25)\n              2 LOAD_FAST                1 (input_ids)\n              4 CALL_FUNCTION            1\n              6 UNPACK_SEQUENCE          1\n              8 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 448 \n448           0 LOAD_GLOBAL             14 (__compiled_fn_26)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               94 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               94 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               94 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               94 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 448 \n448           0 LOAD_GLOBAL             14 (__compiled_fn_27)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               94 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               94 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               94 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               94 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 448 \n448           0 LOAD_GLOBAL             14 (__compiled_fn_28)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               94 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               94 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               94 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               94 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 448 \n448           0 LOAD_GLOBAL             14 (__compiled_fn_29)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               94 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               94 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               94 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               94 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 448 \n448           0 LOAD_GLOBAL             14 (__compiled_fn_30)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               94 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               94 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               94 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               94 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 448 \n448           0 LOAD_GLOBAL             14 (__compiled_fn_31)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               94 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               94 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               94 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               94 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 448 \n448           0 LOAD_GLOBAL             14 (__compiled_fn_32)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               94 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               94 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               94 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               94 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 448 \n448           0 LOAD_GLOBAL             14 (__compiled_fn_33)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               94 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               94 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               94 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               94 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 448 \n448           0 LOAD_GLOBAL             14 (__compiled_fn_34)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               94 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               94 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               94 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               94 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 448 \n448           0 LOAD_GLOBAL             14 (__compiled_fn_35)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               94 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               94 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               94 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               94 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 448 \n448           0 LOAD_GLOBAL             14 (__compiled_fn_36)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               94 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               94 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               94 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               94 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_37)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              6 (__resume_at_14_38)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7ff64a74aef0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_39')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7ff5befb2600, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_40')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7ff64a74aef0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_41')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7ff5befb2600, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_42')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              9 (__resume_at_6_43)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (decoder_hidden_states)\n             14 LOAD_FAST                4 (decoder_attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 LOAD_FAST                6 (encoder_last_hidden_state)\n             20 LOAD_FAST                7 (encoder_hidden_states)\n             22 LOAD_FAST                8 (encoder_attentions)\n             24 CALL_FUNCTION            8\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              9 (__resume_at_14_44)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (decoder_hidden_states)\n             12 LOAD_FAST                3 (decoder_attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 LOAD_FAST                5 (encoder_last_hidden_state)\n             18 LOAD_FAST                6 (encoder_hidden_states)\n             20 LOAD_FAST                7 (encoder_attentions)\n             22 CALL_FUNCTION            7\n             24 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 4 \n  4           0 LOAD_FAST                1 (decoder_hidden_states)\n              2 LOAD_FAST                0 (self)\n\n  5           4 STORE_ATTR               2 (decoder_hidden_states)\n              6 LOAD_GLOBAL              9 (__resume_at_20_45)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (decoder_attentions)\n             12 LOAD_FAST                3 (cross_attentions)\n             14 LOAD_FAST                4 (encoder_last_hidden_state)\n             16 LOAD_FAST                5 (encoder_hidden_states)\n             18 LOAD_FAST                6 (encoder_attentions)\n             20 CALL_FUNCTION            6\n             22 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 5 \n  5           0 LOAD_FAST                1 (decoder_attentions)\n              2 LOAD_FAST                0 (self)\n\n  6           4 STORE_ATTR               3 (decoder_attentions)\n              6 LOAD_GLOBAL              9 (__resume_at_26_46)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (cross_attentions)\n             12 LOAD_FAST                3 (encoder_last_hidden_state)\n             14 LOAD_FAST                4 (encoder_hidden_states)\n             16 LOAD_FAST                5 (encoder_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 6 \n  6           0 LOAD_FAST                1 (cross_attentions)\n              2 LOAD_FAST                0 (self)\n\n  7           4 STORE_ATTR               4 (cross_attentions)\n              6 LOAD_GLOBAL              9 (__resume_at_32_47)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (encoder_last_hidden_state)\n             12 LOAD_FAST                3 (encoder_hidden_states)\n             14 LOAD_FAST                4 (encoder_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 7 \n  7           0 LOAD_FAST                1 (encoder_last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  8           4 STORE_ATTR               5 (encoder_last_hidden_state)\n              6 LOAD_GLOBAL              9 (__resume_at_38_48)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (encoder_hidden_states)\n             12 LOAD_FAST                3 (encoder_attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7ff64a74aef0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_49')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7ff5befb2600, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_50')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7ff64a74aef0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_51')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7ff5befb2600, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_52')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (encoder_last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7ff64a74aef0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_53')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (encoder_last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7ff5befb2600, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_54')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py line 1317 \n1317           0 LOAD_GLOBAL             20 (__compiled_fn_55)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               21 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              29 (graph_out_0)\n              12 LOAD_CONST               7 (<class 'transformers.modeling_outputs.Seq2SeqLMOutput'>)\n              14 LOAD_FAST               29 (graph_out_0)\n              16 LOAD_CONST               3 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               29 (graph_out_0)\n              22 LOAD_CONST               5 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR               13 (past_key_values)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR               14 (decoder_hidden_states)\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               15 (decoder_attentions)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               16 (cross_attentions)\n              42 LOAD_FAST                0 (___stack0)\n              44 LOAD_ATTR               17 (encoder_last_hidden_state)\n              46 LOAD_FAST                0 (___stack0)\n              48 LOAD_ATTR               18 (encoder_hidden_states)\n              50 LOAD_FAST                0 (___stack0)\n              52 LOAD_ATTR               19 (encoder_attentions)\n              54 LOAD_CONST               8 (('loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'))\n              56 CALL_FUNCTION_KW         9\n              58 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_56)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             12 (__import_contextlib)\n             28 LOAD_ATTR               13 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              48 (___context_manager_0_3)\n             34 LOAD_FAST               48 (___context_manager_0_3)\n             36 LOAD_METHOD             14 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               48 (___context_manager_0_3)\n             50 LOAD_METHOD             15 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               48 (___context_manager_0_3)\n             68 LOAD_METHOD             15 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             16 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 1741 \n1741           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                4 (model)\n               4 LOAD_ATTR                5 (decoder)\n               6 LOAD_FAST                1 (input_ids)\n               8 LOAD_FAST                2 (attention_mask)\n              10 LOAD_FAST                3 (encoder_hidden_states)\n              12 LOAD_FAST                4 (encoder_attention_mask)\n              14 LOAD_FAST                5 (head_mask)\n              16 LOAD_FAST                6 (cross_attn_head_mask)\n              18 LOAD_FAST                7 (past_key_values)\n              20 LOAD_FAST                8 (inputs_embeds)\n              22 LOAD_FAST               10 (use_cache)\n              24 LOAD_FAST                0 (self)\n              26 LOAD_ATTR                0 (config)\n              28 LOAD_ATTR                1 (output_attentions)\n              30 LOAD_FAST                0 (self)\n              32 LOAD_ATTR                0 (config)\n              34 LOAD_ATTR                2 (output_hidden_states)\n              36 LOAD_FAST                0 (self)\n              38 LOAD_ATTR                0 (config)\n              40 LOAD_ATTR                3 (use_return_dict)\n              42 LOAD_CONST               2 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                3 (use_return_dict)\n              50 STORE_FAST              13 (return_dict)\n\n1850          52 CALL_FUNCTION_KW        12\n              54 LOAD_GLOBAL             19 (__resume_at_94_5)\n              56 ROT_TWO\n              58 LOAD_FAST                0 (self)\n              60 LOAD_FAST                9 (labels)\n              62 LOAD_FAST               13 (return_dict)\n              64 CALL_FUNCTION            4\n              66 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 910 \n910           0 LOAD_GLOBAL              5 (__compiled_fn_6)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 126 \n126           0 LOAD_GLOBAL             11 (__compiled_fn_7)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_8)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_15)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_16)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_17)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_18)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_19)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_20)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              6 (__resume_at_14_21)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f945958f890, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_22')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f945dc35e70, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_23')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f945958f890, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_24')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f945dc35e70, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_25')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 1850 \n1850           0 LOAD_GLOBAL             17 (__compiled_fn_26)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               18 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              26 (graph_out_0)\n              12 LOAD_CONST               7 (<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>)\n              14 LOAD_FAST               26 (graph_out_0)\n              16 LOAD_CONST               3 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               26 (graph_out_0)\n              22 LOAD_CONST               5 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR               13 (past_key_values)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR               14 (hidden_states)\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               15 (attentions)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               16 (cross_attentions)\n              42 LOAD_CONST               8 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n              44 CALL_FUNCTION_KW         6\n              46 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_27)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_28)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_29)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             12 (__import_contextlib)\n             28 LOAD_ATTR               13 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              42 (___context_manager_0_3)\n             34 LOAD_FAST               42 (___context_manager_0_3)\n             36 LOAD_METHOD             14 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               42 (___context_manager_0_3)\n             50 LOAD_METHOD             15 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               42 (___context_manager_0_3)\n             68 LOAD_METHOD             15 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             16 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 1317 \n1317           0 LOAD_GLOBAL             22 (__compiled_fn_5)\n               2 LOAD_FAST               12 (labels)\n               4 CALL_FUNCTION            1\n               6 STORE_FAST              54 (graph_out_0)\n               8 LOAD_FAST                0 (self)\n              10 LOAD_ATTR                6 (model)\n              12 LOAD_FAST                1 (input_ids)\n              14 LOAD_FAST                2 (attention_mask)\n              16 LOAD_FAST               54 (graph_out_0)\n              18 LOAD_CONST               5 (0)\n              20 BINARY_SUBSCR\n              22 LOAD_FAST                8 (encoder_outputs)\n              24 LOAD_FAST                4 (decoder_attention_mask)\n              26 LOAD_FAST                5 (head_mask)\n              28 LOAD_FAST                6 (decoder_head_mask)\n              30 LOAD_FAST                7 (cross_attn_head_mask)\n              32 LOAD_FAST                9 (past_key_values)\n              34 LOAD_FAST               10 (inputs_embeds)\n              36 LOAD_FAST               11 (decoder_inputs_embeds)\n              38 LOAD_CONST               3 (False)\n              40 LOAD_FAST               14 (output_attentions)\n              42 LOAD_FAST               15 (output_hidden_states)\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                1 (use_return_dict)\n              50 LOAD_CONST               4 (('attention_mask', 'decoder_input_ids', 'encoder_outputs', 'decoder_attention_mask', 'head_mask', 'decoder_head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'decoder_inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              52 LOAD_FAST                0 (self)\n              54 LOAD_ATTR                0 (config)\n              56 LOAD_ATTR                1 (use_return_dict)\n              58 STORE_FAST              16 (return_dict)\n\n1357          60 CALL_FUNCTION_KW        15\n              62 LOAD_GLOBAL             23 (__resume_at_114_6)\n              64 ROT_TWO\n              66 LOAD_FAST                0 (self)\n              68 LOAD_FAST               12 (labels)\n              70 LOAD_FAST               16 (return_dict)\n              72 CALL_FUNCTION            4\n              74 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 1187 \n1187           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                7 (encoder)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                5 (head_mask)\n              10 LOAD_FAST               10 (inputs_embeds)\n              12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (output_attentions)\n              18 LOAD_FAST                0 (self)\n              20 LOAD_ATTR                0 (config)\n              22 LOAD_ATTR                2 (output_hidden_states)\n              24 LOAD_FAST               15 (return_dict)\n              26 LOAD_CONST               1 (('input_ids', 'attention_mask', 'head_mask', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              28 LOAD_FAST                0 (self)\n              30 LOAD_ATTR                0 (config)\n              32 LOAD_ATTR                1 (output_attentions)\n              34 LOAD_FAST                0 (self)\n              36 LOAD_ATTR                0 (config)\n              38 LOAD_ATTR                2 (output_hidden_states)\n              40 STORE_FAST              14 (output_hidden_states)\n              42 STORE_FAST              13 (output_attentions)\n\n1225          44 CALL_FUNCTION_KW         7\n              46 LOAD_GLOBAL             20 (__resume_at_140_7)\n              48 ROT_TWO\n              50 LOAD_FAST                0 (self)\n              52 LOAD_FAST                2 (attention_mask)\n              54 LOAD_FAST                3 (decoder_input_ids)\n              56 LOAD_FAST                4 (decoder_attention_mask)\n              58 LOAD_FAST                6 (decoder_head_mask)\n              60 LOAD_FAST                7 (cross_attn_head_mask)\n              62 LOAD_FAST                9 (past_key_values)\n              64 LOAD_FAST               11 (decoder_inputs_embeds)\n              66 LOAD_FAST               12 (use_cache)\n              68 LOAD_FAST               13 (output_attentions)\n              70 LOAD_FAST               14 (output_hidden_states)\n              72 LOAD_FAST               15 (return_dict)\n              74 CALL_FUNCTION           13\n              76 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 126 \n126           0 LOAD_GLOBAL             11 (__compiled_fn_8)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_15)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_16)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_17)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_18)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_19)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_20)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              4 (__resume_at_6_21)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f825661faa0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_22')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f81d27be600, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_23')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 1225 \n1225           0 LOAD_FAST                1 (self)\n               2 LOAD_ATTR               11 (decoder)\n               4 LOAD_FAST                3 (decoder_input_ids)\n               6 LOAD_FAST                4 (decoder_attention_mask)\n               8 LOAD_FAST                0 (___stack0)\n              10 LOAD_ATTR               13 (last_hidden_state)\n              12 LOAD_FAST                2 (attention_mask)\n              14 LOAD_FAST                5 (decoder_head_mask)\n              16 LOAD_FAST                6 (cross_attn_head_mask)\n              18 LOAD_FAST                7 (past_key_values)\n              20 LOAD_FAST                8 (decoder_inputs_embeds)\n              22 LOAD_FAST                9 (use_cache)\n              24 LOAD_FAST               10 (output_attentions)\n              26 LOAD_FAST               11 (output_hidden_states)\n              28 LOAD_FAST               12 (return_dict)\n              30 LOAD_CONST               6 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              32 LOAD_FAST                0 (___stack0)\n              34 STORE_FAST              15 (encoder_outputs)\n\n1243          36 CALL_FUNCTION_KW        12\n              38 LOAD_GLOBAL             20 (__resume_at_256_24)\n              40 ROT_TWO\n              42 LOAD_FAST               12 (return_dict)\n              44 LOAD_FAST               15 (encoder_outputs)\n              46 CALL_FUNCTION            3\n              48 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 910 \n910           0 LOAD_GLOBAL              5 (__compiled_fn_25)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 126 \n126           0 LOAD_GLOBAL             11 (__compiled_fn_26)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_27)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_28)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_29)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_30)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_31)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_32)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_33)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_34)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_35)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_36)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_37)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 387 \n387           0 LOAD_GLOBAL             14 (__compiled_fn_38)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_39)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f825661faa0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_40')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f81d27be600, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_41')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              9 (__resume_at_6_42)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (decoder_hidden_states)\n             14 LOAD_FAST                4 (decoder_attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 LOAD_FAST                6 (encoder_last_hidden_state)\n             20 LOAD_FAST                7 (encoder_hidden_states)\n             22 LOAD_FAST                8 (encoder_attentions)\n             24 CALL_FUNCTION            8\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              9 (__resume_at_14_43)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (decoder_hidden_states)\n             12 LOAD_FAST                3 (decoder_attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 LOAD_FAST                5 (encoder_last_hidden_state)\n             18 LOAD_FAST                6 (encoder_hidden_states)\n             20 LOAD_FAST                7 (encoder_attentions)\n             22 CALL_FUNCTION            7\n             24 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 4 \n  4           0 LOAD_FAST                1 (decoder_hidden_states)\n              2 LOAD_FAST                0 (self)\n\n  5           4 STORE_ATTR               2 (decoder_hidden_states)\n              6 LOAD_GLOBAL              9 (__resume_at_20_44)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (decoder_attentions)\n             12 LOAD_FAST                3 (cross_attentions)\n             14 LOAD_FAST                4 (encoder_last_hidden_state)\n             16 LOAD_FAST                5 (encoder_hidden_states)\n             18 LOAD_FAST                6 (encoder_attentions)\n             20 CALL_FUNCTION            6\n             22 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 5 \n  5           0 LOAD_FAST                1 (decoder_attentions)\n              2 LOAD_FAST                0 (self)\n\n  6           4 STORE_ATTR               3 (decoder_attentions)\n              6 LOAD_GLOBAL              9 (__resume_at_26_45)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (cross_attentions)\n             12 LOAD_FAST                3 (encoder_last_hidden_state)\n             14 LOAD_FAST                4 (encoder_hidden_states)\n             16 LOAD_FAST                5 (encoder_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 6 \n  6           0 LOAD_FAST                1 (cross_attentions)\n              2 LOAD_FAST                0 (self)\n\n  7           4 STORE_ATTR               4 (cross_attentions)\n              6 LOAD_GLOBAL              9 (__resume_at_32_46)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (encoder_last_hidden_state)\n             12 LOAD_FAST                3 (encoder_hidden_states)\n             14 LOAD_FAST                4 (encoder_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 7 \n  7           0 LOAD_FAST                1 (encoder_last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  8           4 STORE_ATTR               5 (encoder_last_hidden_state)\n              6 LOAD_GLOBAL              9 (__resume_at_38_47)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (encoder_hidden_states)\n             12 LOAD_FAST                3 (encoder_attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f825661faa0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_48')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f81d27be600, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_49')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (encoder_last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f825661faa0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_50')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (encoder_last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f81d27be600, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_51')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py line 1357 \n1357           0 LOAD_GLOBAL             20 (__compiled_fn_52)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               21 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              28 (graph_out_0)\n              12 LOAD_CONST               9 (<class 'transformers.modeling_outputs.Seq2SeqLMOutput'>)\n              14 LOAD_FAST               28 (graph_out_0)\n              16 LOAD_CONST               5 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               28 (graph_out_0)\n              22 LOAD_CONST               7 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR               13 (past_key_values)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR               14 (decoder_hidden_states)\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               15 (decoder_attentions)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               16 (cross_attentions)\n              42 LOAD_FAST                0 (___stack0)\n              44 LOAD_ATTR               17 (encoder_last_hidden_state)\n              46 LOAD_FAST                0 (___stack0)\n              48 LOAD_ATTR               18 (encoder_hidden_states)\n              50 LOAD_FAST                0 (___stack0)\n              52 LOAD_ATTR               19 (encoder_attentions)\n              54 LOAD_CONST              10 (('loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'))\n              56 CALL_FUNCTION_KW         9\n              58 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_53)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_54)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_55)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             22 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('decoder_input_ids')\n             12 BINARY_SUBSCR\n             14 LOAD_FAST                4 (cloned_inputs)\n             16 LOAD_CONST               4 ('labels')\n             18 BINARY_SUBSCR\n             20 CALL_FUNCTION            3\n             22 EXTENDED_ARG             5\n             24 STORE_FAST            1475 (graph_out_0)\n             26 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n             28 LOAD_ATTR               14 (make_cell)\n             30 CALL_FUNCTION            0\n             32 EXTENDED_ARG             5\n             34 STORE_FAST            1524 (tmp_48)\n             36 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n             38 LOAD_ATTR               14 (make_cell)\n             40 CALL_FUNCTION            0\n             42 EXTENDED_ARG             5\n             44 STORE_FAST            1525 (tmp_49)\n             46 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n             48 LOAD_ATTR               14 (make_cell)\n             50 CALL_FUNCTION            0\n             52 EXTENDED_ARG             5\n             54 STORE_FAST            1526 (tmp_50)\n             56 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n             58 LOAD_ATTR               14 (make_cell)\n             60 CALL_FUNCTION            0\n             62 EXTENDED_ARG             5\n             64 STORE_FAST            1527 (tmp_51)\n             66 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n             68 LOAD_ATTR               14 (make_cell)\n             70 CALL_FUNCTION            0\n             72 EXTENDED_ARG             5\n             74 STORE_FAST            1528 (tmp_52)\n             76 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n             78 LOAD_ATTR               14 (make_cell)\n             80 CALL_FUNCTION            0\n             82 EXTENDED_ARG             5\n             84 STORE_FAST            1529 (tmp_53)\n             86 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n             88 LOAD_ATTR               14 (make_cell)\n             90 CALL_FUNCTION            0\n             92 EXTENDED_ARG             5\n             94 STORE_FAST            1530 (tmp_54)\n             96 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n             98 LOAD_ATTR               14 (make_cell)\n            100 CALL_FUNCTION            0\n            102 EXTENDED_ARG             5\n            104 STORE_FAST            1531 (tmp_55)\n            106 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            108 LOAD_ATTR               14 (make_cell)\n            110 CALL_FUNCTION            0\n            112 EXTENDED_ARG             5\n            114 STORE_FAST            1532 (tmp_56)\n            116 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            118 LOAD_ATTR               14 (make_cell)\n            120 CALL_FUNCTION            0\n            122 EXTENDED_ARG             5\n            124 STORE_FAST            1533 (tmp_57)\n            126 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            128 LOAD_ATTR               14 (make_cell)\n            130 CALL_FUNCTION            0\n            132 EXTENDED_ARG             5\n            134 STORE_FAST            1534 (tmp_58)\n            136 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            138 LOAD_ATTR               14 (make_cell)\n            140 CALL_FUNCTION            0\n            142 EXTENDED_ARG             5\n            144 STORE_FAST            1535 (tmp_59)\n            146 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            148 LOAD_ATTR               14 (make_cell)\n            150 CALL_FUNCTION            0\n            152 EXTENDED_ARG             6\n            154 STORE_FAST            1536 (tmp_60)\n            156 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            158 LOAD_ATTR               14 (make_cell)\n            160 CALL_FUNCTION            0\n            162 EXTENDED_ARG             6\n            164 STORE_FAST            1537 (tmp_61)\n            166 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            168 LOAD_ATTR               14 (make_cell)\n            170 CALL_FUNCTION            0\n            172 EXTENDED_ARG             6\n            174 STORE_FAST            1538 (tmp_62)\n            176 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            178 LOAD_ATTR               14 (make_cell)\n            180 CALL_FUNCTION            0\n            182 EXTENDED_ARG             6\n            184 STORE_FAST            1539 (tmp_63)\n            186 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            188 LOAD_ATTR               14 (make_cell)\n            190 CALL_FUNCTION            0\n            192 EXTENDED_ARG             6\n            194 STORE_FAST            1540 (tmp_64)\n            196 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            198 LOAD_ATTR               14 (make_cell)\n            200 CALL_FUNCTION            0\n            202 EXTENDED_ARG             6\n            204 STORE_FAST            1541 (tmp_65)\n            206 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            208 LOAD_ATTR               14 (make_cell)\n            210 CALL_FUNCTION            0\n            212 EXTENDED_ARG             6\n            214 STORE_FAST            1542 (tmp_66)\n            216 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            218 LOAD_ATTR               14 (make_cell)\n            220 CALL_FUNCTION            0\n            222 EXTENDED_ARG             6\n            224 STORE_FAST            1543 (tmp_67)\n            226 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            228 LOAD_ATTR               14 (make_cell)\n            230 CALL_FUNCTION            0\n            232 EXTENDED_ARG             6\n            234 STORE_FAST            1544 (tmp_68)\n            236 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            238 LOAD_ATTR               14 (make_cell)\n            240 CALL_FUNCTION            0\n            242 EXTENDED_ARG             6\n            244 STORE_FAST            1545 (tmp_69)\n            246 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            248 LOAD_ATTR               14 (make_cell)\n            250 CALL_FUNCTION            0\n            252 EXTENDED_ARG             6\n            254 STORE_FAST            1546 (tmp_70)\n            256 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            258 LOAD_ATTR               14 (make_cell)\n            260 CALL_FUNCTION            0\n            262 EXTENDED_ARG             6\n            264 STORE_FAST            1547 (tmp_71)\n            266 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            268 LOAD_ATTR               14 (make_cell)\n            270 CALL_FUNCTION            0\n            272 EXTENDED_ARG             6\n            274 STORE_FAST            1548 (tmp_72)\n            276 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            278 LOAD_ATTR               14 (make_cell)\n            280 CALL_FUNCTION            0\n            282 EXTENDED_ARG             6\n            284 STORE_FAST            1549 (tmp_73)\n            286 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            288 LOAD_ATTR               14 (make_cell)\n            290 CALL_FUNCTION            0\n            292 EXTENDED_ARG             6\n            294 STORE_FAST            1550 (tmp_74)\n            296 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            298 LOAD_ATTR               14 (make_cell)\n            300 CALL_FUNCTION            0\n            302 EXTENDED_ARG             6\n            304 STORE_FAST            1551 (tmp_75)\n            306 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            308 LOAD_ATTR               14 (make_cell)\n            310 CALL_FUNCTION            0\n            312 EXTENDED_ARG             6\n            314 STORE_FAST            1552 (tmp_76)\n            316 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            318 LOAD_ATTR               14 (make_cell)\n            320 CALL_FUNCTION            0\n            322 EXTENDED_ARG             6\n            324 STORE_FAST            1553 (tmp_77)\n            326 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            328 LOAD_ATTR               14 (make_cell)\n            330 CALL_FUNCTION            0\n            332 EXTENDED_ARG             6\n            334 STORE_FAST            1554 (tmp_78)\n            336 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            338 LOAD_ATTR               14 (make_cell)\n            340 CALL_FUNCTION            0\n            342 EXTENDED_ARG             6\n            344 STORE_FAST            1555 (tmp_79)\n            346 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            348 LOAD_ATTR               14 (make_cell)\n            350 CALL_FUNCTION            0\n            352 EXTENDED_ARG             6\n            354 STORE_FAST            1556 (tmp_80)\n            356 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            358 LOAD_ATTR               14 (make_cell)\n            360 CALL_FUNCTION            0\n            362 EXTENDED_ARG             6\n            364 STORE_FAST            1557 (tmp_81)\n            366 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            368 LOAD_ATTR               14 (make_cell)\n            370 CALL_FUNCTION            0\n            372 EXTENDED_ARG             6\n            374 STORE_FAST            1558 (tmp_82)\n            376 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            378 LOAD_ATTR               14 (make_cell)\n            380 CALL_FUNCTION            0\n            382 EXTENDED_ARG             6\n            384 STORE_FAST            1559 (tmp_83)\n            386 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            388 LOAD_ATTR               14 (make_cell)\n            390 CALL_FUNCTION            0\n            392 EXTENDED_ARG             6\n            394 STORE_FAST            1560 (tmp_84)\n            396 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            398 LOAD_ATTR               14 (make_cell)\n            400 CALL_FUNCTION            0\n            402 EXTENDED_ARG             6\n            404 STORE_FAST            1561 (tmp_85)\n            406 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            408 LOAD_ATTR               14 (make_cell)\n            410 CALL_FUNCTION            0\n            412 EXTENDED_ARG             6\n            414 STORE_FAST            1562 (tmp_86)\n            416 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            418 LOAD_ATTR               14 (make_cell)\n            420 CALL_FUNCTION            0\n            422 EXTENDED_ARG             6\n            424 STORE_FAST            1563 (tmp_87)\n            426 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            428 LOAD_ATTR               14 (make_cell)\n            430 CALL_FUNCTION            0\n            432 EXTENDED_ARG             6\n            434 STORE_FAST            1564 (tmp_88)\n            436 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            438 LOAD_ATTR               14 (make_cell)\n            440 CALL_FUNCTION            0\n            442 EXTENDED_ARG             6\n            444 STORE_FAST            1565 (tmp_89)\n            446 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            448 LOAD_ATTR               14 (make_cell)\n            450 CALL_FUNCTION            0\n            452 EXTENDED_ARG             6\n            454 STORE_FAST            1566 (tmp_90)\n            456 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            458 LOAD_ATTR               14 (make_cell)\n            460 CALL_FUNCTION            0\n            462 EXTENDED_ARG             6\n            464 STORE_FAST            1567 (tmp_91)\n            466 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            468 LOAD_ATTR               14 (make_cell)\n            470 CALL_FUNCTION            0\n            472 EXTENDED_ARG             6\n            474 STORE_FAST            1568 (tmp_92)\n            476 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            478 LOAD_ATTR               14 (make_cell)\n            480 CALL_FUNCTION            0\n            482 EXTENDED_ARG             6\n            484 STORE_FAST            1569 (tmp_93)\n            486 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            488 LOAD_ATTR               14 (make_cell)\n            490 CALL_FUNCTION            0\n            492 EXTENDED_ARG             6\n            494 STORE_FAST            1570 (tmp_94)\n            496 LOAD_GLOBAL             13 (__import_torch_dot__dynamo_dot_utils)\n            498 LOAD_ATTR               14 (make_cell)\n            500 CALL_FUNCTION            0\n            502 EXTENDED_ARG             6\n            504 STORE_FAST            1571 (tmp_95)\n            506 EXTENDED_ARG             5\n            508 LOAD_FAST             1475 (graph_out_0)\n            510 LOAD_CONST               5 (0)\n            512 BINARY_SUBSCR\n            514 LOAD_ATTR                6 (backward)\n            516 LOAD_CONST               6 (<class 'transformers.modeling_outputs.Seq2SeqLMOutput'>)\n            518 EXTENDED_ARG             5\n            520 LOAD_FAST             1475 (graph_out_0)\n            522 LOAD_CONST               5 (0)\n            524 BINARY_SUBSCR\n            526 EXTENDED_ARG             5\n            528 LOAD_FAST             1475 (graph_out_0)\n            530 LOAD_CONST               7 (1)\n            532 BINARY_SUBSCR\n            534 EXTENDED_ARG             5\n            536 LOAD_FAST             1475 (graph_out_0)\n            538 LOAD_CONST               8 (2)\n            540 BINARY_SUBSCR\n            542 EXTENDED_ARG             5\n            544 LOAD_FAST             1475 (graph_out_0)\n            546 LOAD_CONST               9 (3)\n            548 BINARY_SUBSCR\n            550 EXTENDED_ARG             5\n            552 LOAD_FAST             1475 (graph_out_0)\n            554 LOAD_CONST              10 (4)\n            556 BINARY_SUBSCR\n            558 EXTENDED_ARG             5\n            560 LOAD_FAST             1475 (graph_out_0)\n            562 LOAD_CONST              11 (5)\n            564 BINARY_SUBSCR\n            566 BUILD_TUPLE              4\n            568 EXTENDED_ARG             5\n            570 LOAD_FAST             1475 (graph_out_0)\n            572 LOAD_CONST              12 (6)\n            574 BINARY_SUBSCR\n            576 EXTENDED_ARG             5\n            578 LOAD_FAST             1475 (graph_out_0)\n            580 LOAD_CONST              13 (7)\n            582 BINARY_SUBSCR\n            584 EXTENDED_ARG             5\n            586 LOAD_FAST             1475 (graph_out_0)\n            588 LOAD_CONST              14 (8)\n            590 BINARY_SUBSCR\n            592 EXTENDED_ARG             5\n            594 LOAD_FAST             1475 (graph_out_0)\n            596 LOAD_CONST              15 (9)\n            598 BINARY_SUBSCR\n            600 BUILD_TUPLE              4\n            602 EXTENDED_ARG             5\n            604 LOAD_FAST             1475 (graph_out_0)\n            606 LOAD_CONST              16 (10)\n            608 BINARY_SUBSCR\n            610 EXTENDED_ARG             5\n            612 LOAD_FAST             1475 (graph_out_0)\n            614 LOAD_CONST              17 (11)\n            616 BINARY_SUBSCR\n            618 EXTENDED_ARG             5\n            620 LOAD_FAST             1475 (graph_out_0)\n            622 LOAD_CONST              18 (12)\n            624 BINARY_SUBSCR\n            626 EXTENDED_ARG             5\n            628 LOAD_FAST             1475 (graph_out_0)\n            630 LOAD_CONST              19 (13)\n            632 BINARY_SUBSCR\n            634 BUILD_TUPLE              4\n            636 EXTENDED_ARG             5\n            638 LOAD_FAST             1475 (graph_out_0)\n            640 LOAD_CONST              20 (14)\n            642 BINARY_SUBSCR\n            644 EXTENDED_ARG             5\n            646 LOAD_FAST             1475 (graph_out_0)\n            648 LOAD_CONST              21 (15)\n            650 BINARY_SUBSCR\n            652 EXTENDED_ARG             5\n            654 LOAD_FAST             1475 (graph_out_0)\n            656 LOAD_CONST              22 (16)\n            658 BINARY_SUBSCR\n            660 EXTENDED_ARG             5\n            662 LOAD_FAST             1475 (graph_out_0)\n            664 LOAD_CONST              23 (17)\n            666 BINARY_SUBSCR\n            668 BUILD_TUPLE              4\n            670 EXTENDED_ARG             5\n            672 LOAD_FAST             1475 (graph_out_0)\n            674 LOAD_CONST              24 (18)\n            676 BINARY_SUBSCR\n            678 EXTENDED_ARG             5\n            680 LOAD_FAST             1475 (graph_out_0)\n            682 LOAD_CONST              25 (19)\n            684 BINARY_SUBSCR\n            686 EXTENDED_ARG             5\n            688 LOAD_FAST             1475 (graph_out_0)\n            690 LOAD_CONST              26 (20)\n            692 BINARY_SUBSCR\n            694 EXTENDED_ARG             5\n            696 LOAD_FAST             1475 (graph_out_0)\n            698 LOAD_CONST              27 (21)\n            700 BINARY_SUBSCR\n            702 BUILD_TUPLE              4\n            704 EXTENDED_ARG             5\n            706 LOAD_FAST             1475 (graph_out_0)\n            708 LOAD_CONST              28 (22)\n            710 BINARY_SUBSCR\n            712 EXTENDED_ARG             5\n            714 LOAD_FAST             1475 (graph_out_0)\n            716 LOAD_CONST              29 (23)\n            718 BINARY_SUBSCR\n            720 EXTENDED_ARG             5\n            722 LOAD_FAST             1475 (graph_out_0)\n            724 LOAD_CONST              30 (24)\n            726 BINARY_SUBSCR\n            728 EXTENDED_ARG             5\n            730 LOAD_FAST             1475 (graph_out_0)\n            732 LOAD_CONST              31 (25)\n            734 BINARY_SUBSCR\n            736 BUILD_TUPLE              4\n            738 EXTENDED_ARG             5\n            740 LOAD_FAST             1475 (graph_out_0)\n            742 LOAD_CONST              32 (26)\n            744 BINARY_SUBSCR\n            746 EXTENDED_ARG             5\n            748 LOAD_FAST             1475 (graph_out_0)\n            750 LOAD_CONST              33 (27)\n            752 BINARY_SUBSCR\n            754 EXTENDED_ARG             5\n            756 LOAD_FAST             1475 (graph_out_0)\n            758 LOAD_CONST              34 (28)\n            760 BINARY_SUBSCR\n            762 EXTENDED_ARG             5\n            764 LOAD_FAST             1475 (graph_out_0)\n            766 LOAD_CONST              35 (29)\n            768 BINARY_SUBSCR\n            770 BUILD_TUPLE              4\n            772 EXTENDED_ARG             5\n            774 LOAD_FAST             1475 (graph_out_0)\n            776 LOAD_CONST              36 (30)\n            778 BINARY_SUBSCR\n            780 EXTENDED_ARG             5\n            782 LOAD_FAST             1475 (graph_out_0)\n            784 LOAD_CONST              37 (31)\n            786 BINARY_SUBSCR\n            788 EXTENDED_ARG             5\n            790 LOAD_FAST             1475 (graph_out_0)\n            792 LOAD_CONST              38 (32)\n            794 BINARY_SUBSCR\n            796 EXTENDED_ARG             5\n            798 LOAD_FAST             1475 (graph_out_0)\n            800 LOAD_CONST              39 (33)\n            802 BINARY_SUBSCR\n            804 BUILD_TUPLE              4\n            806 BUILD_TUPLE              8\n            808 LOAD_CONST               0 (None)\n            810 LOAD_CONST               0 (None)\n            812 LOAD_CONST               0 (None)\n            814 EXTENDED_ARG             5\n            816 LOAD_FAST             1475 (graph_out_0)\n            818 LOAD_CONST              40 (34)\n            820 BINARY_SUBSCR\n            822 LOAD_CONST               0 (None)\n            824 LOAD_CONST               0 (None)\n            826 LOAD_CONST              41 (('loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'))\n            828 CALL_FUNCTION_KW         9\n            830 EXTENDED_ARG             5\n            832 LOAD_FAST             1475 (graph_out_0)\n            834 LOAD_CONST               5 (0)\n            836 BINARY_SUBSCR\n            838 LOAD_CONST              22 (16)\n            840 EXTENDED_ARG             5\n            842 LOAD_FAST             1524 (tmp_48)\n            844 LOAD_FAST                2 (mod)\n            846 LOAD_ATTR               16 (encoder)\n            848 LOAD_ATTR               17 (block)\n            850 LOAD_CONST               5 (0)\n            852 BINARY_SUBSCR\n            854 LOAD_ATTR               18 (layer)\n            856 LOAD_CONST               5 (0)\n            858 BINARY_SUBSCR\n            860 LOAD_ATTR               19 (SelfAttention)\n            862 EXTENDED_ARG             5\n            864 LOAD_FAST             1525 (tmp_49)\n            866 LOAD_CONST              22 (16)\n            868 EXTENDED_ARG             5\n            870 LOAD_FAST             1526 (tmp_50)\n            872 LOAD_FAST                2 (mod)\n            874 LOAD_ATTR               16 (encoder)\n            876 LOAD_ATTR               17 (block)\n            878 LOAD_CONST               7 (1)\n            880 BINARY_SUBSCR\n            882 LOAD_ATTR               18 (layer)\n            884 LOAD_CONST               5 (0)\n            886 BINARY_SUBSCR\n            888 LOAD_ATTR               19 (SelfAttention)\n            890 EXTENDED_ARG             5\n            892 LOAD_FAST             1527 (tmp_51)\n            894 LOAD_CONST              22 (16)\n            896 EXTENDED_ARG             5\n            898 LOAD_FAST             1528 (tmp_52)\n            900 LOAD_FAST                2 (mod)\n            902 LOAD_ATTR               16 (encoder)\n            904 LOAD_ATTR               17 (block)\n            906 LOAD_CONST               8 (2)\n            908 BINARY_SUBSCR\n            910 LOAD_ATTR               18 (layer)\n            912 LOAD_CONST               5 (0)\n            914 BINARY_SUBSCR\n            916 LOAD_ATTR               19 (SelfAttention)\n            918 EXTENDED_ARG             5\n            920 LOAD_FAST             1529 (tmp_53)\n            922 LOAD_CONST              22 (16)\n            924 EXTENDED_ARG             5\n            926 LOAD_FAST             1530 (tmp_54)\n            928 LOAD_FAST                2 (mod)\n            930 LOAD_ATTR               16 (encoder)\n            932 LOAD_ATTR               17 (block)\n            934 LOAD_CONST               9 (3)\n            936 BINARY_SUBSCR\n            938 LOAD_ATTR               18 (layer)\n            940 LOAD_CONST               5 (0)\n            942 BINARY_SUBSCR\n            944 LOAD_ATTR               19 (SelfAttention)\n            946 EXTENDED_ARG             5\n            948 LOAD_FAST             1531 (tmp_55)\n            950 LOAD_CONST              22 (16)\n            952 EXTENDED_ARG             5\n            954 LOAD_FAST             1532 (tmp_56)\n            956 LOAD_FAST                2 (mod)\n            958 LOAD_ATTR               16 (encoder)\n            960 LOAD_ATTR               17 (block)\n            962 LOAD_CONST              10 (4)\n            964 BINARY_SUBSCR\n            966 LOAD_ATTR               18 (layer)\n            968 LOAD_CONST               5 (0)\n            970 BINARY_SUBSCR\n            972 LOAD_ATTR               19 (SelfAttention)\n            974 EXTENDED_ARG             5\n            976 LOAD_FAST             1533 (tmp_57)\n            978 LOAD_CONST              22 (16)\n            980 EXTENDED_ARG             5\n            982 LOAD_FAST             1534 (tmp_58)\n            984 LOAD_FAST                2 (mod)\n            986 LOAD_ATTR               16 (encoder)\n            988 LOAD_ATTR               17 (block)\n            990 LOAD_CONST              11 (5)\n            992 BINARY_SUBSCR\n            994 LOAD_ATTR               18 (layer)\n            996 LOAD_CONST               5 (0)\n            998 BINARY_SUBSCR\n           1000 LOAD_ATTR               19 (SelfAttention)\n           1002 EXTENDED_ARG             5\n           1004 LOAD_FAST             1535 (tmp_59)\n           1006 LOAD_CONST              22 (16)\n           1008 EXTENDED_ARG             6\n           1010 LOAD_FAST             1536 (tmp_60)\n           1012 LOAD_FAST                2 (mod)\n           1014 LOAD_ATTR               16 (encoder)\n           1016 LOAD_ATTR               17 (block)\n           1018 LOAD_CONST              12 (6)\n           1020 BINARY_SUBSCR\n           1022 LOAD_ATTR               18 (layer)\n           1024 LOAD_CONST               5 (0)\n           1026 BINARY_SUBSCR\n           1028 LOAD_ATTR               19 (SelfAttention)\n           1030 EXTENDED_ARG             6\n           1032 LOAD_FAST             1537 (tmp_61)\n           1034 LOAD_CONST              22 (16)\n           1036 EXTENDED_ARG             6\n           1038 LOAD_FAST             1538 (tmp_62)\n           1040 LOAD_FAST                2 (mod)\n           1042 LOAD_ATTR               16 (encoder)\n           1044 LOAD_ATTR               17 (block)\n           1046 LOAD_CONST              13 (7)\n           1048 BINARY_SUBSCR\n           1050 LOAD_ATTR               18 (layer)\n           1052 LOAD_CONST               5 (0)\n           1054 BINARY_SUBSCR\n           1056 LOAD_ATTR               19 (SelfAttention)\n           1058 EXTENDED_ARG             6\n           1060 LOAD_FAST             1539 (tmp_63)\n           1062 LOAD_CONST              22 (16)\n           1064 EXTENDED_ARG             6\n           1066 LOAD_FAST             1540 (tmp_64)\n           1068 LOAD_FAST                2 (mod)\n           1070 LOAD_ATTR               20 (decoder)\n           1072 LOAD_ATTR               17 (block)\n           1074 LOAD_CONST               5 (0)\n           1076 BINARY_SUBSCR\n           1078 LOAD_ATTR               18 (layer)\n           1080 LOAD_CONST               5 (0)\n           1082 BINARY_SUBSCR\n           1084 LOAD_ATTR               19 (SelfAttention)\n           1086 EXTENDED_ARG             6\n           1088 LOAD_FAST             1541 (tmp_65)\n           1090 LOAD_CONST              22 (16)\n           1092 EXTENDED_ARG             6\n           1094 LOAD_FAST             1542 (tmp_66)\n           1096 LOAD_FAST                2 (mod)\n           1098 LOAD_ATTR               20 (decoder)\n           1100 LOAD_ATTR               17 (block)\n           1102 LOAD_CONST               5 (0)\n           1104 BINARY_SUBSCR\n           1106 LOAD_ATTR               18 (layer)\n           1108 LOAD_CONST               7 (1)\n           1110 BINARY_SUBSCR\n           1112 LOAD_ATTR               21 (EncDecAttention)\n           1114 EXTENDED_ARG             6\n           1116 LOAD_FAST             1543 (tmp_67)\n           1118 LOAD_CONST              22 (16)\n           1120 EXTENDED_ARG             6\n           1122 LOAD_FAST             1544 (tmp_68)\n           1124 LOAD_FAST                2 (mod)\n           1126 LOAD_ATTR               20 (decoder)\n           1128 LOAD_ATTR               17 (block)\n           1130 LOAD_CONST               7 (1)\n           1132 BINARY_SUBSCR\n           1134 LOAD_ATTR               18 (layer)\n           1136 LOAD_CONST               5 (0)\n           1138 BINARY_SUBSCR\n           1140 LOAD_ATTR               19 (SelfAttention)\n           1142 EXTENDED_ARG             6\n           1144 LOAD_FAST             1545 (tmp_69)\n           1146 LOAD_CONST              22 (16)\n           1148 EXTENDED_ARG             6\n           1150 LOAD_FAST             1546 (tmp_70)\n           1152 LOAD_FAST                2 (mod)\n           1154 LOAD_ATTR               20 (decoder)\n           1156 LOAD_ATTR               17 (block)\n           1158 LOAD_CONST               7 (1)\n           1160 BINARY_SUBSCR\n           1162 LOAD_ATTR               18 (layer)\n           1164 LOAD_CONST               7 (1)\n           1166 BINARY_SUBSCR\n           1168 LOAD_ATTR               21 (EncDecAttention)\n           1170 EXTENDED_ARG             6\n           1172 LOAD_FAST             1547 (tmp_71)\n           1174 LOAD_CONST              22 (16)\n           1176 EXTENDED_ARG             6\n           1178 LOAD_FAST             1548 (tmp_72)\n           1180 LOAD_FAST                2 (mod)\n           1182 LOAD_ATTR               20 (decoder)\n           1184 LOAD_ATTR               17 (block)\n           1186 LOAD_CONST               8 (2)\n           1188 BINARY_SUBSCR\n           1190 LOAD_ATTR               18 (layer)\n           1192 LOAD_CONST               5 (0)\n           1194 BINARY_SUBSCR\n           1196 LOAD_ATTR               19 (SelfAttention)\n           1198 EXTENDED_ARG             6\n           1200 LOAD_FAST             1549 (tmp_73)\n           1202 LOAD_CONST              22 (16)\n           1204 EXTENDED_ARG             6\n           1206 LOAD_FAST             1550 (tmp_74)\n           1208 LOAD_FAST                2 (mod)\n           1210 LOAD_ATTR               20 (decoder)\n           1212 LOAD_ATTR               17 (block)\n           1214 LOAD_CONST               8 (2)\n           1216 BINARY_SUBSCR\n           1218 LOAD_ATTR               18 (layer)\n           1220 LOAD_CONST               7 (1)\n           1222 BINARY_SUBSCR\n           1224 LOAD_ATTR               21 (EncDecAttention)\n           1226 EXTENDED_ARG             6\n           1228 LOAD_FAST             1551 (tmp_75)\n           1230 LOAD_CONST              22 (16)\n           1232 EXTENDED_ARG             6\n           1234 LOAD_FAST             1552 (tmp_76)\n           1236 LOAD_FAST                2 (mod)\n           1238 LOAD_ATTR               20 (decoder)\n           1240 LOAD_ATTR               17 (block)\n           1242 LOAD_CONST               9 (3)\n           1244 BINARY_SUBSCR\n           1246 LOAD_ATTR               18 (layer)\n           1248 LOAD_CONST               5 (0)\n           1250 BINARY_SUBSCR\n           1252 LOAD_ATTR               19 (SelfAttention)\n           1254 EXTENDED_ARG             6\n           1256 LOAD_FAST             1553 (tmp_77)\n           1258 LOAD_CONST              22 (16)\n           1260 EXTENDED_ARG             6\n           1262 LOAD_FAST             1554 (tmp_78)\n           1264 LOAD_FAST                2 (mod)\n           1266 LOAD_ATTR               20 (decoder)\n           1268 LOAD_ATTR               17 (block)\n           1270 LOAD_CONST               9 (3)\n           1272 BINARY_SUBSCR\n           1274 LOAD_ATTR               18 (layer)\n           1276 LOAD_CONST               7 (1)\n           1278 BINARY_SUBSCR\n           1280 LOAD_ATTR               21 (EncDecAttention)\n           1282 EXTENDED_ARG             6\n           1284 LOAD_FAST             1555 (tmp_79)\n           1286 LOAD_CONST              22 (16)\n           1288 EXTENDED_ARG             6\n           1290 LOAD_FAST             1556 (tmp_80)\n           1292 LOAD_FAST                2 (mod)\n           1294 LOAD_ATTR               20 (decoder)\n           1296 LOAD_ATTR               17 (block)\n           1298 LOAD_CONST              10 (4)\n           1300 BINARY_SUBSCR\n           1302 LOAD_ATTR               18 (layer)\n           1304 LOAD_CONST               5 (0)\n           1306 BINARY_SUBSCR\n           1308 LOAD_ATTR               19 (SelfAttention)\n           1310 EXTENDED_ARG             6\n           1312 LOAD_FAST             1557 (tmp_81)\n           1314 LOAD_CONST              22 (16)\n           1316 EXTENDED_ARG             6\n           1318 LOAD_FAST             1558 (tmp_82)\n           1320 LOAD_FAST                2 (mod)\n           1322 LOAD_ATTR               20 (decoder)\n           1324 LOAD_ATTR               17 (block)\n           1326 LOAD_CONST              10 (4)\n           1328 BINARY_SUBSCR\n           1330 LOAD_ATTR               18 (layer)\n           1332 LOAD_CONST               7 (1)\n           1334 BINARY_SUBSCR\n           1336 LOAD_ATTR               21 (EncDecAttention)\n           1338 EXTENDED_ARG             6\n           1340 LOAD_FAST             1559 (tmp_83)\n           1342 LOAD_CONST              22 (16)\n           1344 EXTENDED_ARG             6\n           1346 LOAD_FAST             1560 (tmp_84)\n           1348 LOAD_FAST                2 (mod)\n           1350 LOAD_ATTR               20 (decoder)\n           1352 LOAD_ATTR               17 (block)\n           1354 LOAD_CONST              11 (5)\n           1356 BINARY_SUBSCR\n           1358 LOAD_ATTR               18 (layer)\n           1360 LOAD_CONST               5 (0)\n           1362 BINARY_SUBSCR\n           1364 LOAD_ATTR               19 (SelfAttention)\n           1366 EXTENDED_ARG             6\n           1368 LOAD_FAST             1561 (tmp_85)\n           1370 LOAD_CONST              22 (16)\n           1372 EXTENDED_ARG             6\n           1374 LOAD_FAST             1562 (tmp_86)\n           1376 LOAD_FAST                2 (mod)\n           1378 LOAD_ATTR               20 (decoder)\n           1380 LOAD_ATTR               17 (block)\n           1382 LOAD_CONST              11 (5)\n           1384 BINARY_SUBSCR\n           1386 LOAD_ATTR               18 (layer)\n           1388 LOAD_CONST               7 (1)\n           1390 BINARY_SUBSCR\n           1392 LOAD_ATTR               21 (EncDecAttention)\n           1394 EXTENDED_ARG             6\n           1396 LOAD_FAST             1563 (tmp_87)\n           1398 LOAD_CONST              22 (16)\n           1400 EXTENDED_ARG             6\n           1402 LOAD_FAST             1564 (tmp_88)\n           1404 LOAD_FAST                2 (mod)\n           1406 LOAD_ATTR               20 (decoder)\n           1408 LOAD_ATTR               17 (block)\n           1410 LOAD_CONST              12 (6)\n           1412 BINARY_SUBSCR\n           1414 LOAD_ATTR               18 (layer)\n           1416 LOAD_CONST               5 (0)\n           1418 BINARY_SUBSCR\n           1420 LOAD_ATTR               19 (SelfAttention)\n           1422 EXTENDED_ARG             6\n           1424 LOAD_FAST             1565 (tmp_89)\n           1426 LOAD_CONST              22 (16)\n           1428 EXTENDED_ARG             6\n           1430 LOAD_FAST             1566 (tmp_90)\n           1432 LOAD_FAST                2 (mod)\n           1434 LOAD_ATTR               20 (decoder)\n           1436 LOAD_ATTR               17 (block)\n           1438 LOAD_CONST              12 (6)\n           1440 BINARY_SUBSCR\n           1442 LOAD_ATTR               18 (layer)\n           1444 LOAD_CONST               7 (1)\n           1446 BINARY_SUBSCR\n           1448 LOAD_ATTR               21 (EncDecAttention)\n           1450 EXTENDED_ARG             6\n           1452 LOAD_FAST             1567 (tmp_91)\n           1454 LOAD_CONST              22 (16)\n           1456 EXTENDED_ARG             6\n           1458 LOAD_FAST             1568 (tmp_92)\n           1460 LOAD_FAST                2 (mod)\n           1462 LOAD_ATTR               20 (decoder)\n           1464 LOAD_ATTR               17 (block)\n           1466 LOAD_CONST              13 (7)\n           1468 BINARY_SUBSCR\n           1470 LOAD_ATTR               18 (layer)\n           1472 LOAD_CONST               5 (0)\n           1474 BINARY_SUBSCR\n           1476 LOAD_ATTR               19 (SelfAttention)\n           1478 EXTENDED_ARG             6\n           1480 LOAD_FAST             1569 (tmp_93)\n           1482 LOAD_CONST              22 (16)\n           1484 EXTENDED_ARG             6\n           1486 LOAD_FAST             1570 (tmp_94)\n           1488 LOAD_FAST                2 (mod)\n           1490 LOAD_ATTR               20 (decoder)\n           1492 LOAD_ATTR               17 (block)\n           1494 LOAD_CONST              13 (7)\n           1496 BINARY_SUBSCR\n           1498 LOAD_ATTR               18 (layer)\n           1500 LOAD_CONST               7 (1)\n           1502 BINARY_SUBSCR\n           1504 LOAD_ATTR               21 (EncDecAttention)\n           1506 EXTENDED_ARG             6\n           1508 LOAD_FAST             1571 (tmp_95)\n           1510 STORE_ATTR              15 (cell_contents)\n           1512 STORE_ATTR              15 (cell_contents)\n           1514 STORE_ATTR              15 (cell_contents)\n           1516 STORE_ATTR              15 (cell_contents)\n           1518 STORE_ATTR              15 (cell_contents)\n           1520 STORE_ATTR              15 (cell_contents)\n           1522 STORE_ATTR              15 (cell_contents)\n           1524 STORE_ATTR              15 (cell_contents)\n           1526 STORE_ATTR              15 (cell_contents)\n           1528 STORE_ATTR              15 (cell_contents)\n           1530 STORE_ATTR              15 (cell_contents)\n           1532 STORE_ATTR              15 (cell_contents)\n           1534 STORE_ATTR              15 (cell_contents)\n           1536 STORE_ATTR              15 (cell_contents)\n           1538 STORE_ATTR              15 (cell_contents)\n           1540 STORE_ATTR              15 (cell_contents)\n           1542 STORE_ATTR              15 (cell_contents)\n           1544 STORE_ATTR              15 (cell_contents)\n           1546 STORE_ATTR              15 (cell_contents)\n           1548 STORE_ATTR              15 (cell_contents)\n           1550 STORE_ATTR              15 (cell_contents)\n           1552 STORE_ATTR              15 (cell_contents)\n           1554 STORE_ATTR              15 (cell_contents)\n           1556 STORE_ATTR              15 (cell_contents)\n           1558 STORE_ATTR              15 (cell_contents)\n           1560 STORE_ATTR              15 (cell_contents)\n           1562 STORE_ATTR              15 (cell_contents)\n           1564 STORE_ATTR              15 (cell_contents)\n           1566 STORE_ATTR              15 (cell_contents)\n           1568 STORE_ATTR              15 (cell_contents)\n           1570 STORE_ATTR              15 (cell_contents)\n           1572 STORE_ATTR              15 (cell_contents)\n           1574 STORE_ATTR              15 (cell_contents)\n           1576 STORE_ATTR              15 (cell_contents)\n           1578 STORE_ATTR              15 (cell_contents)\n           1580 STORE_ATTR              15 (cell_contents)\n           1582 STORE_ATTR              15 (cell_contents)\n           1584 STORE_ATTR              15 (cell_contents)\n           1586 STORE_ATTR              15 (cell_contents)\n           1588 STORE_ATTR              15 (cell_contents)\n           1590 STORE_ATTR              15 (cell_contents)\n           1592 STORE_ATTR              15 (cell_contents)\n           1594 STORE_ATTR              15 (cell_contents)\n           1596 STORE_ATTR              15 (cell_contents)\n           1598 STORE_ATTR              15 (cell_contents)\n           1600 STORE_ATTR              15 (cell_contents)\n           1602 STORE_ATTR              15 (cell_contents)\n           1604 STORE_ATTR              15 (cell_contents)\n           1606 STORE_FAST               7 (loss)\n           1608 STORE_FAST               6 (pred)\n\n559        1610 CALL_FUNCTION            0\n           1612 LOAD_GLOBAL             23 (__resume_at_100_4)\n           1614 ROT_TWO\n           1616 LOAD_FAST                1 (self)\n           1618 LOAD_FAST                2 (mod)\n           1620 LOAD_FAST                3 (collect_outputs)\n           1622 LOAD_FAST                4 (cloned_inputs)\n           1624 LOAD_FAST                6 (pred)\n           1626 LOAD_FAST                7 (loss)\n           1628 CALL_FUNCTION            7\n           1630 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             5\n             18 STORE_FAST            1436 (graph_out_0)\n             20 EXTENDED_ARG             5\n             22 LOAD_FAST             1436 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>)\n             32 EXTENDED_ARG             5\n             34 LOAD_FAST             1436 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             5\n             42 LOAD_FAST             1436 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 LOAD_CONST               0 (None)\n             50 LOAD_CONST               0 (None)\n             52 LOAD_CONST               0 (None)\n             54 LOAD_CONST               0 (None)\n             56 LOAD_CONST               7 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n             58 CALL_FUNCTION_KW         6\n             60 EXTENDED_ARG             5\n             62 LOAD_FAST             1436 (graph_out_0)\n             64 LOAD_CONST               4 (0)\n             66 BINARY_SUBSCR\n             68 STORE_FAST               7 (loss)\n             70 STORE_FAST               6 (pred)\n\n559          72 CALL_FUNCTION            0\n             74 LOAD_GLOBAL             14 (__resume_at_100_4)\n             76 ROT_TWO\n             78 LOAD_FAST                1 (self)\n             80 LOAD_FAST                2 (mod)\n             82 LOAD_FAST                3 (collect_outputs)\n             84 LOAD_FAST                4 (cloned_inputs)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_FAST                7 (loss)\n             90 CALL_FUNCTION            7\n             92 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('start_positions')\n             12 BINARY_SUBSCR\n             14 LOAD_FAST                4 (cloned_inputs)\n             16 LOAD_CONST               4 ('end_positions')\n             18 BINARY_SUBSCR\n             20 CALL_FUNCTION            3\n             22 EXTENDED_ARG             5\n             24 STORE_FAST            1428 (graph_out_0)\n             26 EXTENDED_ARG             5\n             28 LOAD_FAST             1428 (graph_out_0)\n             30 LOAD_CONST               5 (0)\n             32 BINARY_SUBSCR\n             34 LOAD_ATTR                6 (backward)\n             36 LOAD_CONST               6 (<class 'transformers.modeling_outputs.QuestionAnsweringModelOutput'>)\n             38 EXTENDED_ARG             5\n             40 LOAD_FAST             1428 (graph_out_0)\n             42 LOAD_CONST               5 (0)\n             44 BINARY_SUBSCR\n             46 EXTENDED_ARG             5\n             48 LOAD_FAST             1428 (graph_out_0)\n             50 LOAD_CONST               7 (1)\n             52 BINARY_SUBSCR\n             54 EXTENDED_ARG             5\n             56 LOAD_FAST             1428 (graph_out_0)\n             58 LOAD_CONST               8 (2)\n             60 BINARY_SUBSCR\n             62 LOAD_CONST               0 (None)\n             64 LOAD_CONST               0 (None)\n             66 LOAD_CONST               9 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n             68 CALL_FUNCTION_KW         5\n             70 EXTENDED_ARG             5\n             72 LOAD_FAST             1428 (graph_out_0)\n             74 LOAD_CONST               5 (0)\n             76 BINARY_SUBSCR\n             78 STORE_FAST               7 (loss)\n             80 STORE_FAST               6 (pred)\n\n559          82 CALL_FUNCTION            0\n             84 LOAD_GLOBAL             14 (__resume_at_100_4)\n             86 ROT_TWO\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 LOAD_FAST                6 (pred)\n             98 LOAD_FAST                7 (loss)\n            100 CALL_FUNCTION            7\n            102 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                1 (mod)\n               2 LOAD_ATTR                1 (zero_grad)\n               4 LOAD_CONST               1 (True)\n\n1879           6 CALL_FUNCTION            1\n               8 LOAD_GLOBAL              2 (__resume_at_34_2)\n              10 ROT_TWO\n              12 CALL_FUNCTION            1\n              14 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG            10\n             18 STORE_FAST            2804 (graph_out_0)\n             20 EXTENDED_ARG            10\n             22 LOAD_FAST             2804 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.MaskedLMOutput'>)\n             32 EXTENDED_ARG            10\n             34 LOAD_FAST             2804 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG            10\n             42 LOAD_FAST             2804 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 LOAD_CONST               0 (None)\n             50 LOAD_CONST               0 (None)\n             52 LOAD_CONST               7 (('loss', 'logits', 'hidden_states', 'attentions'))\n             54 CALL_FUNCTION_KW         4\n             56 EXTENDED_ARG            10\n             58 LOAD_FAST             2804 (graph_out_0)\n             60 LOAD_CONST               4 (0)\n             62 BINARY_SUBSCR\n             64 STORE_FAST               7 (loss)\n             66 STORE_FAST               6 (pred)\n\n559          68 CALL_FUNCTION            0\n             70 LOAD_GLOBAL             13 (__resume_at_100_4)\n             72 ROT_TWO\n             74 LOAD_FAST                1 (self)\n             76 LOAD_FAST                2 (mod)\n             78 LOAD_FAST                3 (collect_outputs)\n             80 LOAD_FAST                4 (cloned_inputs)\n             82 LOAD_FAST                6 (pred)\n             84 LOAD_FAST                7 (loss)\n             86 CALL_FUNCTION            7\n             88 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                2 (mod)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_FAST                5 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                4 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                1 (mod)\n               2 LOAD_ATTR                1 (zero_grad)\n               4 LOAD_CONST               1 (True)\n\n1879           6 CALL_FUNCTION            1\n               8 LOAD_GLOBAL              2 (__resume_at_34_2)\n              10 ROT_TWO\n              12 CALL_FUNCTION            1\n              14 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('start_positions')\n             12 BINARY_SUBSCR\n             14 LOAD_FAST                4 (cloned_inputs)\n             16 LOAD_CONST               4 ('end_positions')\n             18 BINARY_SUBSCR\n             20 CALL_FUNCTION            3\n             22 EXTENDED_ARG            10\n             24 STORE_FAST            2805 (graph_out_0)\n             26 EXTENDED_ARG            10\n             28 LOAD_FAST             2805 (graph_out_0)\n             30 LOAD_CONST               5 (0)\n             32 BINARY_SUBSCR\n             34 LOAD_ATTR                6 (backward)\n             36 LOAD_CONST               6 (<class 'transformers.modeling_outputs.QuestionAnsweringModelOutput'>)\n             38 EXTENDED_ARG            10\n             40 LOAD_FAST             2805 (graph_out_0)\n             42 LOAD_CONST               5 (0)\n             44 BINARY_SUBSCR\n             46 EXTENDED_ARG            10\n             48 LOAD_FAST             2805 (graph_out_0)\n             50 LOAD_CONST               7 (1)\n             52 BINARY_SUBSCR\n             54 EXTENDED_ARG            10\n             56 LOAD_FAST             2805 (graph_out_0)\n             58 LOAD_CONST               8 (2)\n             60 BINARY_SUBSCR\n             62 LOAD_CONST               0 (None)\n             64 LOAD_CONST               0 (None)\n             66 LOAD_CONST               9 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n             68 CALL_FUNCTION_KW         5\n             70 EXTENDED_ARG            10\n             72 LOAD_FAST             2805 (graph_out_0)\n             74 LOAD_CONST               5 (0)\n             76 BINARY_SUBSCR\n             78 STORE_FAST               7 (loss)\n             80 STORE_FAST               6 (pred)\n\n559          82 CALL_FUNCTION            0\n             84 LOAD_GLOBAL             13 (__resume_at_100_4)\n             86 ROT_TWO\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 LOAD_FAST                6 (pred)\n             98 LOAD_FAST                7 (loss)\n            100 CALL_FUNCTION            7\n            102 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                2 (mod)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_FAST                5 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                4 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             12 (__import_contextlib)\n             28 LOAD_ATTR               13 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              54 (___context_manager_0_3)\n             34 LOAD_FAST               54 (___context_manager_0_3)\n             36 LOAD_METHOD             14 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               54 (___context_manager_0_3)\n             50 LOAD_METHOD             15 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               54 (___context_manager_0_3)\n             68 LOAD_METHOD             15 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             16 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 849 \n849           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                4 (model)\n              4 LOAD_ATTR                5 (decoder)\n              6 LOAD_FAST                1 (input_ids)\n              8 LOAD_FAST                2 (attention_mask)\n             10 LOAD_FAST                3 (head_mask)\n             12 LOAD_FAST                4 (past_key_values)\n             14 LOAD_FAST                5 (inputs_embeds)\n             16 LOAD_FAST                7 (use_cache)\n             18 LOAD_FAST                0 (self)\n             20 LOAD_ATTR                0 (config)\n             22 LOAD_ATTR                1 (output_attentions)\n             24 LOAD_FAST                0 (self)\n             26 LOAD_ATTR                0 (config)\n             28 LOAD_ATTR                2 (output_hidden_states)\n             30 LOAD_FAST                0 (self)\n             32 LOAD_ATTR                0 (config)\n             34 LOAD_ATTR                3 (use_return_dict)\n             36 LOAD_CONST               2 (('input_ids', 'attention_mask', 'head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n             38 LOAD_FAST                0 (self)\n             40 LOAD_ATTR                0 (config)\n             42 LOAD_ATTR                3 (use_return_dict)\n             44 STORE_FAST              10 (return_dict)\n\n944          46 CALL_FUNCTION_KW         9\n             48 LOAD_GLOBAL             19 (__resume_at_88_5)\n             50 ROT_TWO\n             52 LOAD_FAST                0 (self)\n             54 LOAD_FAST                6 (labels)\n             56 LOAD_FAST               10 (return_dict)\n             58 CALL_FUNCTION            4\n             60 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 533 \n533           0 LOAD_GLOBAL              5 (__compiled_fn_6)\n              2 LOAD_FAST                1 (attention_mask)\n              4 CALL_FUNCTION            1\n              6 UNPACK_SEQUENCE          1\n              8 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 109 \n109           0 LOAD_GLOBAL              8 (__compiled_fn_7)\n              2 LOAD_FAST                1 (attention_mask)\n              4 CALL_FUNCTION            1\n              6 UNPACK_SEQUENCE          1\n              8 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 298 \n298           0 LOAD_GLOBAL             16 (__compiled_fn_8)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              56 (graph_out_0)\n             10 LOAD_FAST               56 (graph_out_0)\n             12 LOAD_CONST               4 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               56 (graph_out_0)\n             18 LOAD_CONST               5 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               56 (graph_out_0)\n             24 LOAD_CONST               6 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 298 \n298           0 LOAD_GLOBAL             16 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              56 (graph_out_0)\n             10 LOAD_FAST               56 (graph_out_0)\n             12 LOAD_CONST               4 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               56 (graph_out_0)\n             18 LOAD_CONST               5 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               56 (graph_out_0)\n             24 LOAD_CONST               6 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 298 \n298           0 LOAD_GLOBAL             16 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              56 (graph_out_0)\n             10 LOAD_FAST               56 (graph_out_0)\n             12 LOAD_CONST               4 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               56 (graph_out_0)\n             18 LOAD_CONST               5 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               56 (graph_out_0)\n             24 LOAD_CONST               6 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 298 \n298           0 LOAD_GLOBAL             16 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              56 (graph_out_0)\n             10 LOAD_FAST               56 (graph_out_0)\n             12 LOAD_CONST               4 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               56 (graph_out_0)\n             18 LOAD_CONST               5 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               56 (graph_out_0)\n             24 LOAD_CONST               6 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 298 \n298           0 LOAD_GLOBAL             16 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              56 (graph_out_0)\n             10 LOAD_FAST               56 (graph_out_0)\n             12 LOAD_CONST               4 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               56 (graph_out_0)\n             18 LOAD_CONST               5 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               56 (graph_out_0)\n             24 LOAD_CONST               6 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 298 \n298           0 LOAD_GLOBAL             16 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              56 (graph_out_0)\n             10 LOAD_FAST               56 (graph_out_0)\n             12 LOAD_CONST               4 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               56 (graph_out_0)\n             18 LOAD_CONST               5 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               56 (graph_out_0)\n             24 LOAD_CONST               6 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 298 \n298           0 LOAD_GLOBAL             16 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              56 (graph_out_0)\n             10 LOAD_FAST               56 (graph_out_0)\n             12 LOAD_CONST               4 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               56 (graph_out_0)\n             18 LOAD_CONST               5 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               56 (graph_out_0)\n             24 LOAD_CONST               6 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 298 \n298           0 LOAD_GLOBAL             16 (__compiled_fn_15)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              56 (graph_out_0)\n             10 LOAD_FAST               56 (graph_out_0)\n             12 LOAD_CONST               4 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               56 (graph_out_0)\n             18 LOAD_CONST               5 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               56 (graph_out_0)\n             24 LOAD_CONST               6 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 298 \n298           0 LOAD_GLOBAL             16 (__compiled_fn_16)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              56 (graph_out_0)\n             10 LOAD_FAST               56 (graph_out_0)\n             12 LOAD_CONST               4 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               56 (graph_out_0)\n             18 LOAD_CONST               5 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               56 (graph_out_0)\n             24 LOAD_CONST               6 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 298 \n298           0 LOAD_GLOBAL             16 (__compiled_fn_17)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              56 (graph_out_0)\n             10 LOAD_FAST               56 (graph_out_0)\n             12 LOAD_CONST               4 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               56 (graph_out_0)\n             18 LOAD_CONST               5 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               56 (graph_out_0)\n             24 LOAD_CONST               6 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 298 \n298           0 LOAD_GLOBAL             16 (__compiled_fn_18)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              56 (graph_out_0)\n             10 LOAD_FAST               56 (graph_out_0)\n             12 LOAD_CONST               4 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               56 (graph_out_0)\n             18 LOAD_CONST               5 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               56 (graph_out_0)\n             24 LOAD_CONST               6 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 298 \n298           0 LOAD_GLOBAL             16 (__compiled_fn_19)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              56 (graph_out_0)\n             10 LOAD_FAST               56 (graph_out_0)\n             12 LOAD_CONST               4 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               56 (graph_out_0)\n             18 LOAD_CONST               5 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               56 (graph_out_0)\n             24 LOAD_CONST               6 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              5 (__resume_at_6_20)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              5 (__resume_at_14_21)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f4ffc321370, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_22')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f4ffc32baa0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_23')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f4ffc321370, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_24')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f4ffc32baa0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_25')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py line 944 \n944           0 LOAD_GLOBAL             17 (__compiled_fn_26)\n              2 LOAD_FAST                0 (___stack0)\n              4 LOAD_ATTR               18 (last_hidden_state)\n              6 LOAD_FAST                2 (labels)\n              8 CALL_FUNCTION            2\n             10 STORE_FAST              27 (graph_out_0)\n             12 LOAD_CONST               8 (<class 'transformers.modeling_outputs.CausalLMOutputWithPast'>)\n             14 LOAD_FAST               27 (graph_out_0)\n             16 LOAD_CONST               3 (0)\n             18 BINARY_SUBSCR\n             20 LOAD_FAST               27 (graph_out_0)\n             22 LOAD_CONST               6 (1)\n             24 BINARY_SUBSCR\n             26 LOAD_FAST                0 (___stack0)\n             28 LOAD_ATTR               14 (past_key_values)\n             30 LOAD_FAST                0 (___stack0)\n             32 LOAD_ATTR               15 (hidden_states)\n             34 LOAD_FAST                0 (___stack0)\n             36 LOAD_ATTR               16 (attentions)\n             38 LOAD_CONST               9 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions'))\n             40 CALL_FUNCTION_KW         5\n             42 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_27)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_28)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_29)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             12 (__import_contextlib)\n             28 LOAD_ATTR               13 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              49 (___context_manager_0_3)\n             34 LOAD_FAST               49 (___context_manager_0_3)\n             36 LOAD_METHOD             14 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               49 (___context_manager_0_3)\n             50 LOAD_METHOD             15 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               49 (___context_manager_0_3)\n             68 LOAD_METHOD             15 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             16 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 1588 \n1588           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                4 (model)\n               4 LOAD_ATTR                5 (decoder)\n               6 LOAD_FAST                1 (input_ids)\n               8 LOAD_FAST                2 (attention_mask)\n              10 LOAD_FAST                3 (encoder_hidden_states)\n              12 LOAD_FAST                4 (encoder_attention_mask)\n              14 LOAD_FAST                5 (head_mask)\n              16 LOAD_FAST                6 (cross_attn_head_mask)\n              18 LOAD_FAST                7 (past_key_values)\n              20 LOAD_FAST                8 (inputs_embeds)\n              22 LOAD_FAST               10 (use_cache)\n              24 LOAD_FAST                0 (self)\n              26 LOAD_ATTR                0 (config)\n              28 LOAD_ATTR                1 (output_attentions)\n              30 LOAD_FAST                0 (self)\n              32 LOAD_ATTR                0 (config)\n              34 LOAD_ATTR                2 (output_hidden_states)\n              36 LOAD_FAST                0 (self)\n              38 LOAD_ATTR                0 (config)\n              40 LOAD_ATTR                3 (use_return_dict)\n              42 LOAD_CONST               2 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                3 (use_return_dict)\n              50 STORE_FAST              13 (return_dict)\n\n1697          52 CALL_FUNCTION_KW        12\n              54 LOAD_GLOBAL             19 (__resume_at_94_5)\n              56 ROT_TWO\n              58 LOAD_FAST                0 (self)\n              60 LOAD_FAST                9 (labels)\n              62 LOAD_FAST               13 (return_dict)\n              64 CALL_FUNCTION            4\n              66 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 886 \n886           0 LOAD_GLOBAL              5 (__compiled_fn_6)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 125 \n125           0 LOAD_GLOBAL             11 (__compiled_fn_7)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 388 \n388           0 LOAD_GLOBAL             14 (__compiled_fn_8)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 388 \n388           0 LOAD_GLOBAL             14 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 388 \n388           0 LOAD_GLOBAL             14 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 388 \n388           0 LOAD_GLOBAL             14 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 388 \n388           0 LOAD_GLOBAL             14 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 388 \n388           0 LOAD_GLOBAL             14 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_14)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              6 (__resume_at_14_15)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fe6cc4ec0e0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_16')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fe6cc489fd0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_17')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fe6cc4ec0e0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_18')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fe6cc489fd0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_19')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 1697 \n1697           0 LOAD_GLOBAL             17 (__compiled_fn_20)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               18 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              26 (graph_out_0)\n              12 LOAD_CONST               7 (<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>)\n              14 LOAD_FAST               26 (graph_out_0)\n              16 LOAD_CONST               3 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               26 (graph_out_0)\n              22 LOAD_CONST               5 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR               13 (past_key_values)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR               14 (hidden_states)\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               15 (attentions)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               16 (cross_attentions)\n              42 LOAD_CONST               8 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n              44 CALL_FUNCTION_KW         6\n              46 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_21)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_22)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_23)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             12 (__import_contextlib)\n             28 LOAD_ATTR               13 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              41 (___context_manager_0_3)\n             34 LOAD_FAST               41 (___context_manager_0_3)\n             36 LOAD_METHOD             14 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               41 (___context_manager_0_3)\n             50 LOAD_METHOD             15 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               41 (___context_manager_0_3)\n             68 LOAD_METHOD             15 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             16 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 1290 \n1290           0 LOAD_GLOBAL             22 (__compiled_fn_5)\n               2 LOAD_FAST               12 (labels)\n               4 CALL_FUNCTION            1\n               6 STORE_FAST              53 (graph_out_0)\n               8 LOAD_FAST                0 (self)\n              10 LOAD_ATTR                4 (model)\n              12 LOAD_FAST                1 (input_ids)\n              14 LOAD_FAST                2 (attention_mask)\n              16 LOAD_FAST               53 (graph_out_0)\n              18 LOAD_CONST               3 (0)\n              20 BINARY_SUBSCR\n              22 LOAD_FAST                8 (encoder_outputs)\n              24 LOAD_FAST                4 (decoder_attention_mask)\n              26 LOAD_FAST                5 (head_mask)\n              28 LOAD_FAST                6 (decoder_head_mask)\n              30 LOAD_FAST                7 (cross_attn_head_mask)\n              32 LOAD_FAST                9 (past_key_values)\n              34 LOAD_FAST               10 (inputs_embeds)\n              36 LOAD_FAST               11 (decoder_inputs_embeds)\n              38 LOAD_FAST               13 (use_cache)\n              40 LOAD_FAST               14 (output_attentions)\n              42 LOAD_FAST               15 (output_hidden_states)\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                1 (use_return_dict)\n              50 LOAD_CONST               2 (('attention_mask', 'decoder_input_ids', 'encoder_outputs', 'decoder_attention_mask', 'head_mask', 'decoder_head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'decoder_inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              52 LOAD_FAST                0 (self)\n              54 LOAD_ATTR                0 (config)\n              56 LOAD_ATTR                1 (use_return_dict)\n              58 STORE_FAST              16 (return_dict)\n\n1327          60 CALL_FUNCTION_KW        15\n              62 LOAD_GLOBAL             23 (__resume_at_96_6)\n              64 ROT_TWO\n              66 LOAD_FAST                0 (self)\n              68 LOAD_FAST               12 (labels)\n              70 LOAD_FAST               16 (return_dict)\n              72 CALL_FUNCTION            4\n              74 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 1162 \n1162           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                7 (encoder)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                5 (head_mask)\n              10 LOAD_FAST               10 (inputs_embeds)\n              12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (output_attentions)\n              18 LOAD_FAST                0 (self)\n              20 LOAD_ATTR                0 (config)\n              22 LOAD_ATTR                2 (output_hidden_states)\n              24 LOAD_FAST               15 (return_dict)\n              26 LOAD_CONST               1 (('input_ids', 'attention_mask', 'head_mask', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              28 LOAD_FAST                0 (self)\n              30 LOAD_ATTR                0 (config)\n              32 LOAD_ATTR                3 (use_cache)\n              34 LOAD_FAST                0 (self)\n              36 LOAD_ATTR                0 (config)\n              38 LOAD_ATTR                1 (output_attentions)\n              40 LOAD_FAST                0 (self)\n              42 LOAD_ATTR                0 (config)\n              44 LOAD_ATTR                2 (output_hidden_states)\n              46 STORE_FAST              14 (output_hidden_states)\n              48 STORE_FAST              13 (output_attentions)\n              50 STORE_FAST              12 (use_cache)\n\n1199          52 CALL_FUNCTION_KW         7\n              54 LOAD_GLOBAL             20 (__resume_at_140_7)\n              56 ROT_TWO\n              58 LOAD_FAST                0 (self)\n              60 LOAD_FAST                2 (attention_mask)\n              62 LOAD_FAST                3 (decoder_input_ids)\n              64 LOAD_FAST                4 (decoder_attention_mask)\n              66 LOAD_FAST                6 (decoder_head_mask)\n              68 LOAD_FAST                7 (cross_attn_head_mask)\n              70 LOAD_FAST                9 (past_key_values)\n              72 LOAD_FAST               11 (decoder_inputs_embeds)\n              74 LOAD_FAST               12 (use_cache)\n              76 LOAD_FAST               13 (output_attentions)\n              78 LOAD_FAST               14 (output_hidden_states)\n              80 LOAD_FAST               15 (return_dict)\n              82 CALL_FUNCTION           13\n              84 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 125 \n125           0 LOAD_GLOBAL             11 (__compiled_fn_8)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 309 \n309           0 LOAD_GLOBAL             21 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              4 (__resume_at_6_15)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7eff827ab7e0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_16')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7eff828c3260, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_17')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 1199 \n1199           0 LOAD_FAST                1 (self)\n               2 LOAD_ATTR               11 (decoder)\n               4 LOAD_FAST                3 (decoder_input_ids)\n               6 LOAD_FAST                4 (decoder_attention_mask)\n               8 LOAD_FAST                0 (___stack0)\n              10 LOAD_ATTR               13 (last_hidden_state)\n              12 LOAD_FAST                2 (attention_mask)\n              14 LOAD_FAST                5 (decoder_head_mask)\n              16 LOAD_FAST                6 (cross_attn_head_mask)\n              18 LOAD_FAST                7 (past_key_values)\n              20 LOAD_FAST                8 (decoder_inputs_embeds)\n              22 LOAD_FAST                9 (use_cache)\n              24 LOAD_FAST               10 (output_attentions)\n              26 LOAD_FAST               11 (output_hidden_states)\n              28 LOAD_FAST               12 (return_dict)\n              30 LOAD_CONST               6 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              32 LOAD_FAST                0 (___stack0)\n              34 STORE_FAST              15 (encoder_outputs)\n\n1217          36 CALL_FUNCTION_KW        12\n              38 LOAD_GLOBAL             20 (__resume_at_256_18)\n              40 ROT_TWO\n              42 LOAD_FAST               12 (return_dict)\n              44 LOAD_FAST               15 (encoder_outputs)\n              46 CALL_FUNCTION            3\n              48 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 886 \n886           0 LOAD_GLOBAL              5 (__compiled_fn_19)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 125 \n125           0 LOAD_GLOBAL             11 (__compiled_fn_20)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 388 \n388           0 LOAD_GLOBAL             14 (__compiled_fn_21)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              95 (graph_out_0)\n             12 LOAD_FAST               95 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               95 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               95 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               95 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               95 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 388 \n388           0 LOAD_GLOBAL             14 (__compiled_fn_22)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              95 (graph_out_0)\n             12 LOAD_FAST               95 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               95 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               95 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               95 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               95 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 388 \n388           0 LOAD_GLOBAL             14 (__compiled_fn_23)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              95 (graph_out_0)\n             12 LOAD_FAST               95 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               95 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               95 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               95 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               95 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 388 \n388           0 LOAD_GLOBAL             14 (__compiled_fn_24)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              95 (graph_out_0)\n             12 LOAD_FAST               95 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               95 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               95 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               95 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               95 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 388 \n388           0 LOAD_GLOBAL             14 (__compiled_fn_25)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              95 (graph_out_0)\n             12 LOAD_FAST               95 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               95 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               95 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               95 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               95 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 388 \n388           0 LOAD_GLOBAL             14 (__compiled_fn_26)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              95 (graph_out_0)\n             12 LOAD_FAST               95 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 LOAD_FAST               95 (graph_out_0)\n             20 LOAD_CONST               8 (1)\n             22 BINARY_SUBSCR\n             24 LOAD_FAST               95 (graph_out_0)\n             26 LOAD_CONST               2 (2)\n             28 BINARY_SUBSCR\n             30 LOAD_FAST               95 (graph_out_0)\n             32 LOAD_CONST               9 (3)\n             34 BINARY_SUBSCR\n             36 LOAD_FAST               95 (graph_out_0)\n             38 LOAD_CONST              10 (4)\n             40 BINARY_SUBSCR\n             42 BUILD_TUPLE              4\n             44 BUILD_TUPLE              2\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_27)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              6 (__resume_at_14_28)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7eff827ab7e0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_29')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7eff828c3260, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_30')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7eff827ab7e0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_31')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7eff828c3260, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_32')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              9 (__resume_at_6_33)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (decoder_hidden_states)\n             14 LOAD_FAST                4 (decoder_attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 LOAD_FAST                6 (encoder_last_hidden_state)\n             20 LOAD_FAST                7 (encoder_hidden_states)\n             22 LOAD_FAST                8 (encoder_attentions)\n             24 CALL_FUNCTION            8\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              9 (__resume_at_14_34)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (decoder_hidden_states)\n             12 LOAD_FAST                3 (decoder_attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 LOAD_FAST                5 (encoder_last_hidden_state)\n             18 LOAD_FAST                6 (encoder_hidden_states)\n             20 LOAD_FAST                7 (encoder_attentions)\n             22 CALL_FUNCTION            7\n             24 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 4 \n  4           0 LOAD_FAST                1 (decoder_hidden_states)\n              2 LOAD_FAST                0 (self)\n\n  5           4 STORE_ATTR               2 (decoder_hidden_states)\n              6 LOAD_GLOBAL              9 (__resume_at_20_35)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (decoder_attentions)\n             12 LOAD_FAST                3 (cross_attentions)\n             14 LOAD_FAST                4 (encoder_last_hidden_state)\n             16 LOAD_FAST                5 (encoder_hidden_states)\n             18 LOAD_FAST                6 (encoder_attentions)\n             20 CALL_FUNCTION            6\n             22 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 5 \n  5           0 LOAD_FAST                1 (decoder_attentions)\n              2 LOAD_FAST                0 (self)\n\n  6           4 STORE_ATTR               3 (decoder_attentions)\n              6 LOAD_GLOBAL              9 (__resume_at_26_36)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (cross_attentions)\n             12 LOAD_FAST                3 (encoder_last_hidden_state)\n             14 LOAD_FAST                4 (encoder_hidden_states)\n             16 LOAD_FAST                5 (encoder_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 6 \n  6           0 LOAD_FAST                1 (cross_attentions)\n              2 LOAD_FAST                0 (self)\n\n  7           4 STORE_ATTR               4 (cross_attentions)\n              6 LOAD_GLOBAL              9 (__resume_at_32_37)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (encoder_last_hidden_state)\n             12 LOAD_FAST                3 (encoder_hidden_states)\n             14 LOAD_FAST                4 (encoder_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 7 \n  7           0 LOAD_FAST                1 (encoder_last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  8           4 STORE_ATTR               5 (encoder_last_hidden_state)\n              6 LOAD_GLOBAL              9 (__resume_at_38_38)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (encoder_hidden_states)\n             12 LOAD_FAST                3 (encoder_attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7eff827ab7e0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_39')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7eff828c3260, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_40')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7eff827ab7e0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_41')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7eff828c3260, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_42')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (encoder_last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7eff827ab7e0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_43')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (encoder_last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7eff828c3260, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_44')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py line 1327 \n1327           0 LOAD_GLOBAL             20 (__compiled_fn_45)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               21 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              29 (graph_out_0)\n              12 LOAD_CONST               7 (<class 'transformers.modeling_outputs.Seq2SeqLMOutput'>)\n              14 LOAD_FAST               29 (graph_out_0)\n              16 LOAD_CONST               3 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               29 (graph_out_0)\n              22 LOAD_CONST               5 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR               13 (past_key_values)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR               14 (decoder_hidden_states)\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               15 (decoder_attentions)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               16 (cross_attentions)\n              42 LOAD_FAST                0 (___stack0)\n              44 LOAD_ATTR               17 (encoder_last_hidden_state)\n              46 LOAD_FAST                0 (___stack0)\n              48 LOAD_ATTR               18 (encoder_hidden_states)\n              50 LOAD_FAST                0 (___stack0)\n              52 LOAD_ATTR               19 (encoder_attentions)\n              54 LOAD_CONST               8 (('loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'))\n              56 CALL_FUNCTION_KW         9\n              58 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_46)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             12 (__import_contextlib)\n             28 LOAD_ATTR               13 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              46 (___context_manager_0_3)\n             34 LOAD_FAST               46 (___context_manager_0_3)\n             36 LOAD_METHOD             14 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               46 (___context_manager_0_3)\n             50 LOAD_METHOD             15 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               46 (___context_manager_0_3)\n             68 LOAD_METHOD             15 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             16 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 1566 \n1566           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                4 (model)\n               4 LOAD_ATTR                5 (decoder)\n               6 LOAD_FAST                1 (input_ids)\n               8 LOAD_FAST                2 (attention_mask)\n              10 LOAD_FAST                3 (encoder_hidden_states)\n              12 LOAD_FAST                4 (encoder_attention_mask)\n              14 LOAD_FAST                5 (head_mask)\n              16 LOAD_FAST                6 (cross_attn_head_mask)\n              18 LOAD_FAST                7 (past_key_values)\n              20 LOAD_FAST                8 (inputs_embeds)\n              22 LOAD_FAST               10 (use_cache)\n              24 LOAD_FAST                0 (self)\n              26 LOAD_ATTR                0 (config)\n              28 LOAD_ATTR                1 (output_attentions)\n              30 LOAD_FAST                0 (self)\n              32 LOAD_ATTR                0 (config)\n              34 LOAD_ATTR                2 (output_hidden_states)\n              36 LOAD_FAST                0 (self)\n              38 LOAD_ATTR                0 (config)\n              40 LOAD_ATTR                3 (use_return_dict)\n              42 LOAD_CONST               2 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                3 (use_return_dict)\n              50 STORE_FAST              13 (return_dict)\n\n1676          52 CALL_FUNCTION_KW        12\n              54 LOAD_GLOBAL             19 (__resume_at_94_5)\n              56 ROT_TWO\n              58 LOAD_FAST                0 (self)\n              60 LOAD_FAST                9 (labels)\n              62 LOAD_FAST               13 (return_dict)\n              64 CALL_FUNCTION            4\n              66 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 884 \n884           0 LOAD_GLOBAL              5 (__compiled_fn_6)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 133 \n133           0 LOAD_GLOBAL              8 (__compiled_fn_7)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_8)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_15)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_16)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_17)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_18)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_19)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_20)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              6 (__resume_at_14_21)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f9d735afb50, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_22')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f9d7b0b2d90, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_23')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f9d735afb50, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_24')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f9d7b0b2d90, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_25')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 1676 \n1676           0 LOAD_GLOBAL             17 (__compiled_fn_26)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               18 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              26 (graph_out_0)\n              12 LOAD_CONST               7 (<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>)\n              14 LOAD_FAST               26 (graph_out_0)\n              16 LOAD_CONST               3 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               26 (graph_out_0)\n              22 LOAD_CONST               5 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR               13 (past_key_values)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR               14 (hidden_states)\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               15 (attentions)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               16 (cross_attentions)\n              42 LOAD_CONST               8 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n              44 CALL_FUNCTION_KW         6\n              46 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_27)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_28)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_29)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                1 (mod)\n               2 LOAD_ATTR                1 (zero_grad)\n               4 LOAD_CONST               1 (True)\n\n1879           6 CALL_FUNCTION            1\n               8 LOAD_GLOBAL              2 (__resume_at_34_2)\n              10 ROT_TWO\n              12 CALL_FUNCTION            1\n              14 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('decoder_input_ids')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('decoder_input_ids')\n             22 BINARY_SUBSCR\n             24 LOAD_CONST               4 ('labels')\n             26 LOAD_FAST                4 (cloned_inputs)\n             28 LOAD_CONST               4 ('labels')\n             30 BINARY_SUBSCR\n             32 BUILD_MAP                3\n             34 LOAD_GLOBAL             12 (__import_contextlib)\n             36 LOAD_ATTR               13 (nullcontext)\n             38 CALL_FUNCTION            0\n             40 STORE_FAST              36 (___context_manager_0_3)\n             42 LOAD_FAST               36 (___context_manager_0_3)\n             44 LOAD_METHOD             14 (__enter__)\n             46 CALL_METHOD              0\n             48 POP_TOP\n             50 SETUP_FINALLY           10 (to 72)\n\n557          52 CALL_FUNCTION_EX         1\n             54 POP_BLOCK\n             56 LOAD_FAST               36 (___context_manager_0_3)\n             58 LOAD_METHOD             15 (__exit__)\n             60 LOAD_CONST               0 (None)\n             62 DUP_TOP\n             64 DUP_TOP\n             66 CALL_METHOD              3\n             68 POP_TOP\n             70 JUMP_FORWARD             9 (to 90)\n        >>   72 NOP\n             74 LOAD_FAST               36 (___context_manager_0_3)\n             76 LOAD_METHOD             15 (__exit__)\n             78 LOAD_CONST               0 (None)\n             80 DUP_TOP\n             82 DUP_TOP\n             84 CALL_METHOD              3\n             86 POP_TOP\n             88 RERAISE                  0\n        >>   90 NOP\n             92 LOAD_GLOBAL             16 (__resume_at_44_4)\n             94 ROT_THREE\n             96 LOAD_FAST                1 (self)\n             98 LOAD_FAST                2 (mod)\n            100 LOAD_FAST                3 (collect_outputs)\n            102 LOAD_FAST                4 (cloned_inputs)\n            104 CALL_FUNCTION            6\n            106 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 1372 \n1372           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                7 (model)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                3 (decoder_input_ids)\n              10 LOAD_FAST                8 (encoder_outputs)\n              12 LOAD_FAST                4 (decoder_attention_mask)\n              14 LOAD_FAST                5 (head_mask)\n              16 LOAD_FAST                6 (decoder_head_mask)\n              18 LOAD_FAST                7 (cross_attn_head_mask)\n              20 LOAD_FAST                9 (past_key_values)\n              22 LOAD_FAST               10 (inputs_embeds)\n              24 LOAD_FAST               11 (decoder_inputs_embeds)\n              26 LOAD_CONST               3 (False)\n              28 LOAD_FAST               14 (output_attentions)\n              30 LOAD_FAST               15 (output_hidden_states)\n              32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                0 (config)\n              36 LOAD_ATTR                1 (use_return_dict)\n              38 LOAD_CONST               4 (('attention_mask', 'decoder_input_ids', 'encoder_outputs', 'decoder_attention_mask', 'head_mask', 'decoder_head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'decoder_inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              40 LOAD_FAST                0 (self)\n              42 LOAD_ATTR                0 (config)\n              44 LOAD_ATTR                1 (use_return_dict)\n              46 STORE_FAST              16 (return_dict)\n\n1414          48 CALL_FUNCTION_KW        15\n              50 LOAD_GLOBAL             23 (__resume_at_120_5)\n              52 ROT_TWO\n              54 LOAD_FAST                0 (self)\n              56 LOAD_FAST               12 (labels)\n              58 LOAD_FAST               16 (return_dict)\n              60 CALL_FUNCTION            4\n              62 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 1210 \n1210           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                5 (encoder)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                5 (head_mask)\n              10 LOAD_FAST               10 (inputs_embeds)\n              12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (output_attentions)\n              18 LOAD_FAST                0 (self)\n              20 LOAD_ATTR                0 (config)\n              22 LOAD_ATTR                2 (output_hidden_states)\n              24 LOAD_FAST               15 (return_dict)\n              26 LOAD_CONST               2 (('input_ids', 'attention_mask', 'head_mask', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              28 LOAD_FAST                0 (self)\n              30 LOAD_ATTR                0 (config)\n              32 LOAD_ATTR                1 (output_attentions)\n              34 LOAD_FAST                0 (self)\n              36 LOAD_ATTR                0 (config)\n              38 LOAD_ATTR                2 (output_hidden_states)\n              40 STORE_FAST              14 (output_hidden_states)\n              42 STORE_FAST              13 (output_attentions)\n\n1258          44 CALL_FUNCTION_KW         7\n              46 LOAD_GLOBAL             18 (__resume_at_110_6)\n              48 ROT_TWO\n              50 LOAD_FAST                0 (self)\n              52 LOAD_FAST                2 (attention_mask)\n              54 LOAD_FAST                3 (decoder_input_ids)\n              56 LOAD_FAST                4 (decoder_attention_mask)\n              58 LOAD_FAST                6 (decoder_head_mask)\n              60 LOAD_FAST                7 (cross_attn_head_mask)\n              62 LOAD_FAST                9 (past_key_values)\n              64 LOAD_FAST               11 (decoder_inputs_embeds)\n              66 LOAD_FAST               12 (use_cache)\n              68 LOAD_FAST               13 (output_attentions)\n              70 LOAD_FAST               14 (output_hidden_states)\n              72 LOAD_FAST               15 (return_dict)\n              74 CALL_FUNCTION           13\n              76 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 133 \n133           0 LOAD_GLOBAL              8 (__compiled_fn_7)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 316 \n316           0 LOAD_GLOBAL             21 (__compiled_fn_8)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 316 \n316           0 LOAD_GLOBAL             21 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 316 \n316           0 LOAD_GLOBAL             21 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 316 \n316           0 LOAD_GLOBAL             21 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 316 \n316           0 LOAD_GLOBAL             21 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 316 \n316           0 LOAD_GLOBAL             21 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 316 \n316           0 LOAD_GLOBAL             21 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 316 \n316           0 LOAD_GLOBAL             21 (__compiled_fn_15)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 316 \n316           0 LOAD_GLOBAL             21 (__compiled_fn_16)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 316 \n316           0 LOAD_GLOBAL             21 (__compiled_fn_17)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 316 \n316           0 LOAD_GLOBAL             21 (__compiled_fn_18)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 316 \n316           0 LOAD_GLOBAL             21 (__compiled_fn_19)\n              2 LOAD_FAST                1 (hidden_states)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST              48 (graph_out_0)\n              8 LOAD_FAST               48 (graph_out_0)\n             10 LOAD_CONST               5 (0)\n             12 BINARY_SUBSCR\n             14 BUILD_TUPLE              1\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              4 (__resume_at_6_20)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fe6ea1ccc90, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_21')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fe6ea1cd840, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_22')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 1258 \n1258           0 LOAD_FAST                1 (self)\n               2 LOAD_ATTR                9 (decoder)\n               4 LOAD_FAST                3 (decoder_input_ids)\n               6 LOAD_FAST                4 (decoder_attention_mask)\n               8 LOAD_FAST                0 (___stack0)\n              10 LOAD_ATTR               11 (last_hidden_state)\n              12 LOAD_FAST                2 (attention_mask)\n              14 LOAD_FAST                5 (decoder_head_mask)\n              16 LOAD_FAST                6 (cross_attn_head_mask)\n              18 LOAD_FAST                7 (past_key_values)\n              20 LOAD_FAST                8 (decoder_inputs_embeds)\n              22 LOAD_FAST                9 (use_cache)\n              24 LOAD_FAST               10 (output_attentions)\n              26 LOAD_FAST               11 (output_hidden_states)\n              28 LOAD_FAST               12 (return_dict)\n              30 LOAD_CONST               7 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              32 LOAD_FAST                0 (___stack0)\n              34 STORE_FAST              15 (encoder_outputs)\n\n1276          36 CALL_FUNCTION_KW        12\n              38 LOAD_GLOBAL             18 (__resume_at_226_23)\n              40 ROT_TWO\n              42 LOAD_FAST               12 (return_dict)\n              44 LOAD_FAST               15 (encoder_outputs)\n              46 CALL_FUNCTION            3\n              48 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 884 \n884           0 LOAD_GLOBAL              5 (__compiled_fn_24)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 133 \n133           0 LOAD_GLOBAL              8 (__compiled_fn_25)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_26)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_27)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_28)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_29)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_30)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_31)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_32)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_33)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_34)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_35)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_36)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 395 \n395           0 LOAD_GLOBAL             14 (__compiled_fn_37)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 LOAD_FAST                3 (encoder_hidden_states)\n              8 CALL_FUNCTION            3\n             10 STORE_FAST              94 (graph_out_0)\n             12 LOAD_FAST               94 (graph_out_0)\n             14 LOAD_CONST               7 (0)\n             16 BINARY_SUBSCR\n             18 BUILD_TUPLE              1\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_38)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fe6ea1ccc90, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_39')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fe6ea1cd840, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_40')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              9 (__resume_at_6_41)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (decoder_hidden_states)\n             14 LOAD_FAST                4 (decoder_attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 LOAD_FAST                6 (encoder_last_hidden_state)\n             20 LOAD_FAST                7 (encoder_hidden_states)\n             22 LOAD_FAST                8 (encoder_attentions)\n             24 CALL_FUNCTION            8\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              9 (__resume_at_14_42)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (decoder_hidden_states)\n             12 LOAD_FAST                3 (decoder_attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 LOAD_FAST                5 (encoder_last_hidden_state)\n             18 LOAD_FAST                6 (encoder_hidden_states)\n             20 LOAD_FAST                7 (encoder_attentions)\n             22 CALL_FUNCTION            7\n             24 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 4 \n  4           0 LOAD_FAST                1 (decoder_hidden_states)\n              2 LOAD_FAST                0 (self)\n\n  5           4 STORE_ATTR               2 (decoder_hidden_states)\n              6 LOAD_GLOBAL              9 (__resume_at_20_43)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (decoder_attentions)\n             12 LOAD_FAST                3 (cross_attentions)\n             14 LOAD_FAST                4 (encoder_last_hidden_state)\n             16 LOAD_FAST                5 (encoder_hidden_states)\n             18 LOAD_FAST                6 (encoder_attentions)\n             20 CALL_FUNCTION            6\n             22 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 5 \n  5           0 LOAD_FAST                1 (decoder_attentions)\n              2 LOAD_FAST                0 (self)\n\n  6           4 STORE_ATTR               3 (decoder_attentions)\n              6 LOAD_GLOBAL              9 (__resume_at_26_44)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (cross_attentions)\n             12 LOAD_FAST                3 (encoder_last_hidden_state)\n             14 LOAD_FAST                4 (encoder_hidden_states)\n             16 LOAD_FAST                5 (encoder_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 6 \n  6           0 LOAD_FAST                1 (cross_attentions)\n              2 LOAD_FAST                0 (self)\n\n  7           4 STORE_ATTR               4 (cross_attentions)\n              6 LOAD_GLOBAL              9 (__resume_at_32_45)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (encoder_last_hidden_state)\n             12 LOAD_FAST                3 (encoder_hidden_states)\n             14 LOAD_FAST                4 (encoder_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 7 \n  7           0 LOAD_FAST                1 (encoder_last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  8           4 STORE_ATTR               5 (encoder_last_hidden_state)\n              6 LOAD_GLOBAL              9 (__resume_at_38_46)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (encoder_hidden_states)\n             12 LOAD_FAST                3 (encoder_attentions)\n             14 CALL_FUNCTION            3\n             16 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fe6ea1ccc90, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_47')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fe6ea1cd840, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_48')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (encoder_last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fe6ea1ccc90, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_49')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (encoder_last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7fe6ea1cd840, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_50')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py line 1414 \n1414           0 LOAD_GLOBAL             21 (__compiled_fn_51)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               22 (last_hidden_state)\n               6 LOAD_FAST                2 (labels)\n               8 CALL_FUNCTION            2\n              10 STORE_FAST              28 (graph_out_0)\n              12 LOAD_CONST               9 (<class 'transformers.modeling_outputs.Seq2SeqLMOutput'>)\n              14 LOAD_FAST               28 (graph_out_0)\n              16 LOAD_CONST               5 (0)\n              18 BINARY_SUBSCR\n              20 LOAD_FAST               28 (graph_out_0)\n              22 LOAD_CONST               7 (1)\n              24 BINARY_SUBSCR\n              26 LOAD_FAST                0 (___stack0)\n              28 LOAD_ATTR               14 (past_key_values)\n              30 LOAD_FAST                0 (___stack0)\n              32 LOAD_ATTR               15 (decoder_hidden_states)\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               16 (decoder_attentions)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               17 (cross_attentions)\n              42 LOAD_FAST                0 (___stack0)\n              44 LOAD_ATTR               18 (encoder_last_hidden_state)\n              46 LOAD_FAST                0 (___stack0)\n              48 LOAD_ATTR               19 (encoder_hidden_states)\n              50 LOAD_FAST                0 (___stack0)\n              52 LOAD_ATTR               20 (encoder_attentions)\n              54 LOAD_CONST              10 (('loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'))\n              56 CALL_FUNCTION_KW         9\n              58 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_52)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                2 (mod)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_FAST                5 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                4 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             10 (__import_contextlib)\n              2 LOAD_ATTR               11 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             10 (__import_contextlib)\n             28 LOAD_ATTR               11 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              18 (___context_manager_0_3)\n             34 LOAD_FAST               18 (___context_manager_0_3)\n             36 LOAD_METHOD             12 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               18 (___context_manager_0_3)\n             50 LOAD_METHOD             13 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               18 (___context_manager_0_3)\n             68 LOAD_METHOD             13 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             14 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 896 \n896           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                2 (roberta)\n              4 LOAD_FAST                1 (input_ids)\n              6 LOAD_FAST                2 (attention_mask)\n              8 LOAD_FAST                3 (token_type_ids)\n             10 LOAD_FAST                4 (position_ids)\n             12 LOAD_FAST                5 (head_mask)\n             14 LOAD_FAST                6 (inputs_embeds)\n             16 LOAD_FAST                7 (encoder_hidden_states)\n             18 LOAD_FAST                8 (encoder_attention_mask)\n             20 LOAD_FAST               10 (past_key_values)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_FAST               12 (output_attentions)\n             26 LOAD_FAST               13 (output_hidden_states)\n             28 LOAD_FAST                0 (self)\n             30 LOAD_ATTR                0 (config)\n             32 LOAD_ATTR                1 (use_return_dict)\n             34 LOAD_CONST               3 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n             36 LOAD_FAST                0 (self)\n             38 LOAD_ATTR                0 (config)\n             40 LOAD_ATTR                1 (use_return_dict)\n             42 STORE_FAST              14 (return_dict)\n\n962          44 CALL_FUNCTION_KW        13\n             46 LOAD_GLOBAL             16 (__resume_at_66_5)\n             48 ROT_TWO\n             50 LOAD_FAST                0 (self)\n             52 LOAD_FAST                9 (labels)\n             54 LOAD_FAST               14 (return_dict)\n             56 CALL_FUNCTION            4\n             58 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 734 \n734           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                8 (warn_if_padding_and_no_attention_mask)\n              4 LOAD_FAST                1 (input_ids)\n              6 LOAD_FAST                2 (attention_mask)\n              8 LOAD_CONST               2 (False)\n             10 LOAD_FAST                0 (self)\n             12 LOAD_ATTR                0 (config)\n             14 LOAD_ATTR                1 (output_attentions)\n             16 LOAD_FAST                0 (self)\n             18 LOAD_ATTR                0 (config)\n             20 LOAD_ATTR                2 (output_hidden_states)\n             22 LOAD_GLOBAL             30 (__import_torch)\n             24 LOAD_ATTR               31 (Size)\n             26 LOAD_CONST              15 (16)\n             28 LOAD_CONST              16 (512)\n             30 BUILD_TUPLE              2\n             32 CALL_FUNCTION            1\n             34 STORE_FAST              14 (input_shape)\n             36 STORE_FAST              12 (output_hidden_states)\n             38 STORE_FAST              11 (output_attentions)\n             40 STORE_FAST              10 (use_cache)\n\n792          42 CALL_FUNCTION            2\n             44 LOAD_GLOBAL             32 (__resume_at_144_6)\n             46 ROT_TWO\n             48 LOAD_FAST                0 (self)\n             50 LOAD_FAST                1 (input_ids)\n             52 LOAD_FAST                2 (attention_mask)\n             54 LOAD_FAST                3 (token_type_ids)\n             56 LOAD_FAST                4 (position_ids)\n             58 LOAD_FAST                5 (head_mask)\n             60 LOAD_FAST                6 (inputs_embeds)\n             62 LOAD_FAST                7 (encoder_hidden_states)\n             64 LOAD_FAST                8 (encoder_attention_mask)\n             66 LOAD_FAST                9 (past_key_values)\n             68 LOAD_FAST               10 (use_cache)\n             70 LOAD_FAST               11 (output_attentions)\n             72 LOAD_FAST               12 (output_hidden_states)\n             74 LOAD_FAST               13 (return_dict)\n             76 LOAD_FAST               14 (input_shape)\n             78 CALL_FUNCTION           16\n             80 RETURN_VALUE\n\n", "MODIFIED BYTECODE warn_if_padding_and_no_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/modeling_utils.py line 3482 \n3490           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (config)\n               4 LOAD_ATTR                1 (pad_token_id)\n               6 LOAD_FAST                1 (input_ids)\n               8 LOAD_CONST               1 (None)\n              10 LOAD_CONST               1 (None)\n              12 BUILD_SLICE              2\n              14 LOAD_CONST               2 (-1)\n              16 LOAD_CONST               3 (0)\n              18 BUILD_LIST               2\n              20 BUILD_TUPLE              2\n              22 BINARY_SUBSCR\n              24 CONTAINS_OP              0\n              26 POP_JUMP_IF_FALSE       90 (to 180)\n\n3492          28 LOAD_CONST               4 ('We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.')\n\n3491          30 STORE_FAST               3 (warn_string)\n\n3500          32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                0 (config)\n              36 LOAD_ATTR                2 (bos_token_id)\n              38 LOAD_CONST               1 (None)\n              40 IS_OP                    1\n              42 POP_JUMP_IF_FALSE       30 (to 60)\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                2 (bos_token_id)\n              50 LOAD_FAST                0 (self)\n              52 LOAD_ATTR                0 (config)\n              54 LOAD_ATTR                1 (pad_token_id)\n              56 COMPARE_OP               2 (==)\n              58 POP_JUMP_IF_TRUE        58 (to 116)\n\n3501     >>   60 LOAD_FAST                0 (self)\n              62 LOAD_ATTR                0 (config)\n              64 LOAD_ATTR                3 (eos_token_id)\n              66 LOAD_CONST               1 (None)\n              68 IS_OP                    1\n              70 POP_JUMP_IF_FALSE       44 (to 88)\n              72 LOAD_FAST                0 (self)\n              74 LOAD_ATTR                0 (config)\n              76 LOAD_ATTR                3 (eos_token_id)\n              78 LOAD_FAST                0 (self)\n              80 LOAD_ATTR                0 (config)\n              82 LOAD_ATTR                1 (pad_token_id)\n              84 COMPARE_OP               2 (==)\n              86 POP_JUMP_IF_TRUE        58 (to 116)\n\n3502     >>   88 LOAD_FAST                0 (self)\n              90 LOAD_ATTR                0 (config)\n              92 LOAD_ATTR                4 (sep_token_id)\n              94 LOAD_CONST               1 (None)\n              96 IS_OP                    1\n              98 POP_JUMP_IF_FALSE       83 (to 166)\n             100 LOAD_FAST                0 (self)\n             102 LOAD_ATTR                0 (config)\n             104 LOAD_ATTR                4 (sep_token_id)\n             106 LOAD_FAST                0 (self)\n             108 LOAD_ATTR                0 (config)\n             110 LOAD_ATTR                1 (pad_token_id)\n             112 COMPARE_OP               2 (==)\n             114 POP_JUMP_IF_FALSE       83 (to 166)\n\n3504     >>  116 LOAD_FAST                3 (warn_string)\n\n3505         118 LOAD_CONST               5 ('\\nYou may ignore this warning if your `pad_token_id` (')\n             120 LOAD_FAST                0 (self)\n             122 LOAD_ATTR                0 (config)\n             124 LOAD_ATTR                1 (pad_token_id)\n             126 FORMAT_VALUE             0\n             128 LOAD_CONST               6 (') is identical to the `bos_token_id` (')\n\n3506         130 LOAD_FAST                0 (self)\n             132 LOAD_ATTR                0 (config)\n             134 LOAD_ATTR                2 (bos_token_id)\n\n3505         136 FORMAT_VALUE             0\n             138 LOAD_CONST               7 ('), `eos_token_id` (')\n\n3506         140 LOAD_FAST                0 (self)\n             142 LOAD_ATTR                0 (config)\n             144 LOAD_ATTR                3 (eos_token_id)\n\n3505         146 FORMAT_VALUE             0\n             148 LOAD_CONST               8 ('), or the `sep_token_id` (')\n\n3507         150 LOAD_FAST                0 (self)\n             152 LOAD_ATTR                0 (config)\n             154 LOAD_ATTR                4 (sep_token_id)\n\n3505         156 FORMAT_VALUE             0\n             158 LOAD_CONST               9 ('), and your input is not padded.')\n             160 BUILD_STRING             9\n\n3504         162 INPLACE_ADD\n             164 STORE_FAST               3 (warn_string)\n\n3510     >>  166 LOAD_GLOBAL              5 (logger)\n             168 LOAD_ATTR                6 (warning_once)\n             170 LOAD_FAST                3 (warn_string)\n             172 CALL_FUNCTION            1\n             174 POP_TOP\n             176 LOAD_CONST               1 (None)\n             178 RETURN_VALUE\n\n3490     >>  180 LOAD_CONST               1 (None)\n             182 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 792 \n792           0 LOAD_GLOBAL             33 (__compiled_fn_7)\n              2 LOAD_FAST                2 (input_ids)\n              4 CALL_FUNCTION            1\n              6 EXTENDED_ARG             2\n              8 STORE_FAST             747 (graph_out_0)\n             10 LOAD_CONST              15 (<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>)\n             12 EXTENDED_ARG             2\n             14 LOAD_FAST              747 (graph_out_0)\n             16 LOAD_CONST               6 (0)\n             18 BINARY_SUBSCR\n             20 LOAD_CONST               1 (None)\n             22 LOAD_CONST               1 (None)\n             24 LOAD_CONST               1 (None)\n             26 LOAD_CONST               1 (None)\n             28 LOAD_CONST               1 (None)\n             30 LOAD_CONST              16 (('last_hidden_state', 'pooler_output', 'hidden_states', 'past_key_values', 'attentions', 'cross_attentions'))\n             32 CALL_FUNCTION_KW         6\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 962 \n962           0 LOAD_GLOBAL             16 (__compiled_fn_8)\n              2 LOAD_FAST                0 (___stack0)\n              4 LOAD_ATTR               17 (last_hidden_state)\n              6 LOAD_FAST                2 (labels)\n              8 CALL_FUNCTION            2\n             10 STORE_FAST              38 (graph_out_0)\n             12 LOAD_CONST               9 (<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>)\n             14 LOAD_FAST               38 (graph_out_0)\n             16 LOAD_CONST               4 (0)\n             18 BINARY_SUBSCR\n             20 LOAD_FAST               38 (graph_out_0)\n             22 LOAD_CONST               6 (1)\n             24 BINARY_SUBSCR\n             26 LOAD_FAST                0 (___stack0)\n             28 LOAD_ATTR               11 (past_key_values)\n             30 LOAD_FAST                0 (___stack0)\n             32 LOAD_ATTR               12 (hidden_states)\n             34 LOAD_FAST                0 (___stack0)\n             36 LOAD_ATTR               13 (attentions)\n             38 LOAD_FAST                0 (___stack0)\n             40 LOAD_ATTR               14 (cross_attentions)\n             42 LOAD_CONST              10 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n             44 CALL_FUNCTION_KW         6\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_9)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_10)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_11)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             10 (__import_contextlib)\n              2 LOAD_ATTR               11 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('start_positions')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('start_positions')\n             22 BINARY_SUBSCR\n             24 LOAD_CONST               4 ('end_positions')\n             26 LOAD_FAST                4 (cloned_inputs)\n             28 LOAD_CONST               4 ('end_positions')\n             30 BINARY_SUBSCR\n             32 BUILD_MAP                3\n             34 LOAD_GLOBAL             10 (__import_contextlib)\n             36 LOAD_ATTR               11 (nullcontext)\n             38 CALL_FUNCTION            0\n             40 STORE_FAST              17 (___context_manager_0_3)\n             42 LOAD_FAST               17 (___context_manager_0_3)\n             44 LOAD_METHOD             12 (__enter__)\n             46 CALL_METHOD              0\n             48 POP_TOP\n             50 SETUP_FINALLY           10 (to 72)\n\n557          52 CALL_FUNCTION_EX         1\n             54 POP_BLOCK\n             56 LOAD_FAST               17 (___context_manager_0_3)\n             58 LOAD_METHOD             13 (__exit__)\n             60 LOAD_CONST               0 (None)\n             62 DUP_TOP\n             64 DUP_TOP\n             66 CALL_METHOD              3\n             68 POP_TOP\n             70 JUMP_FORWARD             9 (to 90)\n        >>   72 NOP\n             74 LOAD_FAST               17 (___context_manager_0_3)\n             76 LOAD_METHOD             13 (__exit__)\n             78 LOAD_CONST               0 (None)\n             80 DUP_TOP\n             82 DUP_TOP\n             84 CALL_METHOD              3\n             86 POP_TOP\n             88 RERAISE                  0\n        >>   90 NOP\n             92 LOAD_GLOBAL             14 (__resume_at_44_4)\n             94 ROT_THREE\n             96 LOAD_FAST                1 (self)\n             98 LOAD_FAST                2 (mod)\n            100 LOAD_FAST                3 (collect_outputs)\n            102 LOAD_FAST                4 (cloned_inputs)\n            104 CALL_FUNCTION            6\n            106 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 1464 \n1464           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                2 (roberta)\n               4 LOAD_FAST                1 (input_ids)\n               6 LOAD_FAST                2 (attention_mask)\n               8 LOAD_FAST                3 (token_type_ids)\n              10 LOAD_FAST                4 (position_ids)\n              12 LOAD_FAST                5 (head_mask)\n              14 LOAD_FAST                6 (inputs_embeds)\n              16 LOAD_FAST                9 (output_attentions)\n              18 LOAD_FAST               10 (output_hidden_states)\n              20 LOAD_FAST                0 (self)\n              22 LOAD_ATTR                0 (config)\n              24 LOAD_ATTR                1 (use_return_dict)\n              26 LOAD_CONST               2 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              28 LOAD_FAST                0 (self)\n              30 LOAD_ATTR                0 (config)\n              32 LOAD_ATTR                1 (use_return_dict)\n              34 STORE_FAST              11 (return_dict)\n\n1498          36 CALL_FUNCTION_KW         9\n              38 LOAD_GLOBAL             15 (__resume_at_46_5)\n              40 ROT_TWO\n              42 LOAD_FAST                0 (self)\n              44 LOAD_FAST                7 (start_positions)\n              46 LOAD_FAST                8 (end_positions)\n              48 LOAD_FAST               11 (return_dict)\n              50 CALL_FUNCTION            5\n              52 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 734 \n734           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                8 (warn_if_padding_and_no_attention_mask)\n              4 LOAD_FAST                1 (input_ids)\n              6 LOAD_FAST                2 (attention_mask)\n              8 LOAD_CONST               2 (False)\n             10 LOAD_FAST                0 (self)\n             12 LOAD_ATTR                0 (config)\n             14 LOAD_ATTR                1 (output_attentions)\n             16 LOAD_FAST                0 (self)\n             18 LOAD_ATTR                0 (config)\n             20 LOAD_ATTR                2 (output_hidden_states)\n             22 LOAD_GLOBAL             30 (__import_torch)\n             24 LOAD_ATTR               31 (Size)\n             26 LOAD_CONST              15 (16)\n             28 LOAD_CONST              16 (512)\n             30 BUILD_TUPLE              2\n             32 CALL_FUNCTION            1\n             34 STORE_FAST              14 (input_shape)\n             36 STORE_FAST              12 (output_hidden_states)\n             38 STORE_FAST              11 (output_attentions)\n             40 STORE_FAST              10 (use_cache)\n\n792          42 CALL_FUNCTION            2\n             44 LOAD_GLOBAL             32 (__resume_at_144_6)\n             46 ROT_TWO\n             48 LOAD_FAST                0 (self)\n             50 LOAD_FAST                1 (input_ids)\n             52 LOAD_FAST                2 (attention_mask)\n             54 LOAD_FAST                3 (token_type_ids)\n             56 LOAD_FAST                4 (position_ids)\n             58 LOAD_FAST                5 (head_mask)\n             60 LOAD_FAST                6 (inputs_embeds)\n             62 LOAD_FAST                7 (encoder_hidden_states)\n             64 LOAD_FAST                8 (encoder_attention_mask)\n             66 LOAD_FAST                9 (past_key_values)\n             68 LOAD_FAST               10 (use_cache)\n             70 LOAD_FAST               11 (output_attentions)\n             72 LOAD_FAST               12 (output_hidden_states)\n             74 LOAD_FAST               13 (return_dict)\n             76 LOAD_FAST               14 (input_shape)\n             78 CALL_FUNCTION           16\n             80 RETURN_VALUE\n\n", "MODIFIED BYTECODE warn_if_padding_and_no_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/modeling_utils.py line 3482 \n3490           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (config)\n               4 LOAD_ATTR                1 (pad_token_id)\n               6 LOAD_FAST                1 (input_ids)\n               8 LOAD_CONST               1 (None)\n              10 LOAD_CONST               1 (None)\n              12 BUILD_SLICE              2\n              14 LOAD_CONST               2 (-1)\n              16 LOAD_CONST               3 (0)\n              18 BUILD_LIST               2\n              20 BUILD_TUPLE              2\n              22 BINARY_SUBSCR\n              24 CONTAINS_OP              0\n              26 POP_JUMP_IF_FALSE       90 (to 180)\n\n3492          28 LOAD_CONST               4 ('We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.')\n\n3491          30 STORE_FAST               3 (warn_string)\n\n3500          32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                0 (config)\n              36 LOAD_ATTR                2 (bos_token_id)\n              38 LOAD_CONST               1 (None)\n              40 IS_OP                    1\n              42 POP_JUMP_IF_FALSE       30 (to 60)\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                2 (bos_token_id)\n              50 LOAD_FAST                0 (self)\n              52 LOAD_ATTR                0 (config)\n              54 LOAD_ATTR                1 (pad_token_id)\n              56 COMPARE_OP               2 (==)\n              58 POP_JUMP_IF_TRUE        58 (to 116)\n\n3501     >>   60 LOAD_FAST                0 (self)\n              62 LOAD_ATTR                0 (config)\n              64 LOAD_ATTR                3 (eos_token_id)\n              66 LOAD_CONST               1 (None)\n              68 IS_OP                    1\n              70 POP_JUMP_IF_FALSE       44 (to 88)\n              72 LOAD_FAST                0 (self)\n              74 LOAD_ATTR                0 (config)\n              76 LOAD_ATTR                3 (eos_token_id)\n              78 LOAD_FAST                0 (self)\n              80 LOAD_ATTR                0 (config)\n              82 LOAD_ATTR                1 (pad_token_id)\n              84 COMPARE_OP               2 (==)\n              86 POP_JUMP_IF_TRUE        58 (to 116)\n\n3502     >>   88 LOAD_FAST                0 (self)\n              90 LOAD_ATTR                0 (config)\n              92 LOAD_ATTR                4 (sep_token_id)\n              94 LOAD_CONST               1 (None)\n              96 IS_OP                    1\n              98 POP_JUMP_IF_FALSE       83 (to 166)\n             100 LOAD_FAST                0 (self)\n             102 LOAD_ATTR                0 (config)\n             104 LOAD_ATTR                4 (sep_token_id)\n             106 LOAD_FAST                0 (self)\n             108 LOAD_ATTR                0 (config)\n             110 LOAD_ATTR                1 (pad_token_id)\n             112 COMPARE_OP               2 (==)\n             114 POP_JUMP_IF_FALSE       83 (to 166)\n\n3504     >>  116 LOAD_FAST                3 (warn_string)\n\n3505         118 LOAD_CONST               5 ('\\nYou may ignore this warning if your `pad_token_id` (')\n             120 LOAD_FAST                0 (self)\n             122 LOAD_ATTR                0 (config)\n             124 LOAD_ATTR                1 (pad_token_id)\n             126 FORMAT_VALUE             0\n             128 LOAD_CONST               6 (') is identical to the `bos_token_id` (')\n\n3506         130 LOAD_FAST                0 (self)\n             132 LOAD_ATTR                0 (config)\n             134 LOAD_ATTR                2 (bos_token_id)\n\n3505         136 FORMAT_VALUE             0\n             138 LOAD_CONST               7 ('), `eos_token_id` (')\n\n3506         140 LOAD_FAST                0 (self)\n             142 LOAD_ATTR                0 (config)\n             144 LOAD_ATTR                3 (eos_token_id)\n\n3505         146 FORMAT_VALUE             0\n             148 LOAD_CONST               8 ('), or the `sep_token_id` (')\n\n3507         150 LOAD_FAST                0 (self)\n             152 LOAD_ATTR                0 (config)\n             154 LOAD_ATTR                4 (sep_token_id)\n\n3505         156 FORMAT_VALUE             0\n             158 LOAD_CONST               9 ('), and your input is not padded.')\n             160 BUILD_STRING             9\n\n3504         162 INPLACE_ADD\n             164 STORE_FAST               3 (warn_string)\n\n3510     >>  166 LOAD_GLOBAL              5 (logger)\n             168 LOAD_ATTR                6 (warning_once)\n             170 LOAD_FAST                3 (warn_string)\n             172 CALL_FUNCTION            1\n             174 POP_TOP\n             176 LOAD_CONST               1 (None)\n             178 RETURN_VALUE\n\n3490     >>  180 LOAD_CONST               1 (None)\n             182 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 792 \n792           0 LOAD_GLOBAL             33 (__compiled_fn_7)\n              2 LOAD_FAST                2 (input_ids)\n              4 CALL_FUNCTION            1\n              6 EXTENDED_ARG             2\n              8 STORE_FAST             747 (graph_out_0)\n             10 LOAD_CONST              15 (<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>)\n             12 EXTENDED_ARG             2\n             14 LOAD_FAST              747 (graph_out_0)\n             16 LOAD_CONST               6 (0)\n             18 BINARY_SUBSCR\n             20 LOAD_CONST               1 (None)\n             22 LOAD_CONST               1 (None)\n             24 LOAD_CONST               1 (None)\n             26 LOAD_CONST               1 (None)\n             28 LOAD_CONST               1 (None)\n             30 LOAD_CONST              16 (('last_hidden_state', 'pooler_output', 'hidden_states', 'past_key_values', 'attentions', 'cross_attentions'))\n             32 CALL_FUNCTION_KW         6\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 1498 \n1498           0 LOAD_GLOBAL             14 (__compiled_fn_8)\n               2 LOAD_FAST                0 (___stack0)\n               4 LOAD_ATTR               15 (last_hidden_state)\n               6 LOAD_FAST                2 (start_positions)\n               8 LOAD_FAST                3 (end_positions)\n              10 CALL_FUNCTION            3\n              12 STORE_FAST              39 (graph_out_0)\n              14 LOAD_CONST              10 (<class 'transformers.modeling_outputs.QuestionAnsweringModelOutput'>)\n              16 LOAD_FAST               39 (graph_out_0)\n              18 LOAD_CONST               3 (0)\n              20 BINARY_SUBSCR\n              22 LOAD_FAST               39 (graph_out_0)\n              24 LOAD_CONST               4 (1)\n              26 BINARY_SUBSCR\n              28 LOAD_FAST               39 (graph_out_0)\n              30 LOAD_CONST               8 (2)\n              32 BINARY_SUBSCR\n              34 LOAD_FAST                0 (___stack0)\n              36 LOAD_ATTR               12 (hidden_states)\n              38 LOAD_FAST                0 (___stack0)\n              40 LOAD_ATTR               13 (attentions)\n              42 LOAD_CONST              11 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n              44 CALL_FUNCTION_KW         5\n              46 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_9)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_10)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_11)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             11 (__import_contextlib)\n              2 LOAD_ATTR               12 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             11 (__import_contextlib)\n             28 LOAD_ATTR               12 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              49 (___context_manager_0_3)\n             34 LOAD_FAST               49 (___context_manager_0_3)\n             36 LOAD_METHOD             13 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               49 (___context_manager_0_3)\n             50 LOAD_METHOD             14 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               49 (___context_manager_0_3)\n             68 LOAD_METHOD             14 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             15 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py line 791 \n791           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                4 (model)\n              4 LOAD_ATTR                5 (decoder)\n              6 LOAD_FAST                1 (input_ids)\n              8 LOAD_FAST                2 (attention_mask)\n             10 LOAD_FAST                3 (encoder_hidden_states)\n             12 LOAD_FAST                4 (encoder_attention_mask)\n             14 LOAD_FAST                5 (head_mask)\n             16 LOAD_FAST                6 (cross_attn_head_mask)\n             18 LOAD_FAST                7 (past_key_values)\n             20 LOAD_FAST                8 (inputs_embeds)\n             22 LOAD_FAST               10 (use_cache)\n             24 LOAD_FAST                0 (self)\n             26 LOAD_ATTR                0 (config)\n             28 LOAD_ATTR                1 (output_attentions)\n             30 LOAD_FAST                0 (self)\n             32 LOAD_ATTR                0 (config)\n             34 LOAD_ATTR                2 (output_hidden_states)\n             36 LOAD_FAST                0 (self)\n             38 LOAD_ATTR                0 (config)\n             40 LOAD_ATTR                3 (use_return_dict)\n             42 LOAD_CONST               2 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n             44 LOAD_FAST                0 (self)\n             46 LOAD_ATTR                0 (config)\n             48 LOAD_ATTR                3 (use_return_dict)\n             50 STORE_FAST              13 (return_dict)\n\n918          52 CALL_FUNCTION_KW        12\n             54 LOAD_GLOBAL             16 (__resume_at_94_5)\n             56 ROT_TWO\n             58 LOAD_FAST                0 (self)\n             60 LOAD_FAST                9 (labels)\n             62 LOAD_FAST               13 (return_dict)\n             64 CALL_FUNCTION            4\n             66 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py line 499 \n499           0 LOAD_GLOBAL              4 (__compiled_fn_6)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py line 117 \n117           0 LOAD_GLOBAL             12 (__compiled_fn_7)\n              2 LOAD_FAST                1 (input_ids)\n              4 CALL_FUNCTION            1\n              6 UNPACK_SEQUENCE          1\n              8 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py line 334 \n334           0 LOAD_GLOBAL             14 (__compiled_fn_8)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              62 (graph_out_0)\n             10 LOAD_FAST               62 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               62 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               62 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py line 334 \n334           0 LOAD_GLOBAL             14 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              62 (graph_out_0)\n             10 LOAD_FAST               62 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               62 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               62 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py line 334 \n334           0 LOAD_GLOBAL             14 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              62 (graph_out_0)\n             10 LOAD_FAST               62 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               62 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               62 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py line 334 \n334           0 LOAD_GLOBAL             14 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              62 (graph_out_0)\n             10 LOAD_FAST               62 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               62 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               62 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py line 334 \n334           0 LOAD_GLOBAL             14 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              62 (graph_out_0)\n             10 LOAD_FAST               62 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               62 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               62 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py line 334 \n334           0 LOAD_GLOBAL             14 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              62 (graph_out_0)\n             10 LOAD_FAST               62 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               62 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               62 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_14)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              6 (__resume_at_14_15)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f1627403d60, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_16')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f1626ea0df0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_17')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f1627403d60, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_18')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f1626ea0df0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_19')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py line 923 \n923           0 LOAD_GLOBAL             15 (__compiled_fn_20)\n              2 LOAD_FAST                0 (___stack0)\n              4 LOAD_ATTR               16 (last_hidden_state)\n              6 LOAD_FAST                2 (labels)\n              8 CALL_FUNCTION            2\n             10 STORE_FAST              25 (graph_out_0)\n             12 LOAD_CONST               7 (<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>)\n             14 LOAD_FAST               25 (graph_out_0)\n             16 LOAD_CONST               3 (0)\n             18 BINARY_SUBSCR\n             20 LOAD_FAST               25 (graph_out_0)\n             22 LOAD_CONST               5 (1)\n             24 BINARY_SUBSCR\n             26 LOAD_FAST                0 (___stack0)\n             28 LOAD_ATTR               11 (past_key_values)\n             30 LOAD_FAST                0 (___stack0)\n             32 LOAD_ATTR               12 (hidden_states)\n             34 LOAD_FAST                0 (___stack0)\n             36 LOAD_ATTR               13 (attentions)\n             38 LOAD_FAST                0 (___stack0)\n             40 LOAD_ATTR               14 (cross_attentions)\n             42 LOAD_CONST               8 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n             44 CALL_FUNCTION_KW         6\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_21)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_22)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_23)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             21 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('decoder_input_ids')\n             12 BINARY_SUBSCR\n             14 LOAD_FAST                4 (cloned_inputs)\n             16 LOAD_CONST               4 ('labels')\n             18 BINARY_SUBSCR\n             20 CALL_FUNCTION            3\n             22 EXTENDED_ARG             4\n             24 STORE_FAST            1132 (graph_out_0)\n             26 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             28 LOAD_ATTR               13 (make_cell)\n             30 CALL_FUNCTION            0\n             32 EXTENDED_ARG             4\n             34 STORE_FAST            1169 (tmp_36)\n             36 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             38 LOAD_ATTR               13 (make_cell)\n             40 CALL_FUNCTION            0\n             42 EXTENDED_ARG             4\n             44 STORE_FAST            1170 (tmp_37)\n             46 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             48 LOAD_ATTR               13 (make_cell)\n             50 CALL_FUNCTION            0\n             52 EXTENDED_ARG             4\n             54 STORE_FAST            1171 (tmp_38)\n             56 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             58 LOAD_ATTR               13 (make_cell)\n             60 CALL_FUNCTION            0\n             62 EXTENDED_ARG             4\n             64 STORE_FAST            1172 (tmp_39)\n             66 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             68 LOAD_ATTR               13 (make_cell)\n             70 CALL_FUNCTION            0\n             72 EXTENDED_ARG             4\n             74 STORE_FAST            1173 (tmp_40)\n             76 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             78 LOAD_ATTR               13 (make_cell)\n             80 CALL_FUNCTION            0\n             82 EXTENDED_ARG             4\n             84 STORE_FAST            1174 (tmp_41)\n             86 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             88 LOAD_ATTR               13 (make_cell)\n             90 CALL_FUNCTION            0\n             92 EXTENDED_ARG             4\n             94 STORE_FAST            1175 (tmp_42)\n             96 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             98 LOAD_ATTR               13 (make_cell)\n            100 CALL_FUNCTION            0\n            102 EXTENDED_ARG             4\n            104 STORE_FAST            1176 (tmp_43)\n            106 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            108 LOAD_ATTR               13 (make_cell)\n            110 CALL_FUNCTION            0\n            112 EXTENDED_ARG             4\n            114 STORE_FAST            1177 (tmp_44)\n            116 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            118 LOAD_ATTR               13 (make_cell)\n            120 CALL_FUNCTION            0\n            122 EXTENDED_ARG             4\n            124 STORE_FAST            1178 (tmp_45)\n            126 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            128 LOAD_ATTR               13 (make_cell)\n            130 CALL_FUNCTION            0\n            132 EXTENDED_ARG             4\n            134 STORE_FAST            1179 (tmp_46)\n            136 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            138 LOAD_ATTR               13 (make_cell)\n            140 CALL_FUNCTION            0\n            142 EXTENDED_ARG             4\n            144 STORE_FAST            1180 (tmp_47)\n            146 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            148 LOAD_ATTR               13 (make_cell)\n            150 CALL_FUNCTION            0\n            152 EXTENDED_ARG             4\n            154 STORE_FAST            1181 (tmp_48)\n            156 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            158 LOAD_ATTR               13 (make_cell)\n            160 CALL_FUNCTION            0\n            162 EXTENDED_ARG             4\n            164 STORE_FAST            1182 (tmp_49)\n            166 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            168 LOAD_ATTR               13 (make_cell)\n            170 CALL_FUNCTION            0\n            172 EXTENDED_ARG             4\n            174 STORE_FAST            1183 (tmp_50)\n            176 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            178 LOAD_ATTR               13 (make_cell)\n            180 CALL_FUNCTION            0\n            182 EXTENDED_ARG             4\n            184 STORE_FAST            1184 (tmp_51)\n            186 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            188 LOAD_ATTR               13 (make_cell)\n            190 CALL_FUNCTION            0\n            192 EXTENDED_ARG             4\n            194 STORE_FAST            1185 (tmp_52)\n            196 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            198 LOAD_ATTR               13 (make_cell)\n            200 CALL_FUNCTION            0\n            202 EXTENDED_ARG             4\n            204 STORE_FAST            1186 (tmp_53)\n            206 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            208 LOAD_ATTR               13 (make_cell)\n            210 CALL_FUNCTION            0\n            212 EXTENDED_ARG             4\n            214 STORE_FAST            1187 (tmp_54)\n            216 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            218 LOAD_ATTR               13 (make_cell)\n            220 CALL_FUNCTION            0\n            222 EXTENDED_ARG             4\n            224 STORE_FAST            1188 (tmp_55)\n            226 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            228 LOAD_ATTR               13 (make_cell)\n            230 CALL_FUNCTION            0\n            232 EXTENDED_ARG             4\n            234 STORE_FAST            1189 (tmp_56)\n            236 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            238 LOAD_ATTR               13 (make_cell)\n            240 CALL_FUNCTION            0\n            242 EXTENDED_ARG             4\n            244 STORE_FAST            1190 (tmp_57)\n            246 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            248 LOAD_ATTR               13 (make_cell)\n            250 CALL_FUNCTION            0\n            252 EXTENDED_ARG             4\n            254 STORE_FAST            1191 (tmp_58)\n            256 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            258 LOAD_ATTR               13 (make_cell)\n            260 CALL_FUNCTION            0\n            262 EXTENDED_ARG             4\n            264 STORE_FAST            1192 (tmp_59)\n            266 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            268 LOAD_ATTR               13 (make_cell)\n            270 CALL_FUNCTION            0\n            272 EXTENDED_ARG             4\n            274 STORE_FAST            1193 (tmp_60)\n            276 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            278 LOAD_ATTR               13 (make_cell)\n            280 CALL_FUNCTION            0\n            282 EXTENDED_ARG             4\n            284 STORE_FAST            1194 (tmp_61)\n            286 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            288 LOAD_ATTR               13 (make_cell)\n            290 CALL_FUNCTION            0\n            292 EXTENDED_ARG             4\n            294 STORE_FAST            1195 (tmp_62)\n            296 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            298 LOAD_ATTR               13 (make_cell)\n            300 CALL_FUNCTION            0\n            302 EXTENDED_ARG             4\n            304 STORE_FAST            1196 (tmp_63)\n            306 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            308 LOAD_ATTR               13 (make_cell)\n            310 CALL_FUNCTION            0\n            312 EXTENDED_ARG             4\n            314 STORE_FAST            1197 (tmp_64)\n            316 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            318 LOAD_ATTR               13 (make_cell)\n            320 CALL_FUNCTION            0\n            322 EXTENDED_ARG             4\n            324 STORE_FAST            1198 (tmp_65)\n            326 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            328 LOAD_ATTR               13 (make_cell)\n            330 CALL_FUNCTION            0\n            332 EXTENDED_ARG             4\n            334 STORE_FAST            1199 (tmp_66)\n            336 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            338 LOAD_ATTR               13 (make_cell)\n            340 CALL_FUNCTION            0\n            342 EXTENDED_ARG             4\n            344 STORE_FAST            1200 (tmp_67)\n            346 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            348 LOAD_ATTR               13 (make_cell)\n            350 CALL_FUNCTION            0\n            352 EXTENDED_ARG             4\n            354 STORE_FAST            1201 (tmp_68)\n            356 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            358 LOAD_ATTR               13 (make_cell)\n            360 CALL_FUNCTION            0\n            362 EXTENDED_ARG             4\n            364 STORE_FAST            1202 (tmp_69)\n            366 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            368 LOAD_ATTR               13 (make_cell)\n            370 CALL_FUNCTION            0\n            372 EXTENDED_ARG             4\n            374 STORE_FAST            1203 (tmp_70)\n            376 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            378 LOAD_ATTR               13 (make_cell)\n            380 CALL_FUNCTION            0\n            382 EXTENDED_ARG             4\n            384 STORE_FAST            1204 (tmp_71)\n            386 EXTENDED_ARG             4\n            388 LOAD_FAST             1132 (graph_out_0)\n            390 LOAD_CONST               5 (0)\n            392 BINARY_SUBSCR\n            394 LOAD_ATTR                6 (backward)\n            396 LOAD_CONST               6 (<class 'transformers.modeling_outputs.Seq2SeqLMOutput'>)\n            398 EXTENDED_ARG             4\n            400 LOAD_FAST             1132 (graph_out_0)\n            402 LOAD_CONST               5 (0)\n            404 BINARY_SUBSCR\n            406 EXTENDED_ARG             4\n            408 LOAD_FAST             1132 (graph_out_0)\n            410 LOAD_CONST               7 (1)\n            412 BINARY_SUBSCR\n            414 EXTENDED_ARG             4\n            416 LOAD_FAST             1132 (graph_out_0)\n            418 LOAD_CONST               8 (2)\n            420 BINARY_SUBSCR\n            422 EXTENDED_ARG             4\n            424 LOAD_FAST             1132 (graph_out_0)\n            426 LOAD_CONST               9 (3)\n            428 BINARY_SUBSCR\n            430 EXTENDED_ARG             4\n            432 LOAD_FAST             1132 (graph_out_0)\n            434 LOAD_CONST              10 (4)\n            436 BINARY_SUBSCR\n            438 EXTENDED_ARG             4\n            440 LOAD_FAST             1132 (graph_out_0)\n            442 LOAD_CONST              11 (5)\n            444 BINARY_SUBSCR\n            446 BUILD_TUPLE              4\n            448 EXTENDED_ARG             4\n            450 LOAD_FAST             1132 (graph_out_0)\n            452 LOAD_CONST              12 (6)\n            454 BINARY_SUBSCR\n            456 EXTENDED_ARG             4\n            458 LOAD_FAST             1132 (graph_out_0)\n            460 LOAD_CONST              13 (7)\n            462 BINARY_SUBSCR\n            464 EXTENDED_ARG             4\n            466 LOAD_FAST             1132 (graph_out_0)\n            468 LOAD_CONST              14 (8)\n            470 BINARY_SUBSCR\n            472 EXTENDED_ARG             4\n            474 LOAD_FAST             1132 (graph_out_0)\n            476 LOAD_CONST              15 (9)\n            478 BINARY_SUBSCR\n            480 BUILD_TUPLE              4\n            482 EXTENDED_ARG             4\n            484 LOAD_FAST             1132 (graph_out_0)\n            486 LOAD_CONST              16 (10)\n            488 BINARY_SUBSCR\n            490 EXTENDED_ARG             4\n            492 LOAD_FAST             1132 (graph_out_0)\n            494 LOAD_CONST              17 (11)\n            496 BINARY_SUBSCR\n            498 EXTENDED_ARG             4\n            500 LOAD_FAST             1132 (graph_out_0)\n            502 LOAD_CONST              18 (12)\n            504 BINARY_SUBSCR\n            506 EXTENDED_ARG             4\n            508 LOAD_FAST             1132 (graph_out_0)\n            510 LOAD_CONST              19 (13)\n            512 BINARY_SUBSCR\n            514 BUILD_TUPLE              4\n            516 EXTENDED_ARG             4\n            518 LOAD_FAST             1132 (graph_out_0)\n            520 LOAD_CONST              20 (14)\n            522 BINARY_SUBSCR\n            524 EXTENDED_ARG             4\n            526 LOAD_FAST             1132 (graph_out_0)\n            528 LOAD_CONST              21 (15)\n            530 BINARY_SUBSCR\n            532 EXTENDED_ARG             4\n            534 LOAD_FAST             1132 (graph_out_0)\n            536 LOAD_CONST              22 (16)\n            538 BINARY_SUBSCR\n            540 EXTENDED_ARG             4\n            542 LOAD_FAST             1132 (graph_out_0)\n            544 LOAD_CONST              23 (17)\n            546 BINARY_SUBSCR\n            548 BUILD_TUPLE              4\n            550 EXTENDED_ARG             4\n            552 LOAD_FAST             1132 (graph_out_0)\n            554 LOAD_CONST              24 (18)\n            556 BINARY_SUBSCR\n            558 EXTENDED_ARG             4\n            560 LOAD_FAST             1132 (graph_out_0)\n            562 LOAD_CONST              25 (19)\n            564 BINARY_SUBSCR\n            566 EXTENDED_ARG             4\n            568 LOAD_FAST             1132 (graph_out_0)\n            570 LOAD_CONST              26 (20)\n            572 BINARY_SUBSCR\n            574 EXTENDED_ARG             4\n            576 LOAD_FAST             1132 (graph_out_0)\n            578 LOAD_CONST              27 (21)\n            580 BINARY_SUBSCR\n            582 BUILD_TUPLE              4\n            584 EXTENDED_ARG             4\n            586 LOAD_FAST             1132 (graph_out_0)\n            588 LOAD_CONST              28 (22)\n            590 BINARY_SUBSCR\n            592 EXTENDED_ARG             4\n            594 LOAD_FAST             1132 (graph_out_0)\n            596 LOAD_CONST              29 (23)\n            598 BINARY_SUBSCR\n            600 EXTENDED_ARG             4\n            602 LOAD_FAST             1132 (graph_out_0)\n            604 LOAD_CONST              30 (24)\n            606 BINARY_SUBSCR\n            608 EXTENDED_ARG             4\n            610 LOAD_FAST             1132 (graph_out_0)\n            612 LOAD_CONST              31 (25)\n            614 BINARY_SUBSCR\n            616 BUILD_TUPLE              4\n            618 BUILD_TUPLE              6\n            620 LOAD_CONST               0 (None)\n            622 LOAD_CONST               0 (None)\n            624 LOAD_CONST               0 (None)\n            626 EXTENDED_ARG             4\n            628 LOAD_FAST             1132 (graph_out_0)\n            630 LOAD_CONST              32 (26)\n            632 BINARY_SUBSCR\n            634 LOAD_CONST               0 (None)\n            636 LOAD_CONST               0 (None)\n            638 LOAD_CONST              33 (('loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'))\n            640 CALL_FUNCTION_KW         9\n            642 EXTENDED_ARG             4\n            644 LOAD_FAST             1132 (graph_out_0)\n            646 LOAD_CONST               5 (0)\n            648 BINARY_SUBSCR\n            650 LOAD_CONST              10 (4)\n            652 EXTENDED_ARG             4\n            654 LOAD_FAST             1169 (tmp_36)\n            656 LOAD_FAST                2 (mod)\n            658 LOAD_ATTR               15 (encoder)\n            660 LOAD_ATTR               16 (block)\n            662 LOAD_CONST               5 (0)\n            664 BINARY_SUBSCR\n            666 LOAD_ATTR               17 (layer)\n            668 LOAD_CONST               5 (0)\n            670 BINARY_SUBSCR\n            672 LOAD_ATTR               18 (SelfAttention)\n            674 EXTENDED_ARG             4\n            676 LOAD_FAST             1170 (tmp_37)\n            678 LOAD_CONST              10 (4)\n            680 EXTENDED_ARG             4\n            682 LOAD_FAST             1171 (tmp_38)\n            684 LOAD_FAST                2 (mod)\n            686 LOAD_ATTR               15 (encoder)\n            688 LOAD_ATTR               16 (block)\n            690 LOAD_CONST               7 (1)\n            692 BINARY_SUBSCR\n            694 LOAD_ATTR               17 (layer)\n            696 LOAD_CONST               5 (0)\n            698 BINARY_SUBSCR\n            700 LOAD_ATTR               18 (SelfAttention)\n            702 EXTENDED_ARG             4\n            704 LOAD_FAST             1172 (tmp_39)\n            706 LOAD_CONST              10 (4)\n            708 EXTENDED_ARG             4\n            710 LOAD_FAST             1173 (tmp_40)\n            712 LOAD_FAST                2 (mod)\n            714 LOAD_ATTR               15 (encoder)\n            716 LOAD_ATTR               16 (block)\n            718 LOAD_CONST               8 (2)\n            720 BINARY_SUBSCR\n            722 LOAD_ATTR               17 (layer)\n            724 LOAD_CONST               5 (0)\n            726 BINARY_SUBSCR\n            728 LOAD_ATTR               18 (SelfAttention)\n            730 EXTENDED_ARG             4\n            732 LOAD_FAST             1174 (tmp_41)\n            734 LOAD_CONST              10 (4)\n            736 EXTENDED_ARG             4\n            738 LOAD_FAST             1175 (tmp_42)\n            740 LOAD_FAST                2 (mod)\n            742 LOAD_ATTR               15 (encoder)\n            744 LOAD_ATTR               16 (block)\n            746 LOAD_CONST               9 (3)\n            748 BINARY_SUBSCR\n            750 LOAD_ATTR               17 (layer)\n            752 LOAD_CONST               5 (0)\n            754 BINARY_SUBSCR\n            756 LOAD_ATTR               18 (SelfAttention)\n            758 EXTENDED_ARG             4\n            760 LOAD_FAST             1176 (tmp_43)\n            762 LOAD_CONST              10 (4)\n            764 EXTENDED_ARG             4\n            766 LOAD_FAST             1177 (tmp_44)\n            768 LOAD_FAST                2 (mod)\n            770 LOAD_ATTR               15 (encoder)\n            772 LOAD_ATTR               16 (block)\n            774 LOAD_CONST              10 (4)\n            776 BINARY_SUBSCR\n            778 LOAD_ATTR               17 (layer)\n            780 LOAD_CONST               5 (0)\n            782 BINARY_SUBSCR\n            784 LOAD_ATTR               18 (SelfAttention)\n            786 EXTENDED_ARG             4\n            788 LOAD_FAST             1178 (tmp_45)\n            790 LOAD_CONST              10 (4)\n            792 EXTENDED_ARG             4\n            794 LOAD_FAST             1179 (tmp_46)\n            796 LOAD_FAST                2 (mod)\n            798 LOAD_ATTR               15 (encoder)\n            800 LOAD_ATTR               16 (block)\n            802 LOAD_CONST              11 (5)\n            804 BINARY_SUBSCR\n            806 LOAD_ATTR               17 (layer)\n            808 LOAD_CONST               5 (0)\n            810 BINARY_SUBSCR\n            812 LOAD_ATTR               18 (SelfAttention)\n            814 EXTENDED_ARG             4\n            816 LOAD_FAST             1180 (tmp_47)\n            818 LOAD_CONST              10 (4)\n            820 EXTENDED_ARG             4\n            822 LOAD_FAST             1181 (tmp_48)\n            824 LOAD_FAST                2 (mod)\n            826 LOAD_ATTR               19 (decoder)\n            828 LOAD_ATTR               16 (block)\n            830 LOAD_CONST               5 (0)\n            832 BINARY_SUBSCR\n            834 LOAD_ATTR               17 (layer)\n            836 LOAD_CONST               5 (0)\n            838 BINARY_SUBSCR\n            840 LOAD_ATTR               18 (SelfAttention)\n            842 EXTENDED_ARG             4\n            844 LOAD_FAST             1182 (tmp_49)\n            846 LOAD_CONST              10 (4)\n            848 EXTENDED_ARG             4\n            850 LOAD_FAST             1183 (tmp_50)\n            852 LOAD_FAST                2 (mod)\n            854 LOAD_ATTR               19 (decoder)\n            856 LOAD_ATTR               16 (block)\n            858 LOAD_CONST               5 (0)\n            860 BINARY_SUBSCR\n            862 LOAD_ATTR               17 (layer)\n            864 LOAD_CONST               7 (1)\n            866 BINARY_SUBSCR\n            868 LOAD_ATTR               20 (EncDecAttention)\n            870 EXTENDED_ARG             4\n            872 LOAD_FAST             1184 (tmp_51)\n            874 LOAD_CONST              10 (4)\n            876 EXTENDED_ARG             4\n            878 LOAD_FAST             1185 (tmp_52)\n            880 LOAD_FAST                2 (mod)\n            882 LOAD_ATTR               19 (decoder)\n            884 LOAD_ATTR               16 (block)\n            886 LOAD_CONST               7 (1)\n            888 BINARY_SUBSCR\n            890 LOAD_ATTR               17 (layer)\n            892 LOAD_CONST               5 (0)\n            894 BINARY_SUBSCR\n            896 LOAD_ATTR               18 (SelfAttention)\n            898 EXTENDED_ARG             4\n            900 LOAD_FAST             1186 (tmp_53)\n            902 LOAD_CONST              10 (4)\n            904 EXTENDED_ARG             4\n            906 LOAD_FAST             1187 (tmp_54)\n            908 LOAD_FAST                2 (mod)\n            910 LOAD_ATTR               19 (decoder)\n            912 LOAD_ATTR               16 (block)\n            914 LOAD_CONST               7 (1)\n            916 BINARY_SUBSCR\n            918 LOAD_ATTR               17 (layer)\n            920 LOAD_CONST               7 (1)\n            922 BINARY_SUBSCR\n            924 LOAD_ATTR               20 (EncDecAttention)\n            926 EXTENDED_ARG             4\n            928 LOAD_FAST             1188 (tmp_55)\n            930 LOAD_CONST              10 (4)\n            932 EXTENDED_ARG             4\n            934 LOAD_FAST             1189 (tmp_56)\n            936 LOAD_FAST                2 (mod)\n            938 LOAD_ATTR               19 (decoder)\n            940 LOAD_ATTR               16 (block)\n            942 LOAD_CONST               8 (2)\n            944 BINARY_SUBSCR\n            946 LOAD_ATTR               17 (layer)\n            948 LOAD_CONST               5 (0)\n            950 BINARY_SUBSCR\n            952 LOAD_ATTR               18 (SelfAttention)\n            954 EXTENDED_ARG             4\n            956 LOAD_FAST             1190 (tmp_57)\n            958 LOAD_CONST              10 (4)\n            960 EXTENDED_ARG             4\n            962 LOAD_FAST             1191 (tmp_58)\n            964 LOAD_FAST                2 (mod)\n            966 LOAD_ATTR               19 (decoder)\n            968 LOAD_ATTR               16 (block)\n            970 LOAD_CONST               8 (2)\n            972 BINARY_SUBSCR\n            974 LOAD_ATTR               17 (layer)\n            976 LOAD_CONST               7 (1)\n            978 BINARY_SUBSCR\n            980 LOAD_ATTR               20 (EncDecAttention)\n            982 EXTENDED_ARG             4\n            984 LOAD_FAST             1192 (tmp_59)\n            986 LOAD_CONST              10 (4)\n            988 EXTENDED_ARG             4\n            990 LOAD_FAST             1193 (tmp_60)\n            992 LOAD_FAST                2 (mod)\n            994 LOAD_ATTR               19 (decoder)\n            996 LOAD_ATTR               16 (block)\n            998 LOAD_CONST               9 (3)\n           1000 BINARY_SUBSCR\n           1002 LOAD_ATTR               17 (layer)\n           1004 LOAD_CONST               5 (0)\n           1006 BINARY_SUBSCR\n           1008 LOAD_ATTR               18 (SelfAttention)\n           1010 EXTENDED_ARG             4\n           1012 LOAD_FAST             1194 (tmp_61)\n           1014 LOAD_CONST              10 (4)\n           1016 EXTENDED_ARG             4\n           1018 LOAD_FAST             1195 (tmp_62)\n           1020 LOAD_FAST                2 (mod)\n           1022 LOAD_ATTR               19 (decoder)\n           1024 LOAD_ATTR               16 (block)\n           1026 LOAD_CONST               9 (3)\n           1028 BINARY_SUBSCR\n           1030 LOAD_ATTR               17 (layer)\n           1032 LOAD_CONST               7 (1)\n           1034 BINARY_SUBSCR\n           1036 LOAD_ATTR               20 (EncDecAttention)\n           1038 EXTENDED_ARG             4\n           1040 LOAD_FAST             1196 (tmp_63)\n           1042 LOAD_CONST              10 (4)\n           1044 EXTENDED_ARG             4\n           1046 LOAD_FAST             1197 (tmp_64)\n           1048 LOAD_FAST                2 (mod)\n           1050 LOAD_ATTR               19 (decoder)\n           1052 LOAD_ATTR               16 (block)\n           1054 LOAD_CONST              10 (4)\n           1056 BINARY_SUBSCR\n           1058 LOAD_ATTR               17 (layer)\n           1060 LOAD_CONST               5 (0)\n           1062 BINARY_SUBSCR\n           1064 LOAD_ATTR               18 (SelfAttention)\n           1066 EXTENDED_ARG             4\n           1068 LOAD_FAST             1198 (tmp_65)\n           1070 LOAD_CONST              10 (4)\n           1072 EXTENDED_ARG             4\n           1074 LOAD_FAST             1199 (tmp_66)\n           1076 LOAD_FAST                2 (mod)\n           1078 LOAD_ATTR               19 (decoder)\n           1080 LOAD_ATTR               16 (block)\n           1082 LOAD_CONST              10 (4)\n           1084 BINARY_SUBSCR\n           1086 LOAD_ATTR               17 (layer)\n           1088 LOAD_CONST               7 (1)\n           1090 BINARY_SUBSCR\n           1092 LOAD_ATTR               20 (EncDecAttention)\n           1094 EXTENDED_ARG             4\n           1096 LOAD_FAST             1200 (tmp_67)\n           1098 LOAD_CONST              10 (4)\n           1100 EXTENDED_ARG             4\n           1102 LOAD_FAST             1201 (tmp_68)\n           1104 LOAD_FAST                2 (mod)\n           1106 LOAD_ATTR               19 (decoder)\n           1108 LOAD_ATTR               16 (block)\n           1110 LOAD_CONST              11 (5)\n           1112 BINARY_SUBSCR\n           1114 LOAD_ATTR               17 (layer)\n           1116 LOAD_CONST               5 (0)\n           1118 BINARY_SUBSCR\n           1120 LOAD_ATTR               18 (SelfAttention)\n           1122 EXTENDED_ARG             4\n           1124 LOAD_FAST             1202 (tmp_69)\n           1126 LOAD_CONST              10 (4)\n           1128 EXTENDED_ARG             4\n           1130 LOAD_FAST             1203 (tmp_70)\n           1132 LOAD_FAST                2 (mod)\n           1134 LOAD_ATTR               19 (decoder)\n           1136 LOAD_ATTR               16 (block)\n           1138 LOAD_CONST              11 (5)\n           1140 BINARY_SUBSCR\n           1142 LOAD_ATTR               17 (layer)\n           1144 LOAD_CONST               7 (1)\n           1146 BINARY_SUBSCR\n           1148 LOAD_ATTR               20 (EncDecAttention)\n           1150 EXTENDED_ARG             4\n           1152 LOAD_FAST             1204 (tmp_71)\n           1154 STORE_ATTR              14 (cell_contents)\n           1156 STORE_ATTR              14 (cell_contents)\n           1158 STORE_ATTR              14 (cell_contents)\n           1160 STORE_ATTR              14 (cell_contents)\n           1162 STORE_ATTR              14 (cell_contents)\n           1164 STORE_ATTR              14 (cell_contents)\n           1166 STORE_ATTR              14 (cell_contents)\n           1168 STORE_ATTR              14 (cell_contents)\n           1170 STORE_ATTR              14 (cell_contents)\n           1172 STORE_ATTR              14 (cell_contents)\n           1174 STORE_ATTR              14 (cell_contents)\n           1176 STORE_ATTR              14 (cell_contents)\n           1178 STORE_ATTR              14 (cell_contents)\n           1180 STORE_ATTR              14 (cell_contents)\n           1182 STORE_ATTR              14 (cell_contents)\n           1184 STORE_ATTR              14 (cell_contents)\n           1186 STORE_ATTR              14 (cell_contents)\n           1188 STORE_ATTR              14 (cell_contents)\n           1190 STORE_ATTR              14 (cell_contents)\n           1192 STORE_ATTR              14 (cell_contents)\n           1194 STORE_ATTR              14 (cell_contents)\n           1196 STORE_ATTR              14 (cell_contents)\n           1198 STORE_ATTR              14 (cell_contents)\n           1200 STORE_ATTR              14 (cell_contents)\n           1202 STORE_ATTR              14 (cell_contents)\n           1204 STORE_ATTR              14 (cell_contents)\n           1206 STORE_ATTR              14 (cell_contents)\n           1208 STORE_ATTR              14 (cell_contents)\n           1210 STORE_ATTR              14 (cell_contents)\n           1212 STORE_ATTR              14 (cell_contents)\n           1214 STORE_ATTR              14 (cell_contents)\n           1216 STORE_ATTR              14 (cell_contents)\n           1218 STORE_ATTR              14 (cell_contents)\n           1220 STORE_ATTR              14 (cell_contents)\n           1222 STORE_ATTR              14 (cell_contents)\n           1224 STORE_ATTR              14 (cell_contents)\n           1226 STORE_FAST               7 (loss)\n           1228 STORE_FAST               6 (pred)\n\n559        1230 CALL_FUNCTION            0\n           1232 LOAD_GLOBAL             22 (__resume_at_100_4)\n           1234 ROT_TWO\n           1236 LOAD_FAST                1 (self)\n           1238 LOAD_FAST                2 (mod)\n           1240 LOAD_FAST                3 (collect_outputs)\n           1242 LOAD_FAST                4 (cloned_inputs)\n           1244 LOAD_FAST                6 (pred)\n           1246 LOAD_FAST                7 (loss)\n           1248 CALL_FUNCTION            7\n           1250 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             21 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('decoder_input_ids')\n             12 BINARY_SUBSCR\n             14 LOAD_FAST                4 (cloned_inputs)\n             16 LOAD_CONST               4 ('labels')\n             18 BINARY_SUBSCR\n             20 CALL_FUNCTION            3\n             22 EXTENDED_ARG             4\n             24 STORE_FAST            1132 (graph_out_0)\n             26 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             28 LOAD_ATTR               13 (make_cell)\n             30 CALL_FUNCTION            0\n             32 EXTENDED_ARG             4\n             34 STORE_FAST            1169 (tmp_36)\n             36 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             38 LOAD_ATTR               13 (make_cell)\n             40 CALL_FUNCTION            0\n             42 EXTENDED_ARG             4\n             44 STORE_FAST            1170 (tmp_37)\n             46 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             48 LOAD_ATTR               13 (make_cell)\n             50 CALL_FUNCTION            0\n             52 EXTENDED_ARG             4\n             54 STORE_FAST            1171 (tmp_38)\n             56 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             58 LOAD_ATTR               13 (make_cell)\n             60 CALL_FUNCTION            0\n             62 EXTENDED_ARG             4\n             64 STORE_FAST            1172 (tmp_39)\n             66 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             68 LOAD_ATTR               13 (make_cell)\n             70 CALL_FUNCTION            0\n             72 EXTENDED_ARG             4\n             74 STORE_FAST            1173 (tmp_40)\n             76 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             78 LOAD_ATTR               13 (make_cell)\n             80 CALL_FUNCTION            0\n             82 EXTENDED_ARG             4\n             84 STORE_FAST            1174 (tmp_41)\n             86 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             88 LOAD_ATTR               13 (make_cell)\n             90 CALL_FUNCTION            0\n             92 EXTENDED_ARG             4\n             94 STORE_FAST            1175 (tmp_42)\n             96 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n             98 LOAD_ATTR               13 (make_cell)\n            100 CALL_FUNCTION            0\n            102 EXTENDED_ARG             4\n            104 STORE_FAST            1176 (tmp_43)\n            106 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            108 LOAD_ATTR               13 (make_cell)\n            110 CALL_FUNCTION            0\n            112 EXTENDED_ARG             4\n            114 STORE_FAST            1177 (tmp_44)\n            116 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            118 LOAD_ATTR               13 (make_cell)\n            120 CALL_FUNCTION            0\n            122 EXTENDED_ARG             4\n            124 STORE_FAST            1178 (tmp_45)\n            126 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            128 LOAD_ATTR               13 (make_cell)\n            130 CALL_FUNCTION            0\n            132 EXTENDED_ARG             4\n            134 STORE_FAST            1179 (tmp_46)\n            136 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            138 LOAD_ATTR               13 (make_cell)\n            140 CALL_FUNCTION            0\n            142 EXTENDED_ARG             4\n            144 STORE_FAST            1180 (tmp_47)\n            146 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            148 LOAD_ATTR               13 (make_cell)\n            150 CALL_FUNCTION            0\n            152 EXTENDED_ARG             4\n            154 STORE_FAST            1181 (tmp_48)\n            156 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            158 LOAD_ATTR               13 (make_cell)\n            160 CALL_FUNCTION            0\n            162 EXTENDED_ARG             4\n            164 STORE_FAST            1182 (tmp_49)\n            166 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            168 LOAD_ATTR               13 (make_cell)\n            170 CALL_FUNCTION            0\n            172 EXTENDED_ARG             4\n            174 STORE_FAST            1183 (tmp_50)\n            176 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            178 LOAD_ATTR               13 (make_cell)\n            180 CALL_FUNCTION            0\n            182 EXTENDED_ARG             4\n            184 STORE_FAST            1184 (tmp_51)\n            186 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            188 LOAD_ATTR               13 (make_cell)\n            190 CALL_FUNCTION            0\n            192 EXTENDED_ARG             4\n            194 STORE_FAST            1185 (tmp_52)\n            196 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            198 LOAD_ATTR               13 (make_cell)\n            200 CALL_FUNCTION            0\n            202 EXTENDED_ARG             4\n            204 STORE_FAST            1186 (tmp_53)\n            206 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            208 LOAD_ATTR               13 (make_cell)\n            210 CALL_FUNCTION            0\n            212 EXTENDED_ARG             4\n            214 STORE_FAST            1187 (tmp_54)\n            216 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            218 LOAD_ATTR               13 (make_cell)\n            220 CALL_FUNCTION            0\n            222 EXTENDED_ARG             4\n            224 STORE_FAST            1188 (tmp_55)\n            226 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            228 LOAD_ATTR               13 (make_cell)\n            230 CALL_FUNCTION            0\n            232 EXTENDED_ARG             4\n            234 STORE_FAST            1189 (tmp_56)\n            236 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            238 LOAD_ATTR               13 (make_cell)\n            240 CALL_FUNCTION            0\n            242 EXTENDED_ARG             4\n            244 STORE_FAST            1190 (tmp_57)\n            246 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            248 LOAD_ATTR               13 (make_cell)\n            250 CALL_FUNCTION            0\n            252 EXTENDED_ARG             4\n            254 STORE_FAST            1191 (tmp_58)\n            256 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            258 LOAD_ATTR               13 (make_cell)\n            260 CALL_FUNCTION            0\n            262 EXTENDED_ARG             4\n            264 STORE_FAST            1192 (tmp_59)\n            266 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            268 LOAD_ATTR               13 (make_cell)\n            270 CALL_FUNCTION            0\n            272 EXTENDED_ARG             4\n            274 STORE_FAST            1193 (tmp_60)\n            276 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            278 LOAD_ATTR               13 (make_cell)\n            280 CALL_FUNCTION            0\n            282 EXTENDED_ARG             4\n            284 STORE_FAST            1194 (tmp_61)\n            286 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            288 LOAD_ATTR               13 (make_cell)\n            290 CALL_FUNCTION            0\n            292 EXTENDED_ARG             4\n            294 STORE_FAST            1195 (tmp_62)\n            296 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            298 LOAD_ATTR               13 (make_cell)\n            300 CALL_FUNCTION            0\n            302 EXTENDED_ARG             4\n            304 STORE_FAST            1196 (tmp_63)\n            306 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            308 LOAD_ATTR               13 (make_cell)\n            310 CALL_FUNCTION            0\n            312 EXTENDED_ARG             4\n            314 STORE_FAST            1197 (tmp_64)\n            316 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            318 LOAD_ATTR               13 (make_cell)\n            320 CALL_FUNCTION            0\n            322 EXTENDED_ARG             4\n            324 STORE_FAST            1198 (tmp_65)\n            326 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            328 LOAD_ATTR               13 (make_cell)\n            330 CALL_FUNCTION            0\n            332 EXTENDED_ARG             4\n            334 STORE_FAST            1199 (tmp_66)\n            336 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            338 LOAD_ATTR               13 (make_cell)\n            340 CALL_FUNCTION            0\n            342 EXTENDED_ARG             4\n            344 STORE_FAST            1200 (tmp_67)\n            346 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            348 LOAD_ATTR               13 (make_cell)\n            350 CALL_FUNCTION            0\n            352 EXTENDED_ARG             4\n            354 STORE_FAST            1201 (tmp_68)\n            356 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            358 LOAD_ATTR               13 (make_cell)\n            360 CALL_FUNCTION            0\n            362 EXTENDED_ARG             4\n            364 STORE_FAST            1202 (tmp_69)\n            366 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            368 LOAD_ATTR               13 (make_cell)\n            370 CALL_FUNCTION            0\n            372 EXTENDED_ARG             4\n            374 STORE_FAST            1203 (tmp_70)\n            376 LOAD_GLOBAL             12 (__import_torch_dot__dynamo_dot_utils)\n            378 LOAD_ATTR               13 (make_cell)\n            380 CALL_FUNCTION            0\n            382 EXTENDED_ARG             4\n            384 STORE_FAST            1204 (tmp_71)\n            386 EXTENDED_ARG             4\n            388 LOAD_FAST             1132 (graph_out_0)\n            390 LOAD_CONST               5 (0)\n            392 BINARY_SUBSCR\n            394 LOAD_ATTR                6 (backward)\n            396 LOAD_CONST               6 (<class 'transformers.modeling_outputs.Seq2SeqLMOutput'>)\n            398 EXTENDED_ARG             4\n            400 LOAD_FAST             1132 (graph_out_0)\n            402 LOAD_CONST               5 (0)\n            404 BINARY_SUBSCR\n            406 EXTENDED_ARG             4\n            408 LOAD_FAST             1132 (graph_out_0)\n            410 LOAD_CONST               7 (1)\n            412 BINARY_SUBSCR\n            414 EXTENDED_ARG             4\n            416 LOAD_FAST             1132 (graph_out_0)\n            418 LOAD_CONST               8 (2)\n            420 BINARY_SUBSCR\n            422 EXTENDED_ARG             4\n            424 LOAD_FAST             1132 (graph_out_0)\n            426 LOAD_CONST               9 (3)\n            428 BINARY_SUBSCR\n            430 EXTENDED_ARG             4\n            432 LOAD_FAST             1132 (graph_out_0)\n            434 LOAD_CONST              10 (4)\n            436 BINARY_SUBSCR\n            438 EXTENDED_ARG             4\n            440 LOAD_FAST             1132 (graph_out_0)\n            442 LOAD_CONST              11 (5)\n            444 BINARY_SUBSCR\n            446 BUILD_TUPLE              4\n            448 EXTENDED_ARG             4\n            450 LOAD_FAST             1132 (graph_out_0)\n            452 LOAD_CONST              12 (6)\n            454 BINARY_SUBSCR\n            456 EXTENDED_ARG             4\n            458 LOAD_FAST             1132 (graph_out_0)\n            460 LOAD_CONST              13 (7)\n            462 BINARY_SUBSCR\n            464 EXTENDED_ARG             4\n            466 LOAD_FAST             1132 (graph_out_0)\n            468 LOAD_CONST              14 (8)\n            470 BINARY_SUBSCR\n            472 EXTENDED_ARG             4\n            474 LOAD_FAST             1132 (graph_out_0)\n            476 LOAD_CONST              15 (9)\n            478 BINARY_SUBSCR\n            480 BUILD_TUPLE              4\n            482 EXTENDED_ARG             4\n            484 LOAD_FAST             1132 (graph_out_0)\n            486 LOAD_CONST              16 (10)\n            488 BINARY_SUBSCR\n            490 EXTENDED_ARG             4\n            492 LOAD_FAST             1132 (graph_out_0)\n            494 LOAD_CONST              17 (11)\n            496 BINARY_SUBSCR\n            498 EXTENDED_ARG             4\n            500 LOAD_FAST             1132 (graph_out_0)\n            502 LOAD_CONST              18 (12)\n            504 BINARY_SUBSCR\n            506 EXTENDED_ARG             4\n            508 LOAD_FAST             1132 (graph_out_0)\n            510 LOAD_CONST              19 (13)\n            512 BINARY_SUBSCR\n            514 BUILD_TUPLE              4\n            516 EXTENDED_ARG             4\n            518 LOAD_FAST             1132 (graph_out_0)\n            520 LOAD_CONST              20 (14)\n            522 BINARY_SUBSCR\n            524 EXTENDED_ARG             4\n            526 LOAD_FAST             1132 (graph_out_0)\n            528 LOAD_CONST              21 (15)\n            530 BINARY_SUBSCR\n            532 EXTENDED_ARG             4\n            534 LOAD_FAST             1132 (graph_out_0)\n            536 LOAD_CONST              22 (16)\n            538 BINARY_SUBSCR\n            540 EXTENDED_ARG             4\n            542 LOAD_FAST             1132 (graph_out_0)\n            544 LOAD_CONST              23 (17)\n            546 BINARY_SUBSCR\n            548 BUILD_TUPLE              4\n            550 EXTENDED_ARG             4\n            552 LOAD_FAST             1132 (graph_out_0)\n            554 LOAD_CONST              24 (18)\n            556 BINARY_SUBSCR\n            558 EXTENDED_ARG             4\n            560 LOAD_FAST             1132 (graph_out_0)\n            562 LOAD_CONST              25 (19)\n            564 BINARY_SUBSCR\n            566 EXTENDED_ARG             4\n            568 LOAD_FAST             1132 (graph_out_0)\n            570 LOAD_CONST              26 (20)\n            572 BINARY_SUBSCR\n            574 EXTENDED_ARG             4\n            576 LOAD_FAST             1132 (graph_out_0)\n            578 LOAD_CONST              27 (21)\n            580 BINARY_SUBSCR\n            582 BUILD_TUPLE              4\n            584 EXTENDED_ARG             4\n            586 LOAD_FAST             1132 (graph_out_0)\n            588 LOAD_CONST              28 (22)\n            590 BINARY_SUBSCR\n            592 EXTENDED_ARG             4\n            594 LOAD_FAST             1132 (graph_out_0)\n            596 LOAD_CONST              29 (23)\n            598 BINARY_SUBSCR\n            600 EXTENDED_ARG             4\n            602 LOAD_FAST             1132 (graph_out_0)\n            604 LOAD_CONST              30 (24)\n            606 BINARY_SUBSCR\n            608 EXTENDED_ARG             4\n            610 LOAD_FAST             1132 (graph_out_0)\n            612 LOAD_CONST              31 (25)\n            614 BINARY_SUBSCR\n            616 BUILD_TUPLE              4\n            618 BUILD_TUPLE              6\n            620 LOAD_CONST               0 (None)\n            622 LOAD_CONST               0 (None)\n            624 LOAD_CONST               0 (None)\n            626 EXTENDED_ARG             4\n            628 LOAD_FAST             1132 (graph_out_0)\n            630 LOAD_CONST              32 (26)\n            632 BINARY_SUBSCR\n            634 LOAD_CONST               0 (None)\n            636 LOAD_CONST               0 (None)\n            638 LOAD_CONST              33 (('loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'))\n            640 CALL_FUNCTION_KW         9\n            642 EXTENDED_ARG             4\n            644 LOAD_FAST             1132 (graph_out_0)\n            646 LOAD_CONST               5 (0)\n            648 BINARY_SUBSCR\n            650 LOAD_CONST              10 (4)\n            652 EXTENDED_ARG             4\n            654 LOAD_FAST             1169 (tmp_36)\n            656 LOAD_FAST                2 (mod)\n            658 LOAD_ATTR               15 (encoder)\n            660 LOAD_ATTR               16 (block)\n            662 LOAD_CONST               5 (0)\n            664 BINARY_SUBSCR\n            666 LOAD_ATTR               17 (layer)\n            668 LOAD_CONST               5 (0)\n            670 BINARY_SUBSCR\n            672 LOAD_ATTR               18 (SelfAttention)\n            674 EXTENDED_ARG             4\n            676 LOAD_FAST             1170 (tmp_37)\n            678 LOAD_CONST              10 (4)\n            680 EXTENDED_ARG             4\n            682 LOAD_FAST             1171 (tmp_38)\n            684 LOAD_FAST                2 (mod)\n            686 LOAD_ATTR               15 (encoder)\n            688 LOAD_ATTR               16 (block)\n            690 LOAD_CONST               7 (1)\n            692 BINARY_SUBSCR\n            694 LOAD_ATTR               17 (layer)\n            696 LOAD_CONST               5 (0)\n            698 BINARY_SUBSCR\n            700 LOAD_ATTR               18 (SelfAttention)\n            702 EXTENDED_ARG             4\n            704 LOAD_FAST             1172 (tmp_39)\n            706 LOAD_CONST              10 (4)\n            708 EXTENDED_ARG             4\n            710 LOAD_FAST             1173 (tmp_40)\n            712 LOAD_FAST                2 (mod)\n            714 LOAD_ATTR               15 (encoder)\n            716 LOAD_ATTR               16 (block)\n            718 LOAD_CONST               8 (2)\n            720 BINARY_SUBSCR\n            722 LOAD_ATTR               17 (layer)\n            724 LOAD_CONST               5 (0)\n            726 BINARY_SUBSCR\n            728 LOAD_ATTR               18 (SelfAttention)\n            730 EXTENDED_ARG             4\n            732 LOAD_FAST             1174 (tmp_41)\n            734 LOAD_CONST              10 (4)\n            736 EXTENDED_ARG             4\n            738 LOAD_FAST             1175 (tmp_42)\n            740 LOAD_FAST                2 (mod)\n            742 LOAD_ATTR               15 (encoder)\n            744 LOAD_ATTR               16 (block)\n            746 LOAD_CONST               9 (3)\n            748 BINARY_SUBSCR\n            750 LOAD_ATTR               17 (layer)\n            752 LOAD_CONST               5 (0)\n            754 BINARY_SUBSCR\n            756 LOAD_ATTR               18 (SelfAttention)\n            758 EXTENDED_ARG             4\n            760 LOAD_FAST             1176 (tmp_43)\n            762 LOAD_CONST              10 (4)\n            764 EXTENDED_ARG             4\n            766 LOAD_FAST             1177 (tmp_44)\n            768 LOAD_FAST                2 (mod)\n            770 LOAD_ATTR               15 (encoder)\n            772 LOAD_ATTR               16 (block)\n            774 LOAD_CONST              10 (4)\n            776 BINARY_SUBSCR\n            778 LOAD_ATTR               17 (layer)\n            780 LOAD_CONST               5 (0)\n            782 BINARY_SUBSCR\n            784 LOAD_ATTR               18 (SelfAttention)\n            786 EXTENDED_ARG             4\n            788 LOAD_FAST             1178 (tmp_45)\n            790 LOAD_CONST              10 (4)\n            792 EXTENDED_ARG             4\n            794 LOAD_FAST             1179 (tmp_46)\n            796 LOAD_FAST                2 (mod)\n            798 LOAD_ATTR               15 (encoder)\n            800 LOAD_ATTR               16 (block)\n            802 LOAD_CONST              11 (5)\n            804 BINARY_SUBSCR\n            806 LOAD_ATTR               17 (layer)\n            808 LOAD_CONST               5 (0)\n            810 BINARY_SUBSCR\n            812 LOAD_ATTR               18 (SelfAttention)\n            814 EXTENDED_ARG             4\n            816 LOAD_FAST             1180 (tmp_47)\n            818 LOAD_CONST              10 (4)\n            820 EXTENDED_ARG             4\n            822 LOAD_FAST             1181 (tmp_48)\n            824 LOAD_FAST                2 (mod)\n            826 LOAD_ATTR               19 (decoder)\n            828 LOAD_ATTR               16 (block)\n            830 LOAD_CONST               5 (0)\n            832 BINARY_SUBSCR\n            834 LOAD_ATTR               17 (layer)\n            836 LOAD_CONST               5 (0)\n            838 BINARY_SUBSCR\n            840 LOAD_ATTR               18 (SelfAttention)\n            842 EXTENDED_ARG             4\n            844 LOAD_FAST             1182 (tmp_49)\n            846 LOAD_CONST              10 (4)\n            848 EXTENDED_ARG             4\n            850 LOAD_FAST             1183 (tmp_50)\n            852 LOAD_FAST                2 (mod)\n            854 LOAD_ATTR               19 (decoder)\n            856 LOAD_ATTR               16 (block)\n            858 LOAD_CONST               5 (0)\n            860 BINARY_SUBSCR\n            862 LOAD_ATTR               17 (layer)\n            864 LOAD_CONST               7 (1)\n            866 BINARY_SUBSCR\n            868 LOAD_ATTR               20 (EncDecAttention)\n            870 EXTENDED_ARG             4\n            872 LOAD_FAST             1184 (tmp_51)\n            874 LOAD_CONST              10 (4)\n            876 EXTENDED_ARG             4\n            878 LOAD_FAST             1185 (tmp_52)\n            880 LOAD_FAST                2 (mod)\n            882 LOAD_ATTR               19 (decoder)\n            884 LOAD_ATTR               16 (block)\n            886 LOAD_CONST               7 (1)\n            888 BINARY_SUBSCR\n            890 LOAD_ATTR               17 (layer)\n            892 LOAD_CONST               5 (0)\n            894 BINARY_SUBSCR\n            896 LOAD_ATTR               18 (SelfAttention)\n            898 EXTENDED_ARG             4\n            900 LOAD_FAST             1186 (tmp_53)\n            902 LOAD_CONST              10 (4)\n            904 EXTENDED_ARG             4\n            906 LOAD_FAST             1187 (tmp_54)\n            908 LOAD_FAST                2 (mod)\n            910 LOAD_ATTR               19 (decoder)\n            912 LOAD_ATTR               16 (block)\n            914 LOAD_CONST               7 (1)\n            916 BINARY_SUBSCR\n            918 LOAD_ATTR               17 (layer)\n            920 LOAD_CONST               7 (1)\n            922 BINARY_SUBSCR\n            924 LOAD_ATTR               20 (EncDecAttention)\n            926 EXTENDED_ARG             4\n            928 LOAD_FAST             1188 (tmp_55)\n            930 LOAD_CONST              10 (4)\n            932 EXTENDED_ARG             4\n            934 LOAD_FAST             1189 (tmp_56)\n            936 LOAD_FAST                2 (mod)\n            938 LOAD_ATTR               19 (decoder)\n            940 LOAD_ATTR               16 (block)\n            942 LOAD_CONST               8 (2)\n            944 BINARY_SUBSCR\n            946 LOAD_ATTR               17 (layer)\n            948 LOAD_CONST               5 (0)\n            950 BINARY_SUBSCR\n            952 LOAD_ATTR               18 (SelfAttention)\n            954 EXTENDED_ARG             4\n            956 LOAD_FAST             1190 (tmp_57)\n            958 LOAD_CONST              10 (4)\n            960 EXTENDED_ARG             4\n            962 LOAD_FAST             1191 (tmp_58)\n            964 LOAD_FAST                2 (mod)\n            966 LOAD_ATTR               19 (decoder)\n            968 LOAD_ATTR               16 (block)\n            970 LOAD_CONST               8 (2)\n            972 BINARY_SUBSCR\n            974 LOAD_ATTR               17 (layer)\n            976 LOAD_CONST               7 (1)\n            978 BINARY_SUBSCR\n            980 LOAD_ATTR               20 (EncDecAttention)\n            982 EXTENDED_ARG             4\n            984 LOAD_FAST             1192 (tmp_59)\n            986 LOAD_CONST              10 (4)\n            988 EXTENDED_ARG             4\n            990 LOAD_FAST             1193 (tmp_60)\n            992 LOAD_FAST                2 (mod)\n            994 LOAD_ATTR               19 (decoder)\n            996 LOAD_ATTR               16 (block)\n            998 LOAD_CONST               9 (3)\n           1000 BINARY_SUBSCR\n           1002 LOAD_ATTR               17 (layer)\n           1004 LOAD_CONST               5 (0)\n           1006 BINARY_SUBSCR\n           1008 LOAD_ATTR               18 (SelfAttention)\n           1010 EXTENDED_ARG             4\n           1012 LOAD_FAST             1194 (tmp_61)\n           1014 LOAD_CONST              10 (4)\n           1016 EXTENDED_ARG             4\n           1018 LOAD_FAST             1195 (tmp_62)\n           1020 LOAD_FAST                2 (mod)\n           1022 LOAD_ATTR               19 (decoder)\n           1024 LOAD_ATTR               16 (block)\n           1026 LOAD_CONST               9 (3)\n           1028 BINARY_SUBSCR\n           1030 LOAD_ATTR               17 (layer)\n           1032 LOAD_CONST               7 (1)\n           1034 BINARY_SUBSCR\n           1036 LOAD_ATTR               20 (EncDecAttention)\n           1038 EXTENDED_ARG             4\n           1040 LOAD_FAST             1196 (tmp_63)\n           1042 LOAD_CONST              10 (4)\n           1044 EXTENDED_ARG             4\n           1046 LOAD_FAST             1197 (tmp_64)\n           1048 LOAD_FAST                2 (mod)\n           1050 LOAD_ATTR               19 (decoder)\n           1052 LOAD_ATTR               16 (block)\n           1054 LOAD_CONST              10 (4)\n           1056 BINARY_SUBSCR\n           1058 LOAD_ATTR               17 (layer)\n           1060 LOAD_CONST               5 (0)\n           1062 BINARY_SUBSCR\n           1064 LOAD_ATTR               18 (SelfAttention)\n           1066 EXTENDED_ARG             4\n           1068 LOAD_FAST             1198 (tmp_65)\n           1070 LOAD_CONST              10 (4)\n           1072 EXTENDED_ARG             4\n           1074 LOAD_FAST             1199 (tmp_66)\n           1076 LOAD_FAST                2 (mod)\n           1078 LOAD_ATTR               19 (decoder)\n           1080 LOAD_ATTR               16 (block)\n           1082 LOAD_CONST              10 (4)\n           1084 BINARY_SUBSCR\n           1086 LOAD_ATTR               17 (layer)\n           1088 LOAD_CONST               7 (1)\n           1090 BINARY_SUBSCR\n           1092 LOAD_ATTR               20 (EncDecAttention)\n           1094 EXTENDED_ARG             4\n           1096 LOAD_FAST             1200 (tmp_67)\n           1098 LOAD_CONST              10 (4)\n           1100 EXTENDED_ARG             4\n           1102 LOAD_FAST             1201 (tmp_68)\n           1104 LOAD_FAST                2 (mod)\n           1106 LOAD_ATTR               19 (decoder)\n           1108 LOAD_ATTR               16 (block)\n           1110 LOAD_CONST              11 (5)\n           1112 BINARY_SUBSCR\n           1114 LOAD_ATTR               17 (layer)\n           1116 LOAD_CONST               5 (0)\n           1118 BINARY_SUBSCR\n           1120 LOAD_ATTR               18 (SelfAttention)\n           1122 EXTENDED_ARG             4\n           1124 LOAD_FAST             1202 (tmp_69)\n           1126 LOAD_CONST              10 (4)\n           1128 EXTENDED_ARG             4\n           1130 LOAD_FAST             1203 (tmp_70)\n           1132 LOAD_FAST                2 (mod)\n           1134 LOAD_ATTR               19 (decoder)\n           1136 LOAD_ATTR               16 (block)\n           1138 LOAD_CONST              11 (5)\n           1140 BINARY_SUBSCR\n           1142 LOAD_ATTR               17 (layer)\n           1144 LOAD_CONST               7 (1)\n           1146 BINARY_SUBSCR\n           1148 LOAD_ATTR               20 (EncDecAttention)\n           1150 EXTENDED_ARG             4\n           1152 LOAD_FAST             1204 (tmp_71)\n           1154 STORE_ATTR              14 (cell_contents)\n           1156 STORE_ATTR              14 (cell_contents)\n           1158 STORE_ATTR              14 (cell_contents)\n           1160 STORE_ATTR              14 (cell_contents)\n           1162 STORE_ATTR              14 (cell_contents)\n           1164 STORE_ATTR              14 (cell_contents)\n           1166 STORE_ATTR              14 (cell_contents)\n           1168 STORE_ATTR              14 (cell_contents)\n           1170 STORE_ATTR              14 (cell_contents)\n           1172 STORE_ATTR              14 (cell_contents)\n           1174 STORE_ATTR              14 (cell_contents)\n           1176 STORE_ATTR              14 (cell_contents)\n           1178 STORE_ATTR              14 (cell_contents)\n           1180 STORE_ATTR              14 (cell_contents)\n           1182 STORE_ATTR              14 (cell_contents)\n           1184 STORE_ATTR              14 (cell_contents)\n           1186 STORE_ATTR              14 (cell_contents)\n           1188 STORE_ATTR              14 (cell_contents)\n           1190 STORE_ATTR              14 (cell_contents)\n           1192 STORE_ATTR              14 (cell_contents)\n           1194 STORE_ATTR              14 (cell_contents)\n           1196 STORE_ATTR              14 (cell_contents)\n           1198 STORE_ATTR              14 (cell_contents)\n           1200 STORE_ATTR              14 (cell_contents)\n           1202 STORE_ATTR              14 (cell_contents)\n           1204 STORE_ATTR              14 (cell_contents)\n           1206 STORE_ATTR              14 (cell_contents)\n           1208 STORE_ATTR              14 (cell_contents)\n           1210 STORE_ATTR              14 (cell_contents)\n           1212 STORE_ATTR              14 (cell_contents)\n           1214 STORE_ATTR              14 (cell_contents)\n           1216 STORE_ATTR              14 (cell_contents)\n           1218 STORE_ATTR              14 (cell_contents)\n           1220 STORE_ATTR              14 (cell_contents)\n           1222 STORE_ATTR              14 (cell_contents)\n           1224 STORE_ATTR              14 (cell_contents)\n           1226 STORE_FAST               7 (loss)\n           1228 STORE_FAST               6 (pred)\n\n559        1230 CALL_FUNCTION            0\n           1232 LOAD_GLOBAL             22 (__resume_at_100_4)\n           1234 ROT_TWO\n           1236 LOAD_FAST                1 (self)\n           1238 LOAD_FAST                2 (mod)\n           1240 LOAD_FAST                3 (collect_outputs)\n           1242 LOAD_FAST                4 (cloned_inputs)\n           1244 LOAD_FAST                6 (pred)\n           1246 LOAD_FAST                7 (loss)\n           1248 CALL_FUNCTION            7\n           1250 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             12 (__import_contextlib)\n              2 LOAD_ATTR               13 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             12 (__import_contextlib)\n             28 LOAD_ATTR               13 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              48 (___context_manager_0_3)\n             34 LOAD_FAST               48 (___context_manager_0_3)\n             36 LOAD_METHOD             14 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               48 (___context_manager_0_3)\n             50 LOAD_METHOD             15 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               48 (___context_manager_0_3)\n             68 LOAD_METHOD             15 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             16 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 823 \n 823           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                4 (model)\n               4 LOAD_ATTR                5 (decoder)\n               6 LOAD_FAST                1 (input_ids)\n               8 LOAD_FAST                2 (attention_mask)\n              10 LOAD_FAST                3 (encoder_hidden_states)\n              12 LOAD_FAST                4 (encoder_attention_mask)\n              14 LOAD_FAST                5 (head_mask)\n              16 LOAD_FAST                6 (cross_attn_head_mask)\n              18 LOAD_FAST                7 (past_key_values)\n              20 LOAD_FAST                8 (inputs_embeds)\n              22 LOAD_FAST               10 (use_cache)\n              24 LOAD_FAST                0 (self)\n              26 LOAD_ATTR                0 (config)\n              28 LOAD_ATTR                1 (output_attentions)\n              30 LOAD_FAST                0 (self)\n              32 LOAD_ATTR                0 (config)\n              34 LOAD_ATTR                2 (output_hidden_states)\n              36 LOAD_FAST                0 (self)\n              38 LOAD_ATTR                0 (config)\n              40 LOAD_ATTR                3 (use_return_dict)\n              42 LOAD_CONST               2 (('input_ids', 'attention_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              44 LOAD_FAST                0 (self)\n              46 LOAD_ATTR                0 (config)\n              48 LOAD_ATTR                3 (use_return_dict)\n              50 STORE_FAST              13 (return_dict)\n\n 950          52 CALL_FUNCTION_KW        12\n              54 LOAD_GLOBAL             17 (__resume_at_94_5)\n              56 ROT_TWO\n              58 LOAD_FAST                0 (self)\n              60 LOAD_FAST                9 (labels)\n              62 LOAD_FAST               13 (return_dict)\n              64 CALL_FUNCTION            4\n              66 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 90 \n 90           0 LOAD_GLOBAL             11 (__compiled_fn_6)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 522 \n522           0 LOAD_GLOBAL              4 (__compiled_fn_7)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 347 \n347           0 LOAD_GLOBAL             14 (__compiled_fn_8)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 347 \n347           0 LOAD_GLOBAL             14 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 347 \n347           0 LOAD_GLOBAL             14 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 347 \n347           0 LOAD_GLOBAL             14 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 347 \n347           0 LOAD_GLOBAL             14 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 347 \n347           0 LOAD_GLOBAL             14 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 347 \n347           0 LOAD_GLOBAL             14 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 347 \n347           0 LOAD_GLOBAL             14 (__compiled_fn_15)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 347 \n347           0 LOAD_GLOBAL             14 (__compiled_fn_16)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 347 \n347           0 LOAD_GLOBAL             14 (__compiled_fn_17)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 347 \n347           0 LOAD_GLOBAL             14 (__compiled_fn_18)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 347 \n347           0 LOAD_GLOBAL             14 (__compiled_fn_19)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              63 (graph_out_0)\n             10 LOAD_FAST               63 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               63 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               63 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_20)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              6 (__resume_at_14_21)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f2477a2eb80, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_22')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f2477a28be0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_23')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f2477a2eb80, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_24')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f2477a28be0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_25')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py line 963 \n963           0 LOAD_GLOBAL             15 (__compiled_fn_26)\n              2 LOAD_FAST                0 (___stack0)\n              4 LOAD_ATTR               16 (last_hidden_state)\n              6 LOAD_FAST                2 (labels)\n              8 CALL_FUNCTION            2\n             10 STORE_FAST              25 (graph_out_0)\n             12 LOAD_CONST               7 (<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>)\n             14 LOAD_FAST               25 (graph_out_0)\n             16 LOAD_CONST               3 (0)\n             18 BINARY_SUBSCR\n             20 LOAD_FAST               25 (graph_out_0)\n             22 LOAD_CONST               5 (1)\n             24 BINARY_SUBSCR\n             26 LOAD_FAST                0 (___stack0)\n             28 LOAD_ATTR               11 (past_key_values)\n             30 LOAD_FAST                0 (___stack0)\n             32 LOAD_ATTR               12 (hidden_states)\n             34 LOAD_FAST                0 (___stack0)\n             36 LOAD_ATTR               13 (attentions)\n             38 LOAD_FAST                0 (___stack0)\n             40 LOAD_ATTR               14 (cross_attentions)\n             42 LOAD_CONST               8 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n             44 CALL_FUNCTION_KW         6\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_27)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_28)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_29)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             11 (__import_contextlib)\n              2 LOAD_ATTR               12 (nullcontext)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_CONST               1 (())\n              8 LOAD_CONST               2 ('input_ids')\n             10 LOAD_FAST                4 (cloned_inputs)\n             12 LOAD_CONST               2 ('input_ids')\n             14 BINARY_SUBSCR\n             16 LOAD_CONST               3 ('labels')\n             18 LOAD_FAST                4 (cloned_inputs)\n             20 LOAD_CONST               3 ('labels')\n             22 BINARY_SUBSCR\n             24 BUILD_MAP                2\n             26 LOAD_GLOBAL             11 (__import_contextlib)\n             28 LOAD_ATTR               12 (nullcontext)\n             30 CALL_FUNCTION            0\n             32 STORE_FAST              48 (___context_manager_0_3)\n             34 LOAD_FAST               48 (___context_manager_0_3)\n             36 LOAD_METHOD             13 (__enter__)\n             38 CALL_METHOD              0\n             40 POP_TOP\n             42 SETUP_FINALLY           10 (to 64)\n\n557          44 CALL_FUNCTION_EX         1\n             46 POP_BLOCK\n             48 LOAD_FAST               48 (___context_manager_0_3)\n             50 LOAD_METHOD             14 (__exit__)\n             52 LOAD_CONST               0 (None)\n             54 DUP_TOP\n             56 DUP_TOP\n             58 CALL_METHOD              3\n             60 POP_TOP\n             62 JUMP_FORWARD             9 (to 82)\n        >>   64 NOP\n             66 LOAD_FAST               48 (___context_manager_0_3)\n             68 LOAD_METHOD             14 (__exit__)\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_METHOD              3\n             78 POP_TOP\n             80 RERAISE                  0\n        >>   82 NOP\n             84 LOAD_GLOBAL             15 (__resume_at_44_4)\n             86 ROT_THREE\n             88 LOAD_FAST                1 (self)\n             90 LOAD_FAST                2 (mod)\n             92 LOAD_FAST                3 (collect_outputs)\n             94 LOAD_FAST                4 (cloned_inputs)\n             96 CALL_FUNCTION            6\n             98 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 774 \n774           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                4 (model)\n              4 LOAD_FAST                1 (input_ids)\n              6 LOAD_FAST                2 (attention_mask)\n              8 LOAD_FAST                3 (position_ids)\n             10 LOAD_FAST                4 (encoder_hidden_states)\n             12 LOAD_FAST                5 (encoder_attention_mask)\n             14 LOAD_FAST                6 (head_mask)\n             16 LOAD_FAST                7 (cross_attn_head_mask)\n             18 LOAD_FAST                8 (past_key_values)\n             20 LOAD_FAST                9 (inputs_embeds)\n             22 LOAD_FAST               11 (use_cache)\n             24 LOAD_FAST                0 (self)\n             26 LOAD_ATTR                0 (config)\n             28 LOAD_ATTR                1 (output_attentions)\n             30 LOAD_FAST                0 (self)\n             32 LOAD_ATTR                0 (config)\n             34 LOAD_ATTR                2 (output_hidden_states)\n             36 LOAD_FAST                0 (self)\n             38 LOAD_ATTR                0 (config)\n             40 LOAD_ATTR                3 (use_return_dict)\n             42 LOAD_CONST               2 (('input_ids', 'attention_mask', 'position_ids', 'encoder_hidden_states', 'encoder_attention_mask', 'head_mask', 'cross_attn_head_mask', 'past_key_values', 'inputs_embeds', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n             44 LOAD_FAST                0 (self)\n             46 LOAD_ATTR                0 (config)\n             48 LOAD_ATTR                3 (use_return_dict)\n             50 STORE_FAST              14 (return_dict)\n\n811          52 CALL_FUNCTION_KW        13\n             54 LOAD_GLOBAL             19 (__resume_at_94_5)\n             56 ROT_TWO\n             58 LOAD_FAST                0 (self)\n             60 LOAD_FAST               10 (labels)\n             62 LOAD_FAST               14 (return_dict)\n             64 CALL_FUNCTION            4\n             66 RETURN_VALUE\n\n", "MODIFIED BYTECODE _prepare_decoder_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 555 \n555           0 LOAD_GLOBAL              4 (__compiled_fn_6)\n              2 CALL_FUNCTION            0\n              4 UNPACK_SEQUENCE          1\n              6 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 202 \n202           0 LOAD_GLOBAL             10 (__compiled_fn_7)\n              2 LOAD_FAST                1 (position_ids)\n              4 CALL_FUNCTION            1\n              6 UNPACK_SEQUENCE          1\n              8 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_8)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_9)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_10)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_11)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_12)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_13)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_14)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_15)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_16)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_17)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_18)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_19)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_20)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_21)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_22)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_23)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_24)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_25)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_26)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_27)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_28)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_29)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_30)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 399 \n399           0 LOAD_GLOBAL             14 (__compiled_fn_31)\n              2 LOAD_FAST                1 (hidden_states)\n              4 LOAD_FAST                2 (attention_mask)\n              6 CALL_FUNCTION            2\n              8 STORE_FAST              64 (graph_out_0)\n             10 LOAD_FAST               64 (graph_out_0)\n             12 LOAD_CONST               7 (0)\n             14 BINARY_SUBSCR\n             16 LOAD_FAST               64 (graph_out_0)\n             18 LOAD_CONST               8 (1)\n             20 BINARY_SUBSCR\n             22 LOAD_FAST               64 (graph_out_0)\n             24 LOAD_CONST               2 (2)\n             26 BINARY_SUBSCR\n             28 BUILD_TUPLE              2\n             30 BUILD_TUPLE              2\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE __init__ <string> line 2 \n  2           0 LOAD_FAST                1 (last_hidden_state)\n              2 LOAD_FAST                0 (self)\n\n  3           4 STORE_ATTR               0 (last_hidden_state)\n              6 LOAD_GLOBAL              6 (__resume_at_6_32)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (past_key_values)\n             12 LOAD_FAST                3 (hidden_states)\n             14 LOAD_FAST                4 (attentions)\n             16 LOAD_FAST                5 (cross_attentions)\n             18 CALL_FUNCTION            5\n             20 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __init__> <string> line 3 \n  3           0 LOAD_FAST                1 (past_key_values)\n              2 LOAD_FAST                0 (self)\n\n  4           4 STORE_ATTR               1 (past_key_values)\n              6 LOAD_GLOBAL              6 (__resume_at_14_33)\n              8 LOAD_FAST                0 (self)\n             10 LOAD_FAST                2 (hidden_states)\n             12 LOAD_FAST                3 (attentions)\n             14 LOAD_FAST                4 (cross_attentions)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                0 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n             18 DUP_TOP\n             20 STORE_FAST               2 (value)\n\n330          22 CALL_FUNCTION            2\n             24 LOAD_CLOSURE             0 (__class__)\n             26 BUILD_TUPLE              1\n             28 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f9964da1fd0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             30 LOAD_CONST               2 ('__resume_at_12_34')\n             32 MAKE_FUNCTION            8 (closure)\n             34 ROT_TWO\n             36 LOAD_FAST                0 (self)\n             38 LOAD_FAST                1 (key)\n             40 LOAD_FAST                2 (value)\n             42 CALL_FUNCTION            4\n             44 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                1 (self)\n             16 LOAD_ATTR                4 (last_hidden_state)\n\n332          18 CALL_FUNCTION            2\n             20 LOAD_CLOSURE             0 (__class__)\n             22 BUILD_TUPLE              1\n             24 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f996b89d420, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             26 LOAD_CONST               2 ('__resume_at_38_35')\n             28 MAKE_FUNCTION            8 (closure)\n             30 ROT_TWO\n             32 CALL_FUNCTION            1\n             34 RETURN_VALUE\n\n", "MODIFIED BYTECODE __setitem__ /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 328 \n328           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                0 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                1 (__setitem__)\n             12 LOAD_FAST                1 (key)\n             14 LOAD_FAST                2 (value)\n\n330          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f9964da1fd0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 330>)\n             24 LOAD_CONST               2 ('__resume_at_12_36')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 LOAD_FAST                0 (self)\n             32 LOAD_FAST                1 (key)\n             34 LOAD_FAST                2 (value)\n             36 CALL_FUNCTION            4\n             38 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in __setitem__> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py line 330 \n330           0 LOAD_GLOBAL              3 (__import_builtins)\n              2 LOAD_ATTR                0 (super)\n              4 LOAD_DEREF               0 (__class__)\n              6 LOAD_FAST                1 (self)\n              8 CALL_FUNCTION            2\n             10 LOAD_ATTR                2 (__setattr__)\n             12 LOAD_FAST                2 (key)\n             14 LOAD_FAST                3 (value)\n\n332          16 CALL_FUNCTION            2\n             18 LOAD_CLOSURE             0 (__class__)\n             20 BUILD_TUPLE              1\n             22 LOAD_CONST               1 (<code object <resume in __setitem__> at 0x7f996b89d420, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/utils/generic.py\", line 332>)\n             24 LOAD_CONST               2 ('__resume_at_38_37')\n             26 MAKE_FUNCTION            8 (closure)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py line 811 \n811           0 LOAD_GLOBAL             18 (__compiled_fn_38)\n              2 LOAD_FAST                0 (___stack0)\n              4 LOAD_ATTR               19 (last_hidden_state)\n              6 LOAD_FAST                2 (labels)\n              8 CALL_FUNCTION            2\n             10 STORE_FAST              28 (graph_out_0)\n             12 LOAD_CONST               7 (<class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>)\n             14 LOAD_FAST               28 (graph_out_0)\n             16 LOAD_CONST               3 (0)\n             18 BINARY_SUBSCR\n             20 LOAD_FAST               28 (graph_out_0)\n             22 LOAD_CONST               4 (1)\n             24 BINARY_SUBSCR\n             26 LOAD_FAST                0 (___stack0)\n             28 LOAD_ATTR               14 (past_key_values)\n             30 LOAD_FAST                0 (___stack0)\n             32 LOAD_ATTR               15 (hidden_states)\n             34 LOAD_FAST                0 (___stack0)\n             36 LOAD_ATTR               16 (attentions)\n             38 LOAD_FAST                0 (___stack0)\n             40 LOAD_ATTR               17 (cross_attentions)\n             42 LOAD_CONST               8 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n             44 CALL_FUNCTION_KW         6\n             46 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 557 \n557           0 LOAD_FAST                1 (___stack1)\n              2 LOAD_ATTR                9 (loss)\n              4 LOAD_ATTR                6 (backward)\n              6 LOAD_FAST                1 (___stack1)\n              8 LOAD_FAST                1 (___stack1)\n             10 LOAD_ATTR                9 (loss)\n             12 STORE_FAST               8 (loss)\n             14 STORE_FAST               7 (pred)\n\n559          16 CALL_FUNCTION            0\n             18 LOAD_GLOBAL             10 (__resume_at_144_39)\n             20 ROT_TWO\n             22 LOAD_FAST                2 (self)\n             24 LOAD_FAST                3 (mod)\n             26 LOAD_FAST                4 (collect_outputs)\n             28 LOAD_FAST                5 (cloned_inputs)\n             30 LOAD_FAST                7 (pred)\n             32 LOAD_FAST                8 (loss)\n             34 CALL_FUNCTION            7\n             36 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_40)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_41)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             13 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             4\n             18 STORE_FAST            1241 (graph_out_0)\n             20 EXTENDED_ARG             4\n             22 LOAD_FAST             1241 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.models.xlnet.modeling_xlnet.XLNetLMHeadModelOutput'>)\n             32 EXTENDED_ARG             4\n             34 LOAD_FAST             1241 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             4\n             42 LOAD_FAST             1241 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 LOAD_CONST               0 (None)\n             50 LOAD_CONST               0 (None)\n             52 LOAD_CONST               0 (None)\n             54 LOAD_CONST               7 (('loss', 'logits', 'mems', 'hidden_states', 'attentions'))\n             56 CALL_FUNCTION_KW         5\n             58 EXTENDED_ARG             4\n             60 LOAD_FAST             1241 (graph_out_0)\n             62 LOAD_CONST               4 (0)\n             64 BINARY_SUBSCR\n             66 STORE_FAST               7 (loss)\n             68 STORE_FAST               6 (pred)\n\n559          70 CALL_FUNCTION            0\n             72 LOAD_GLOBAL             14 (__resume_at_100_4)\n             74 ROT_TWO\n             76 LOAD_FAST                1 (self)\n             78 LOAD_FAST                2 (mod)\n             80 LOAD_FAST                3 (collect_outputs)\n             82 LOAD_FAST                4 (cloned_inputs)\n             84 LOAD_FAST                6 (pred)\n             86 LOAD_FAST                7 (loss)\n             88 CALL_FUNCTION            7\n             90 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n", "MODIFIED BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 553 \n553           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n\n554           4 CALL_FUNCTION            1\n              6 LOAD_GLOBAL              9 (__resume_at_6_0)\n              8 ROT_TWO\n             10 LOAD_FAST                0 (self)\n             12 LOAD_FAST                1 (mod)\n             14 LOAD_FAST                3 (collect_outputs)\n             16 CALL_FUNCTION            4\n             18 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 554 \n554           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                1 (optimizer_zero_grad)\n              4 LOAD_FAST                2 (mod)\n              6 LOAD_FAST                0 (___stack0)\n              8 STORE_FAST               5 (cloned_inputs)\n\n555          10 CALL_FUNCTION            1\n             12 LOAD_GLOBAL              9 (__resume_at_20_1)\n             14 ROT_TWO\n             16 LOAD_FAST                1 (self)\n             18 LOAD_FAST                2 (mod)\n             20 LOAD_FAST                3 (collect_outputs)\n             22 LOAD_FAST                5 (cloned_inputs)\n             24 CALL_FUNCTION            5\n             26 RETURN_VALUE\n\n", "MODIFIED BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1875           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_ATTR                1 (zero_grad)\n               6 LOAD_CONST               1 (True)\n\n1877           8 CALL_FUNCTION            1\n              10 LOAD_GLOBAL              2 (__resume_at_20_2)\n              12 ROT_TWO\n              14 CALL_FUNCTION            1\n              16 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 555 \n555           0 LOAD_GLOBAL             14 (__compiled_fn_3)\n              2 LOAD_FAST                4 (cloned_inputs)\n              4 LOAD_CONST               2 ('input_ids')\n              6 BINARY_SUBSCR\n              8 LOAD_FAST                4 (cloned_inputs)\n             10 LOAD_CONST               3 ('labels')\n             12 BINARY_SUBSCR\n             14 CALL_FUNCTION            2\n             16 EXTENDED_ARG             3\n             18 STORE_FAST             971 (graph_out_0)\n             20 EXTENDED_ARG             3\n             22 LOAD_FAST              971 (graph_out_0)\n             24 LOAD_CONST               4 (0)\n             26 BINARY_SUBSCR\n             28 LOAD_ATTR                6 (backward)\n             30 LOAD_CONST               5 (<class 'transformers.modeling_outputs.MaskedLMOutput'>)\n             32 EXTENDED_ARG             3\n             34 LOAD_FAST              971 (graph_out_0)\n             36 LOAD_CONST               4 (0)\n             38 BINARY_SUBSCR\n             40 EXTENDED_ARG             3\n             42 LOAD_FAST              971 (graph_out_0)\n             44 LOAD_CONST               6 (1)\n             46 BINARY_SUBSCR\n             48 LOAD_CONST               0 (None)\n             50 LOAD_CONST               0 (None)\n             52 LOAD_CONST               7 (('loss', 'logits', 'hidden_states', 'attentions'))\n             54 CALL_FUNCTION_KW         4\n             56 EXTENDED_ARG             3\n             58 LOAD_FAST              971 (graph_out_0)\n             60 LOAD_CONST               4 (0)\n             62 BINARY_SUBSCR\n             64 STORE_FAST               7 (loss)\n             66 STORE_FAST               6 (pred)\n\n559          68 CALL_FUNCTION            0\n             70 LOAD_GLOBAL             15 (__resume_at_100_4)\n             72 ROT_TWO\n             74 LOAD_FAST                1 (self)\n             76 LOAD_FAST                2 (mod)\n             78 LOAD_FAST                3 (collect_outputs)\n             80 LOAD_FAST                4 (cloned_inputs)\n             82 LOAD_FAST                6 (pred)\n             84 LOAD_FAST                7 (loss)\n             86 CALL_FUNCTION            7\n             88 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 559 \n559           0 LOAD_FAST                1 (self)\n              2 LOAD_ATTR                7 (optimizer_step)\n              4 LOAD_FAST                5 (pred)\n              6 LOAD_ATTR               10 (loss)\n              8 STORE_FAST               6 (loss)\n\n560          10 CALL_FUNCTION            0\n             12 LOAD_GLOBAL             11 (__resume_at_108_5)\n             14 ROT_TWO\n             16 LOAD_FAST                2 (mod)\n             18 LOAD_FAST                3 (collect_outputs)\n             20 LOAD_FAST                4 (cloned_inputs)\n             22 LOAD_FAST                5 (pred)\n             24 LOAD_FAST                6 (loss)\n             26 CALL_FUNCTION            6\n             28 RETURN_VALUE\n\n", "MODIFIED BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n185           0 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n              2 LOAD_FAST                0 (params)\n              4 LOAD_FAST                1 (d_p_list)\n              6 LOAD_FAST                2 (momentum_buffer_list)\n              8 LOAD_FAST                5 (weight_decay)\n             10 LOAD_FAST                6 (momentum)\n             12 LOAD_FAST                7 (lr)\n             14 LOAD_FAST                8 (dampening)\n             16 LOAD_FAST                9 (nesterov)\n             18 LOAD_FAST                3 (has_sparse_grad)\n             20 LOAD_FAST               10 (maximize)\n             22 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n\n220          24 CALL_FUNCTION_KW        10\n             26 LOAD_GLOBAL              7 (__resume_at_114_6)\n             28 ROT_TWO\n             30 CALL_FUNCTION            1\n             32 RETURN_VALUE\n\n", "MODIFIED BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 560 \n560           0 LOAD_GLOBAL              8 (collect_results)\n              2 LOAD_FAST                1 (mod)\n              4 LOAD_FAST                4 (pred)\n              6 LOAD_FAST                4 (pred)\n              8 LOAD_ATTR                9 (loss)\n             10 LOAD_FAST                3 (cloned_inputs)\n\n562          12 CALL_FUNCTION            4\n             14 RETURN_VALUE\n\n"]