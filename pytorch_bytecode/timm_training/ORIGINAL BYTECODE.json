["ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 LOAD_CONST               2 ('cuda')\n              4 LOAD_CONST               3 (torch.float16)\n              6 LOAD_CONST               4 (True)\n              8 LOAD_CONST               4 (True)\n             10 CALL_FUNCTION            4\n             12 SETUP_WITH              10 (to 34)\n             14 POP_TOP\n             16 LOAD_FAST                1 (___stack1)\n             18 JUMP_ABSOLUTE           59 (to 118)\n             20 POP_BLOCK\n             22 LOAD_CONST               0 (None)\n             24 DUP_TOP\n             26 DUP_TOP\n             28 CALL_FUNCTION            3\n             30 POP_TOP\n             32 JUMP_FORWARD             8 (to 50)\n        >>   34 WITH_EXCEPT_START\n             36 POP_JUMP_IF_TRUE        20 (to 40)\n             38 RERAISE                  0\n        >>   40 POP_TOP\n             42 POP_TOP\n             44 POP_TOP\n             46 POP_EXCEPT\n             48 POP_TOP\n        >>   50 NOP\n             52 LOAD_CONST               0 (None)\n             54 RAISE_VARARGS            1\n             56 LOAD_GLOBAL              0 (clone_inputs)\n             58 LOAD_FAST                7 (inputs)\n             60 CALL_FUNCTION            1\n             62 STORE_FAST               5 (cloned_inputs)\n             64 LOAD_FAST                2 (self)\n             66 LOAD_ATTR                1 (optimizer_zero_grad)\n             68 LOAD_FAST                3 (mod)\n             70 CALL_FUNCTION            1\n             72 POP_TOP\n             74 LOAD_FAST                2 (self)\n             76 LOAD_ATTR                2 (autocast)\n             78 CALL_FUNCTION            0\n             80 SETUP_WITH              26 (to 134)\n             82 POP_TOP\n             84 LOAD_FAST                3 (mod)\n             86 LOAD_FAST                5 (cloned_inputs)\n             88 CALL_FUNCTION_EX         0\n             90 STORE_FAST               6 (pred)\n             92 LOAD_GLOBAL              3 (isinstance)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_GLOBAL              4 (tuple)\n             98 CALL_FUNCTION            2\n            100 POP_JUMP_IF_FALSE       55 (to 110)\n            102 LOAD_FAST                6 (pred)\n            104 LOAD_CONST               1 (0)\n            106 BINARY_SUBSCR\n            108 STORE_FAST               6 (pred)\n        >>  110 LOAD_FAST                2 (self)\n            112 LOAD_ATTR                5 (compute_loss)\n            114 LOAD_FAST                6 (pred)\n            116 CALL_FUNCTION            1\n        >>  118 STORE_FAST               8 (loss)\n            120 POP_BLOCK\n\n328         122 LOAD_CONST               0 (None)\n            124 DUP_TOP\n            126 DUP_TOP\n            128 CALL_FUNCTION            3\n            130 POP_TOP\n            132 JUMP_FORWARD             8 (to 150)\n        >>  134 WITH_EXCEPT_START\n            136 POP_JUMP_IF_TRUE        70 (to 140)\n            138 RERAISE                  1\n        >>  140 POP_TOP\n            142 POP_TOP\n            144 POP_TOP\n            146 POP_EXCEPT\n            148 POP_TOP\n\n333     >>  150 LOAD_FAST                2 (self)\n            152 LOAD_ATTR                6 (grad_scaler)\n            154 LOAD_ATTR                7 (scale)\n            156 LOAD_FAST                8 (loss)\n            158 CALL_FUNCTION            1\n            160 LOAD_ATTR                8 (backward)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n334         166 LOAD_FAST                2 (self)\n            168 LOAD_ATTR                9 (optimizer_step)\n            170 CALL_FUNCTION            0\n            172 POP_TOP\n\n335         174 LOAD_FAST                4 (collect_outputs)\n            176 POP_JUMP_IF_FALSE       96 (to 192)\n\n336         178 LOAD_GLOBAL             10 (collect_results)\n            180 LOAD_FAST                3 (mod)\n            182 LOAD_FAST                6 (pred)\n            184 LOAD_FAST                8 (loss)\n            186 LOAD_FAST                5 (cloned_inputs)\n            188 CALL_FUNCTION            4\n            190 RETURN_VALUE\n\n337     >>  192 LOAD_CONST               0 (None)\n            194 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE scaled_compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 317 \n319           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 LOAD_CONST               1 (1000.0)\n              8 BINARY_TRUE_DIVIDE\n             10 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in scaled_compute_loss> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 319 \n319           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              6 LOAD_FAST                2 (pred)\n              8 CALL_FUNCTION            1\n        >>   10 LOAD_CONST               1 (1000.0)\n             12 BINARY_TRUE_DIVIDE\n             14 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE scaled_compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 317 \n319           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 LOAD_CONST               1 (1000.0)\n              8 BINARY_TRUE_DIVIDE\n             10 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in scaled_compute_loss> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 319 \n319           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              6 LOAD_FAST                2 (pred)\n              8 CALL_FUNCTION            1\n        >>   10 LOAD_CONST               1 (1000.0)\n             12 BINARY_TRUE_DIVIDE\n             14 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE scaled_compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 317 \n319           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 LOAD_CONST               1 (1000.0)\n              8 BINARY_TRUE_DIVIDE\n             10 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in scaled_compute_loss> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 319 \n319           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              6 LOAD_FAST                2 (pred)\n              8 CALL_FUNCTION            1\n        >>   10 LOAD_CONST               1 (1000.0)\n             12 BINARY_TRUE_DIVIDE\n             14 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE scaled_compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 317 \n319           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 LOAD_CONST               1 (1000.0)\n              8 BINARY_TRUE_DIVIDE\n             10 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in scaled_compute_loss> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 319 \n319           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              6 LOAD_FAST                2 (pred)\n              8 CALL_FUNCTION            1\n        >>   10 LOAD_CONST               1 (1000.0)\n             12 BINARY_TRUE_DIVIDE\n             14 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE scaled_compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 317 \n319           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 LOAD_CONST               1 (1000.0)\n              8 BINARY_TRUE_DIVIDE\n             10 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in scaled_compute_loss> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 319 \n319           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              6 LOAD_FAST                2 (pred)\n              8 CALL_FUNCTION            1\n        >>   10 LOAD_CONST               1 (1000.0)\n             12 BINARY_TRUE_DIVIDE\n             14 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 CALL_FUNCTION            0\n              4 SETUP_WITH              10 (to 26)\n              6 POP_TOP\n              8 LOAD_FAST                1 (___stack1)\n             10 JUMP_ABSOLUTE           55 (to 110)\n             12 POP_BLOCK\n             14 LOAD_CONST               0 (None)\n             16 DUP_TOP\n             18 DUP_TOP\n             20 CALL_FUNCTION            3\n             22 POP_TOP\n             24 JUMP_FORWARD             8 (to 42)\n        >>   26 WITH_EXCEPT_START\n             28 POP_JUMP_IF_TRUE        16 (to 32)\n             30 RERAISE                  0\n        >>   32 POP_TOP\n             34 POP_TOP\n             36 POP_TOP\n             38 POP_EXCEPT\n             40 POP_TOP\n        >>   42 NOP\n             44 LOAD_CONST               0 (None)\n             46 RAISE_VARARGS            1\n             48 LOAD_GLOBAL              0 (clone_inputs)\n             50 LOAD_FAST                7 (inputs)\n             52 CALL_FUNCTION            1\n             54 STORE_FAST               5 (cloned_inputs)\n             56 LOAD_FAST                2 (self)\n             58 LOAD_ATTR                1 (optimizer_zero_grad)\n             60 LOAD_FAST                3 (mod)\n             62 CALL_FUNCTION            1\n             64 POP_TOP\n             66 LOAD_FAST                2 (self)\n             68 LOAD_ATTR                2 (autocast)\n             70 CALL_FUNCTION            0\n             72 SETUP_WITH              26 (to 126)\n             74 POP_TOP\n             76 LOAD_FAST                3 (mod)\n             78 LOAD_FAST                5 (cloned_inputs)\n             80 CALL_FUNCTION_EX         0\n             82 STORE_FAST               6 (pred)\n             84 LOAD_GLOBAL              3 (isinstance)\n             86 LOAD_FAST                6 (pred)\n             88 LOAD_GLOBAL              4 (tuple)\n             90 CALL_FUNCTION            2\n             92 POP_JUMP_IF_FALSE       51 (to 102)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_CONST               1 (0)\n             98 BINARY_SUBSCR\n            100 STORE_FAST               6 (pred)\n        >>  102 LOAD_FAST                2 (self)\n            104 LOAD_ATTR                5 (compute_loss)\n            106 LOAD_FAST                6 (pred)\n            108 CALL_FUNCTION            1\n        >>  110 STORE_FAST               8 (loss)\n            112 POP_BLOCK\n\n328         114 LOAD_CONST               0 (None)\n            116 DUP_TOP\n            118 DUP_TOP\n            120 CALL_FUNCTION            3\n            122 POP_TOP\n            124 JUMP_FORWARD             8 (to 142)\n        >>  126 WITH_EXCEPT_START\n            128 POP_JUMP_IF_TRUE        66 (to 132)\n            130 RERAISE                  1\n        >>  132 POP_TOP\n            134 POP_TOP\n            136 POP_TOP\n            138 POP_EXCEPT\n            140 POP_TOP\n\n333     >>  142 LOAD_FAST                2 (self)\n            144 LOAD_ATTR                6 (grad_scaler)\n            146 LOAD_ATTR                7 (scale)\n            148 LOAD_FAST                8 (loss)\n            150 CALL_FUNCTION            1\n            152 LOAD_ATTR                8 (backward)\n            154 CALL_FUNCTION            0\n            156 POP_TOP\n\n334         158 LOAD_FAST                2 (self)\n            160 LOAD_ATTR                9 (optimizer_step)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n335         166 LOAD_FAST                4 (collect_outputs)\n            168 POP_JUMP_IF_FALSE       92 (to 184)\n\n336         170 LOAD_GLOBAL             10 (collect_results)\n            172 LOAD_FAST                3 (mod)\n            174 LOAD_FAST                6 (pred)\n            176 LOAD_FAST                8 (loss)\n            178 LOAD_FAST                5 (cloned_inputs)\n            180 CALL_FUNCTION            4\n            182 RETURN_VALUE\n\n337     >>  184 LOAD_CONST               0 (None)\n            186 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_and_backward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 325 \n326           0 LOAD_GLOBAL              0 (clone_inputs)\n              2 LOAD_FAST                2 (inputs)\n              4 CALL_FUNCTION            1\n              6 STORE_FAST               4 (cloned_inputs)\n\n327           8 LOAD_FAST                0 (self)\n             10 LOAD_METHOD              1 (optimizer_zero_grad)\n             12 LOAD_FAST                1 (mod)\n             14 CALL_METHOD              1\n             16 POP_TOP\n\n328          18 LOAD_FAST                0 (self)\n             20 LOAD_METHOD              2 (autocast)\n             22 CALL_METHOD              0\n             24 SETUP_WITH              26 (to 78)\n             26 POP_TOP\n\n329          28 LOAD_FAST                1 (mod)\n             30 LOAD_FAST                4 (cloned_inputs)\n             32 CALL_FUNCTION_EX         0\n             34 STORE_FAST               5 (pred)\n\n330          36 LOAD_GLOBAL              3 (isinstance)\n             38 LOAD_FAST                5 (pred)\n             40 LOAD_GLOBAL              4 (tuple)\n             42 CALL_FUNCTION            2\n             44 POP_JUMP_IF_FALSE       27 (to 54)\n\n331          46 LOAD_FAST                5 (pred)\n             48 LOAD_CONST               1 (0)\n             50 BINARY_SUBSCR\n             52 STORE_FAST               5 (pred)\n\n332     >>   54 LOAD_FAST                0 (self)\n             56 LOAD_METHOD              5 (compute_loss)\n             58 LOAD_FAST                5 (pred)\n             60 CALL_METHOD              1\n             62 STORE_FAST               6 (loss)\n             64 POP_BLOCK\n\n328          66 LOAD_CONST               0 (None)\n             68 DUP_TOP\n             70 DUP_TOP\n             72 CALL_FUNCTION            3\n             74 POP_TOP\n             76 JUMP_FORWARD             8 (to 94)\n        >>   78 WITH_EXCEPT_START\n             80 POP_JUMP_IF_TRUE        42 (to 84)\n             82 RERAISE                  1\n        >>   84 POP_TOP\n             86 POP_TOP\n             88 POP_TOP\n             90 POP_EXCEPT\n             92 POP_TOP\n\n333     >>   94 LOAD_FAST                0 (self)\n             96 LOAD_ATTR                6 (grad_scaler)\n             98 LOAD_METHOD              7 (scale)\n            100 LOAD_FAST                6 (loss)\n            102 CALL_METHOD              1\n            104 LOAD_METHOD              8 (backward)\n            106 CALL_METHOD              0\n            108 POP_TOP\n\n334         110 LOAD_FAST                0 (self)\n            112 LOAD_METHOD              9 (optimizer_step)\n            114 CALL_METHOD              0\n            116 POP_TOP\n\n335         118 LOAD_FAST                3 (collect_outputs)\n            120 POP_JUMP_IF_FALSE       68 (to 136)\n\n336         122 LOAD_GLOBAL             10 (collect_results)\n            124 LOAD_FAST                1 (mod)\n            126 LOAD_FAST                5 (pred)\n            128 LOAD_FAST                6 (loss)\n            130 LOAD_FAST                4 (cloned_inputs)\n            132 CALL_FUNCTION            4\n            134 RETURN_VALUE\n\n337     >>  136 LOAD_CONST               0 (None)\n            138 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 326 \n326           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE            5 (to 10)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                4 (inputs)\n              8 CALL_FUNCTION            1\n        >>   10 STORE_FAST               5 (cloned_inputs)\n\n327          12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                5 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                5 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE optimizer_zero_grad /workspace/youkaichao/code/pytorch/benchmarks/dynamo/common.py line 1875 \n1876           0 LOAD_FAST                0 (self)\n               2 LOAD_ATTR                0 (optimizer)\n               4 LOAD_CONST               0 (None)\n               6 IS_OP                    1\n               8 POP_JUMP_IF_FALSE       13 (to 26)\n\n1877          10 LOAD_FAST                0 (self)\n              12 LOAD_ATTR                0 (optimizer)\n              14 LOAD_METHOD              1 (zero_grad)\n              16 LOAD_CONST               1 (True)\n              18 CALL_METHOD              1\n              20 POP_TOP\n              22 LOAD_CONST               0 (None)\n              24 RETURN_VALUE\n\n1879     >>   26 LOAD_FAST                1 (mod)\n              28 LOAD_METHOD              1 (zero_grad)\n              30 LOAD_CONST               1 (True)\n              32 CALL_METHOD              1\n              34 POP_TOP\n              36 LOAD_CONST               0 (None)\n              38 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 327 \n327           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           10 (to 20)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                5 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n        >>   20 POP_TOP\n\n328          22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n\n329          32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               6 (pred)\n\n330          40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                6 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n\n331          50 LOAD_FAST                6 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               6 (pred)\n\n332     >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                6 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               7 (loss)\n             68 POP_BLOCK\n\n328          70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n\n333     >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                7 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                6 (pred)\n            132 LOAD_FAST                7 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE compute_loss /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 312 \n315           0 LOAD_GLOBAL              0 (reduce_to_scalar_loss)\n              2 LOAD_FAST                1 (pred)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 332 \n332           0 LOAD_FAST                0 (___stack0)\n              2 LOAD_CONST               2 ('cuda')\n              4 LOAD_CONST               3 (torch.float16)\n              6 LOAD_CONST               4 (True)\n              8 LOAD_CONST               4 (True)\n             10 CALL_FUNCTION            4\n             12 SETUP_WITH              10 (to 34)\n             14 POP_TOP\n             16 LOAD_FAST                1 (___stack1)\n             18 JUMP_ABSOLUTE           59 (to 118)\n             20 POP_BLOCK\n             22 LOAD_CONST               0 (None)\n             24 DUP_TOP\n             26 DUP_TOP\n             28 CALL_FUNCTION            3\n             30 POP_TOP\n             32 JUMP_FORWARD             8 (to 50)\n        >>   34 WITH_EXCEPT_START\n             36 POP_JUMP_IF_TRUE        20 (to 40)\n             38 RERAISE                  0\n        >>   40 POP_TOP\n             42 POP_TOP\n             44 POP_TOP\n             46 POP_EXCEPT\n             48 POP_TOP\n        >>   50 NOP\n             52 LOAD_CONST               0 (None)\n             54 RAISE_VARARGS            1\n             56 LOAD_GLOBAL              0 (clone_inputs)\n             58 LOAD_FAST                7 (inputs)\n             60 CALL_FUNCTION            1\n             62 STORE_FAST               5 (cloned_inputs)\n             64 LOAD_FAST                2 (self)\n             66 LOAD_ATTR                1 (optimizer_zero_grad)\n             68 LOAD_FAST                3 (mod)\n             70 CALL_FUNCTION            1\n             72 POP_TOP\n             74 LOAD_FAST                2 (self)\n             76 LOAD_ATTR                2 (autocast)\n             78 CALL_FUNCTION            0\n             80 SETUP_WITH              26 (to 134)\n             82 POP_TOP\n             84 LOAD_FAST                3 (mod)\n             86 LOAD_FAST                5 (cloned_inputs)\n             88 CALL_FUNCTION_EX         0\n             90 STORE_FAST               6 (pred)\n             92 LOAD_GLOBAL              3 (isinstance)\n             94 LOAD_FAST                6 (pred)\n             96 LOAD_GLOBAL              4 (tuple)\n             98 CALL_FUNCTION            2\n            100 POP_JUMP_IF_FALSE       55 (to 110)\n            102 LOAD_FAST                6 (pred)\n            104 LOAD_CONST               1 (0)\n            106 BINARY_SUBSCR\n            108 STORE_FAST               6 (pred)\n        >>  110 LOAD_FAST                2 (self)\n            112 LOAD_ATTR                5 (compute_loss)\n            114 LOAD_FAST                6 (pred)\n            116 CALL_FUNCTION            1\n        >>  118 STORE_FAST               8 (loss)\n            120 POP_BLOCK\n\n328         122 LOAD_CONST               0 (None)\n            124 DUP_TOP\n            126 DUP_TOP\n            128 CALL_FUNCTION            3\n            130 POP_TOP\n            132 JUMP_FORWARD             8 (to 150)\n        >>  134 WITH_EXCEPT_START\n            136 POP_JUMP_IF_TRUE        70 (to 140)\n            138 RERAISE                  1\n        >>  140 POP_TOP\n            142 POP_TOP\n            144 POP_TOP\n            146 POP_EXCEPT\n            148 POP_TOP\n\n333     >>  150 LOAD_FAST                2 (self)\n            152 LOAD_ATTR                6 (grad_scaler)\n            154 LOAD_ATTR                7 (scale)\n            156 LOAD_FAST                8 (loss)\n            158 CALL_FUNCTION            1\n            160 LOAD_ATTR                8 (backward)\n            162 CALL_FUNCTION            0\n            164 POP_TOP\n\n334         166 LOAD_FAST                2 (self)\n            168 LOAD_ATTR                9 (optimizer_step)\n            170 CALL_FUNCTION            0\n            172 POP_TOP\n\n335         174 LOAD_FAST                4 (collect_outputs)\n            176 POP_JUMP_IF_FALSE       96 (to 192)\n\n336         178 LOAD_GLOBAL             10 (collect_results)\n            180 LOAD_FAST                3 (mod)\n            182 LOAD_FAST                6 (pred)\n            184 LOAD_FAST                8 (loss)\n            186 LOAD_FAST                5 (cloned_inputs)\n            188 CALL_FUNCTION            4\n            190 RETURN_VALUE\n\n337     >>  192 LOAD_CONST               0 (None)\n            194 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 333 \n333           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           56 (to 112)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               4 (cloned_inputs)\n             12 LOAD_FAST                1 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                2 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                1 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                2 (mod)\n             34 LOAD_FAST                4 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               5 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                5 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                5 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               5 (pred)\n        >>   58 LOAD_FAST                1 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                5 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               6 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                1 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                6 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n        >>  112 POP_TOP\n\n334         114 LOAD_FAST                1 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n            120 POP_TOP\n\n335         122 LOAD_FAST                3 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                2 (mod)\n            130 LOAD_FAST                5 (pred)\n            132 LOAD_FAST                6 (loss)\n            134 LOAD_FAST                4 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n", "ORIGINAL BYTECODE sgd /workspace/youkaichao/code/pytorch/torch/optim/sgd.py line 185 \n204           0 LOAD_FAST                4 (foreach)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    0\n              6 POP_JUMP_IF_FALSE       21 (to 42)\n\n207           8 LOAD_GLOBAL              0 (torch)\n             10 LOAD_ATTR                1 (jit)\n             12 LOAD_METHOD              2 (is_scripting)\n             14 CALL_METHOD              0\n             16 POP_JUMP_IF_TRUE        19 (to 38)\n\n208          18 LOAD_GLOBAL              3 (_default_to_fused_or_foreach)\n             20 LOAD_FAST                0 (params)\n             22 LOAD_CONST               2 (False)\n             24 LOAD_CONST               2 (False)\n             26 LOAD_CONST               3 (('differentiable', 'use_fused'))\n             28 CALL_FUNCTION_KW         3\n             30 UNPACK_SEQUENCE          2\n             32 STORE_FAST              11 (_)\n             34 STORE_FAST               4 (foreach)\n             36 JUMP_FORWARD             2 (to 42)\n\n210     >>   38 LOAD_CONST               2 (False)\n             40 STORE_FAST               4 (foreach)\n\n212     >>   42 LOAD_FAST                4 (foreach)\n             44 POP_JUMP_IF_FALSE       32 (to 64)\n             46 LOAD_GLOBAL              0 (torch)\n             48 LOAD_ATTR                1 (jit)\n             50 LOAD_METHOD              2 (is_scripting)\n             52 CALL_METHOD              0\n             54 POP_JUMP_IF_FALSE       32 (to 64)\n\n213          56 LOAD_GLOBAL              4 (RuntimeError)\n             58 LOAD_CONST               4 ('torch.jit.script not supported with foreach optimizers')\n             60 CALL_FUNCTION            1\n             62 RAISE_VARARGS            1\n\n215     >>   64 LOAD_FAST                4 (foreach)\n             66 POP_JUMP_IF_FALSE       42 (to 84)\n             68 LOAD_GLOBAL              0 (torch)\n             70 LOAD_ATTR                1 (jit)\n             72 LOAD_METHOD              2 (is_scripting)\n             74 CALL_METHOD              0\n             76 POP_JUMP_IF_TRUE        42 (to 84)\n\n216          78 LOAD_GLOBAL              5 (_multi_tensor_sgd)\n             80 STORE_FAST              12 (func)\n             82 JUMP_FORWARD             2 (to 88)\n\n218     >>   84 LOAD_GLOBAL              6 (_single_tensor_sgd)\n             86 STORE_FAST              12 (func)\n\n220     >>   88 LOAD_FAST               12 (func)\n             90 LOAD_FAST                0 (params)\n\n221          92 LOAD_FAST                1 (d_p_list)\n\n222          94 LOAD_FAST                2 (momentum_buffer_list)\n\n223          96 LOAD_FAST                5 (weight_decay)\n\n224          98 LOAD_FAST                6 (momentum)\n\n225         100 LOAD_FAST                7 (lr)\n\n226         102 LOAD_FAST                8 (dampening)\n\n227         104 LOAD_FAST                9 (nesterov)\n\n228         106 LOAD_FAST                3 (has_sparse_grad)\n\n229         108 LOAD_FAST               10 (maximize)\n\n220         110 LOAD_CONST               5 (('weight_decay', 'momentum', 'lr', 'dampening', 'nesterov', 'has_sparse_grad', 'maximize'))\n            112 CALL_FUNCTION_KW        10\n            114 POP_TOP\n            116 LOAD_CONST               1 (None)\n            118 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward_and_backward_pass> /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/timm_models.py line 334 \n334           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           60 (to 120)\n              4 LOAD_GLOBAL              0 (clone_inputs)\n              6 LOAD_FAST                7 (inputs)\n              8 CALL_FUNCTION            1\n             10 STORE_FAST               3 (cloned_inputs)\n             12 LOAD_FAST                6 (self)\n             14 LOAD_ATTR                1 (optimizer_zero_grad)\n             16 LOAD_FAST                1 (mod)\n             18 CALL_FUNCTION            1\n             20 POP_TOP\n             22 LOAD_FAST                6 (self)\n             24 LOAD_ATTR                2 (autocast)\n             26 CALL_FUNCTION            0\n             28 SETUP_WITH              26 (to 82)\n             30 POP_TOP\n             32 LOAD_FAST                1 (mod)\n             34 LOAD_FAST                3 (cloned_inputs)\n             36 CALL_FUNCTION_EX         0\n             38 STORE_FAST               4 (pred)\n             40 LOAD_GLOBAL              3 (isinstance)\n             42 LOAD_FAST                4 (pred)\n             44 LOAD_GLOBAL              4 (tuple)\n             46 CALL_FUNCTION            2\n             48 POP_JUMP_IF_FALSE       29 (to 58)\n             50 LOAD_FAST                4 (pred)\n             52 LOAD_CONST               1 (0)\n             54 BINARY_SUBSCR\n             56 STORE_FAST               4 (pred)\n        >>   58 LOAD_FAST                6 (self)\n             60 LOAD_ATTR                5 (compute_loss)\n             62 LOAD_FAST                4 (pred)\n             64 CALL_FUNCTION            1\n             66 STORE_FAST               5 (loss)\n             68 POP_BLOCK\n             70 LOAD_CONST               0 (None)\n             72 DUP_TOP\n             74 DUP_TOP\n             76 CALL_FUNCTION            3\n             78 POP_TOP\n             80 JUMP_FORWARD             8 (to 98)\n        >>   82 WITH_EXCEPT_START\n             84 POP_JUMP_IF_TRUE        44 (to 88)\n             86 RERAISE                  1\n        >>   88 POP_TOP\n             90 POP_TOP\n             92 POP_TOP\n             94 POP_EXCEPT\n             96 POP_TOP\n        >>   98 LOAD_FAST                6 (self)\n            100 LOAD_ATTR                6 (grad_scaler)\n            102 LOAD_ATTR                7 (scale)\n            104 LOAD_FAST                5 (loss)\n            106 CALL_FUNCTION            1\n            108 LOAD_ATTR                8 (backward)\n            110 CALL_FUNCTION            0\n            112 POP_TOP\n            114 LOAD_FAST                6 (self)\n            116 LOAD_ATTR                9 (optimizer_step)\n            118 CALL_FUNCTION            0\n        >>  120 POP_TOP\n\n335         122 LOAD_FAST                2 (collect_outputs)\n            124 POP_JUMP_IF_FALSE       70 (to 140)\n\n336         126 LOAD_GLOBAL             10 (collect_results)\n            128 LOAD_FAST                1 (mod)\n            130 LOAD_FAST                4 (pred)\n            132 LOAD_FAST                5 (loss)\n            134 LOAD_FAST                3 (cloned_inputs)\n            136 CALL_FUNCTION            4\n            138 RETURN_VALUE\n\n337     >>  140 LOAD_CONST               0 (None)\n            142 RETURN_VALUE\n\n"]