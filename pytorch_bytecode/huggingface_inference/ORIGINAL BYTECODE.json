["ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py line 1789 \n1841           0 LOAD_FAST               11 (return_dict)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_FALSE        6 (to 12)\n               8 LOAD_FAST               11 (return_dict)\n              10 JUMP_FORWARD             3 (to 18)\n         >>   12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (use_return_dict)\n         >>   18 STORE_FAST              11 (return_dict)\n\n1843          20 LOAD_FAST                0 (self)\n              22 LOAD_ATTR                2 (longformer)\n\n1844          24 LOAD_FAST                1 (input_ids)\n\n1845          26 LOAD_FAST                2 (attention_mask)\n\n1846          28 LOAD_FAST                3 (global_attention_mask)\n\n1847          30 LOAD_FAST                4 (head_mask)\n\n1848          32 LOAD_FAST                5 (token_type_ids)\n\n1849          34 LOAD_FAST                6 (position_ids)\n\n1850          36 LOAD_FAST                7 (inputs_embeds)\n\n1851          38 LOAD_FAST                9 (output_attentions)\n\n1852          40 LOAD_FAST               10 (output_hidden_states)\n\n1853          42 LOAD_FAST               11 (return_dict)\n\n1843          44 LOAD_CONST               2 (('attention_mask', 'global_attention_mask', 'head_mask', 'token_type_ids', 'position_ids', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              46 CALL_FUNCTION_KW        10\n              48 STORE_FAST              12 (outputs)\n\n1855          50 LOAD_FAST               12 (outputs)\n              52 LOAD_CONST               3 (0)\n              54 BINARY_SUBSCR\n              56 STORE_FAST              13 (sequence_output)\n\n1856          58 LOAD_FAST                0 (self)\n              60 LOAD_METHOD              3 (lm_head)\n              62 LOAD_FAST               13 (sequence_output)\n              64 CALL_METHOD              1\n              66 STORE_FAST              14 (prediction_scores)\n\n1858          68 LOAD_CONST               1 (None)\n              70 STORE_FAST              15 (masked_lm_loss)\n\n1859          72 LOAD_FAST                8 (labels)\n              74 LOAD_CONST               1 (None)\n              76 IS_OP                    1\n              78 POP_JUMP_IF_FALSE       63 (to 126)\n\n1860          80 LOAD_GLOBAL              4 (CrossEntropyLoss)\n              82 CALL_FUNCTION            0\n              84 STORE_FAST              16 (loss_fct)\n\n1862          86 LOAD_FAST                8 (labels)\n              88 LOAD_METHOD              5 (to)\n              90 LOAD_FAST               14 (prediction_scores)\n              92 LOAD_ATTR                6 (device)\n              94 CALL_METHOD              1\n              96 STORE_FAST               8 (labels)\n\n1863          98 LOAD_FAST               16 (loss_fct)\n             100 LOAD_FAST               14 (prediction_scores)\n             102 LOAD_METHOD              7 (view)\n             104 LOAD_CONST               4 (-1)\n             106 LOAD_FAST                0 (self)\n             108 LOAD_ATTR                0 (config)\n             110 LOAD_ATTR                8 (vocab_size)\n             112 CALL_METHOD              2\n             114 LOAD_FAST                8 (labels)\n             116 LOAD_METHOD              7 (view)\n             118 LOAD_CONST               4 (-1)\n             120 CALL_METHOD              1\n             122 CALL_FUNCTION            2\n             124 STORE_FAST              15 (masked_lm_loss)\n\n1865     >>  126 LOAD_FAST               11 (return_dict)\n             128 POP_JUMP_IF_TRUE        85 (to 170)\n\n1866         130 LOAD_FAST               14 (prediction_scores)\n             132 BUILD_TUPLE              1\n             134 LOAD_FAST               12 (outputs)\n             136 LOAD_CONST               5 (2)\n             138 LOAD_CONST               1 (None)\n             140 BUILD_SLICE              2\n             142 BINARY_SUBSCR\n             144 BINARY_ADD\n             146 STORE_FAST              17 (output)\n\n1867         148 LOAD_FAST               15 (masked_lm_loss)\n             150 LOAD_CONST               1 (None)\n             152 IS_OP                    1\n             154 POP_JUMP_IF_FALSE       83 (to 166)\n             156 LOAD_FAST               15 (masked_lm_loss)\n             158 BUILD_TUPLE              1\n             160 LOAD_FAST               17 (output)\n             162 BINARY_ADD\n             164 RETURN_VALUE\n         >>  166 LOAD_FAST               17 (output)\n             168 RETURN_VALUE\n\n1869     >>  170 LOAD_GLOBAL              9 (LongformerMaskedLMOutput)\n\n1870         172 LOAD_FAST               15 (masked_lm_loss)\n\n1871         174 LOAD_FAST               14 (prediction_scores)\n\n1872         176 LOAD_FAST               12 (outputs)\n             178 LOAD_ATTR               10 (hidden_states)\n\n1873         180 LOAD_FAST               12 (outputs)\n             182 LOAD_ATTR               11 (attentions)\n\n1874         184 LOAD_FAST               12 (outputs)\n             186 LOAD_ATTR               12 (global_attentions)\n\n1869         188 LOAD_CONST               6 (('loss', 'logits', 'hidden_states', 'attentions', 'global_attentions'))\n             190 CALL_FUNCTION_KW         5\n             192 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py line 1647 \n1701           0 LOAD_FAST                8 (output_attentions)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_FALSE        6 (to 12)\n               8 LOAD_FAST                8 (output_attentions)\n              10 JUMP_FORWARD             3 (to 18)\n         >>   12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (output_attentions)\n         >>   18 STORE_FAST               8 (output_attentions)\n\n1703          20 LOAD_FAST                9 (output_hidden_states)\n              22 LOAD_CONST               1 (None)\n              24 IS_OP                    1\n              26 POP_JUMP_IF_FALSE       16 (to 32)\n              28 LOAD_FAST                9 (output_hidden_states)\n              30 JUMP_FORWARD             3 (to 38)\n         >>   32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                0 (config)\n              36 LOAD_ATTR                2 (output_hidden_states)\n\n1702     >>   38 STORE_FAST               9 (output_hidden_states)\n\n1705          40 LOAD_FAST               10 (return_dict)\n              42 LOAD_CONST               1 (None)\n              44 IS_OP                    1\n              46 POP_JUMP_IF_FALSE       26 (to 52)\n              48 LOAD_FAST               10 (return_dict)\n              50 JUMP_FORWARD             3 (to 58)\n         >>   52 LOAD_FAST                0 (self)\n              54 LOAD_ATTR                0 (config)\n              56 LOAD_ATTR                3 (use_return_dict)\n         >>   58 STORE_FAST              10 (return_dict)\n\n1707          60 LOAD_FAST                1 (input_ids)\n              62 LOAD_CONST               1 (None)\n              64 IS_OP                    1\n              66 POP_JUMP_IF_FALSE       42 (to 84)\n              68 LOAD_FAST                7 (inputs_embeds)\n              70 LOAD_CONST               1 (None)\n              72 IS_OP                    1\n              74 POP_JUMP_IF_FALSE       42 (to 84)\n\n1708          76 LOAD_GLOBAL              4 (ValueError)\n              78 LOAD_CONST               2 ('You cannot specify both input_ids and inputs_embeds at the same time')\n              80 CALL_FUNCTION            1\n              82 RAISE_VARARGS            1\n\n1709     >>   84 LOAD_FAST                1 (input_ids)\n              86 LOAD_CONST               1 (None)\n              88 IS_OP                    1\n              90 POP_JUMP_IF_FALSE       51 (to 102)\n\n1710          92 LOAD_FAST                1 (input_ids)\n              94 LOAD_METHOD              5 (size)\n              96 CALL_METHOD              0\n              98 STORE_FAST              11 (input_shape)\n             100 JUMP_FORWARD            17 (to 136)\n\n1711     >>  102 LOAD_FAST                7 (inputs_embeds)\n             104 LOAD_CONST               1 (None)\n             106 IS_OP                    1\n             108 POP_JUMP_IF_FALSE       64 (to 128)\n\n1712         110 LOAD_FAST                7 (inputs_embeds)\n             112 LOAD_METHOD              5 (size)\n             114 CALL_METHOD              0\n             116 LOAD_CONST               1 (None)\n             118 LOAD_CONST               3 (-1)\n             120 BUILD_SLICE              2\n             122 BINARY_SUBSCR\n             124 STORE_FAST              11 (input_shape)\n             126 JUMP_FORWARD             4 (to 136)\n\n1714     >>  128 LOAD_GLOBAL              4 (ValueError)\n             130 LOAD_CONST               4 ('You have to specify either input_ids or inputs_embeds')\n             132 CALL_FUNCTION            1\n             134 RAISE_VARARGS            1\n\n1716     >>  136 LOAD_FAST                1 (input_ids)\n             138 LOAD_CONST               1 (None)\n             140 IS_OP                    1\n             142 POP_JUMP_IF_FALSE       75 (to 150)\n             144 LOAD_FAST                1 (input_ids)\n             146 LOAD_ATTR                6 (device)\n             148 JUMP_FORWARD             2 (to 154)\n         >>  150 LOAD_FAST                7 (inputs_embeds)\n             152 LOAD_ATTR                6 (device)\n         >>  154 STORE_FAST              12 (device)\n\n1718         156 LOAD_FAST                2 (attention_mask)\n             158 LOAD_CONST               1 (None)\n             160 IS_OP                    0\n             162 POP_JUMP_IF_FALSE       89 (to 178)\n\n1719         164 LOAD_GLOBAL              7 (torch)\n             166 LOAD_ATTR                8 (ones)\n             168 LOAD_FAST               11 (input_shape)\n             170 LOAD_FAST               12 (device)\n             172 LOAD_CONST               5 (('device',))\n             174 CALL_FUNCTION_KW         2\n             176 STORE_FAST               2 (attention_mask)\n\n1720     >>  178 LOAD_FAST                5 (token_type_ids)\n             180 LOAD_CONST               1 (None)\n             182 IS_OP                    0\n             184 POP_JUMP_IF_FALSE      102 (to 204)\n\n1721         186 LOAD_GLOBAL              7 (torch)\n             188 LOAD_ATTR                9 (zeros)\n             190 LOAD_FAST               11 (input_shape)\n             192 LOAD_GLOBAL              7 (torch)\n             194 LOAD_ATTR               10 (long)\n             196 LOAD_FAST               12 (device)\n             198 LOAD_CONST               6 (('dtype', 'device'))\n             200 CALL_FUNCTION_KW         3\n             202 STORE_FAST               5 (token_type_ids)\n\n1724     >>  204 LOAD_FAST                3 (global_attention_mask)\n             206 LOAD_CONST               1 (None)\n             208 IS_OP                    1\n             210 POP_JUMP_IF_FALSE      112 (to 224)\n\n1725         212 LOAD_FAST                0 (self)\n             214 LOAD_METHOD             11 (_merge_to_attention_mask)\n             216 LOAD_FAST                2 (attention_mask)\n             218 LOAD_FAST                3 (global_attention_mask)\n             220 CALL_METHOD              2\n             222 STORE_FAST               2 (attention_mask)\n\n1727     >>  224 LOAD_FAST                0 (self)\n             226 LOAD_ATTR               12 (_pad_to_window_size)\n\n1728         228 LOAD_FAST                1 (input_ids)\n\n1729         230 LOAD_FAST                2 (attention_mask)\n\n1730         232 LOAD_FAST                5 (token_type_ids)\n\n1731         234 LOAD_FAST                6 (position_ids)\n\n1732         236 LOAD_FAST                7 (inputs_embeds)\n\n1733         238 LOAD_FAST                0 (self)\n             240 LOAD_ATTR                0 (config)\n             242 LOAD_ATTR               13 (pad_token_id)\n\n1727         244 LOAD_CONST               7 (('input_ids', 'attention_mask', 'token_type_ids', 'position_ids', 'inputs_embeds', 'pad_token_id'))\n             246 CALL_FUNCTION_KW         6\n             248 UNPACK_SEQUENCE          6\n             250 STORE_FAST              13 (padding_len)\n             252 STORE_FAST               1 (input_ids)\n             254 STORE_FAST               2 (attention_mask)\n             256 STORE_FAST               5 (token_type_ids)\n             258 STORE_FAST               6 (position_ids)\n             260 STORE_FAST               7 (inputs_embeds)\n\n1738         262 LOAD_FAST                0 (self)\n             264 LOAD_METHOD             14 (get_extended_attention_mask)\n             266 LOAD_FAST                2 (attention_mask)\n             268 LOAD_FAST               11 (input_shape)\n             270 CALL_METHOD              2\n\n1739         272 LOAD_CONST               1 (None)\n             274 LOAD_CONST               1 (None)\n             276 BUILD_SLICE              2\n             278 LOAD_CONST               8 (0)\n             280 LOAD_CONST               8 (0)\n             282 LOAD_CONST               1 (None)\n             284 LOAD_CONST               1 (None)\n             286 BUILD_SLICE              2\n             288 BUILD_TUPLE              4\n\n1738         290 BINARY_SUBSCR\n             292 STORE_FAST              14 (extended_attention_mask)\n\n1742         294 LOAD_FAST                0 (self)\n             296 LOAD_ATTR               15 (embeddings)\n\n1743         298 LOAD_FAST                1 (input_ids)\n             300 LOAD_FAST                6 (position_ids)\n             302 LOAD_FAST                5 (token_type_ids)\n             304 LOAD_FAST                7 (inputs_embeds)\n\n1742         306 LOAD_CONST               9 (('input_ids', 'position_ids', 'token_type_ids', 'inputs_embeds'))\n             308 CALL_FUNCTION_KW         4\n             310 STORE_FAST              15 (embedding_output)\n\n1746         312 LOAD_FAST                0 (self)\n             314 LOAD_ATTR               16 (encoder)\n\n1747         316 LOAD_FAST               15 (embedding_output)\n\n1748         318 LOAD_FAST               14 (extended_attention_mask)\n\n1749         320 LOAD_FAST                4 (head_mask)\n\n1750         322 LOAD_FAST               13 (padding_len)\n\n1751         324 LOAD_FAST                8 (output_attentions)\n\n1752         326 LOAD_FAST                9 (output_hidden_states)\n\n1753         328 LOAD_FAST               10 (return_dict)\n\n1746         330 LOAD_CONST              10 (('attention_mask', 'head_mask', 'padding_len', 'output_attentions', 'output_hidden_states', 'return_dict'))\n             332 CALL_FUNCTION_KW         7\n             334 STORE_FAST              16 (encoder_outputs)\n\n1755         336 LOAD_FAST               16 (encoder_outputs)\n             338 LOAD_CONST               8 (0)\n             340 BINARY_SUBSCR\n             342 STORE_FAST              17 (sequence_output)\n\n1756         344 LOAD_FAST                0 (self)\n             346 LOAD_ATTR               17 (pooler)\n             348 LOAD_CONST               1 (None)\n             350 IS_OP                    1\n             352 POP_JUMP_IF_FALSE      182 (to 364)\n             354 LOAD_FAST                0 (self)\n             356 LOAD_METHOD             17 (pooler)\n             358 LOAD_FAST               17 (sequence_output)\n             360 CALL_METHOD              1\n             362 JUMP_FORWARD             1 (to 366)\n         >>  364 LOAD_CONST               1 (None)\n         >>  366 STORE_FAST              18 (pooled_output)\n\n1758         368 LOAD_FAST               10 (return_dict)\n             370 POP_JUMP_IF_TRUE       196 (to 392)\n\n1759         372 LOAD_FAST               17 (sequence_output)\n             374 LOAD_FAST               18 (pooled_output)\n             376 BUILD_TUPLE              2\n             378 LOAD_FAST               16 (encoder_outputs)\n             380 LOAD_CONST              11 (1)\n             382 LOAD_CONST               1 (None)\n             384 BUILD_SLICE              2\n             386 BINARY_SUBSCR\n             388 BINARY_ADD\n             390 RETURN_VALUE\n\n1761     >>  392 LOAD_GLOBAL             18 (LongformerBaseModelOutputWithPooling)\n\n1762         394 LOAD_FAST               17 (sequence_output)\n\n1763         396 LOAD_FAST               18 (pooled_output)\n\n1764         398 LOAD_FAST               16 (encoder_outputs)\n             400 LOAD_ATTR               19 (hidden_states)\n\n1765         402 LOAD_FAST               16 (encoder_outputs)\n             404 LOAD_ATTR               20 (attentions)\n\n1766         406 LOAD_FAST               16 (encoder_outputs)\n             408 LOAD_ATTR               21 (global_attentions)\n\n1761         410 LOAD_CONST              12 (('last_hidden_state', 'pooler_output', 'hidden_states', 'attentions', 'global_attentions'))\n             412 CALL_FUNCTION_KW         5\n             414 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py line 1277 \n1287           0 LOAD_FAST                2 (attention_mask)\n               2 LOAD_CONST               1 (0)\n               4 COMPARE_OP               0 (<)\n               6 STORE_FAST               8 (is_index_masked)\n\n1288           8 LOAD_FAST                2 (attention_mask)\n              10 LOAD_CONST               1 (0)\n              12 COMPARE_OP               4 (>)\n              14 STORE_FAST               9 (is_index_global_attn)\n\n1291          16 LOAD_FAST                9 (is_index_global_attn)\n              18 LOAD_METHOD              0 (flatten)\n              20 CALL_METHOD              0\n              22 LOAD_METHOD              1 (any)\n              24 CALL_METHOD              0\n              26 LOAD_METHOD              2 (item)\n              28 CALL_METHOD              0\n              30 STORE_DEREF              0 (is_global_attn)\n\n1293          32 LOAD_FAST                6 (output_hidden_states)\n              34 POP_JUMP_IF_FALSE       20 (to 40)\n              36 LOAD_CONST               2 (())\n              38 JUMP_FORWARD             1 (to 42)\n         >>   40 LOAD_CONST               0 (None)\n         >>   42 STORE_FAST              10 (all_hidden_states)\n\n1294          44 LOAD_DEREF               1 (output_attentions)\n              46 POP_JUMP_IF_FALSE       26 (to 52)\n              48 LOAD_CONST               2 (())\n              50 JUMP_FORWARD             1 (to 54)\n         >>   52 LOAD_CONST               0 (None)\n         >>   54 STORE_FAST              11 (all_attentions)\n\n1295          56 LOAD_DEREF               1 (output_attentions)\n              58 POP_JUMP_IF_FALSE       34 (to 68)\n              60 LOAD_DEREF               0 (is_global_attn)\n              62 POP_JUMP_IF_FALSE       34 (to 68)\n              64 LOAD_CONST               2 (())\n              66 JUMP_FORWARD             1 (to 70)\n         >>   68 LOAD_CONST               0 (None)\n         >>   70 STORE_FAST              12 (all_global_attentions)\n\n1298          72 LOAD_FAST                3 (head_mask)\n              74 LOAD_CONST               0 (None)\n              76 IS_OP                    1\n              78 POP_JUMP_IF_FALSE       69 (to 138)\n\n1299          80 LOAD_FAST                3 (head_mask)\n              82 LOAD_METHOD              3 (size)\n              84 CALL_METHOD              0\n              86 LOAD_CONST               1 (0)\n              88 BINARY_SUBSCR\n\n1300          90 LOAD_GLOBAL              4 (len)\n              92 LOAD_FAST                0 (self)\n              94 LOAD_ATTR                5 (layer)\n              96 CALL_FUNCTION            1\n\n1299          98 COMPARE_OP               2 (==)\n             100 POP_JUMP_IF_TRUE        69 (to 138)\n             102 LOAD_ASSERTION_ERROR\n\n1301         104 LOAD_CONST               3 ('The head_mask should be specified for ')\n             106 LOAD_GLOBAL              4 (len)\n             108 LOAD_FAST                0 (self)\n             110 LOAD_ATTR                5 (layer)\n             112 CALL_FUNCTION            1\n             114 FORMAT_VALUE             0\n             116 LOAD_CONST               4 (' layers, but it is for ')\n             118 LOAD_FAST                3 (head_mask)\n             120 LOAD_METHOD              3 (size)\n             122 CALL_METHOD              0\n             124 LOAD_CONST               1 (0)\n             126 BINARY_SUBSCR\n             128 FORMAT_VALUE             0\n             130 LOAD_CONST               5 ('.')\n             132 BUILD_STRING             5\n\n1299         134 CALL_FUNCTION            1\n             136 RAISE_VARARGS            1\n\n1302     >>  138 LOAD_GLOBAL              6 (enumerate)\n             140 LOAD_FAST                0 (self)\n             142 LOAD_ATTR                5 (layer)\n             144 CALL_FUNCTION            1\n             146 GET_ITER\n         >>  148 FOR_ITER                96 (to 342)\n             150 UNPACK_SEQUENCE          2\n             152 STORE_FAST              13 (idx)\n             154 STORE_FAST              14 (layer_module)\n\n1303         156 LOAD_FAST                6 (output_hidden_states)\n             158 POP_JUMP_IF_FALSE       85 (to 170)\n\n1304         160 LOAD_FAST               10 (all_hidden_states)\n             162 LOAD_FAST                1 (hidden_states)\n             164 BUILD_TUPLE              1\n             166 BINARY_ADD\n             168 STORE_FAST              10 (all_hidden_states)\n\n1306     >>  170 LOAD_FAST                0 (self)\n             172 LOAD_ATTR                7 (gradient_checkpointing)\n             174 POP_JUMP_IF_FALSE      121 (to 242)\n             176 LOAD_FAST                0 (self)\n             178 LOAD_ATTR                8 (training)\n             180 POP_JUMP_IF_FALSE      121 (to 242)\n\n1308         182 LOAD_CLOSURE             0 (is_global_attn)\n             184 LOAD_CLOSURE             1 (output_attentions)\n             186 BUILD_TUPLE              2\n             188 LOAD_CONST               6 (<code object create_custom_forward at 0x7fd55ec01e70, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1308>)\n             190 LOAD_CONST               7 ('LongformerEncoder.forward.<locals>.create_custom_forward')\n             192 MAKE_FUNCTION            8 (closure)\n             194 STORE_FAST              15 (create_custom_forward)\n\n1314         196 LOAD_GLOBAL              9 (torch)\n             198 LOAD_ATTR               10 (utils)\n             200 LOAD_ATTR               11 (checkpoint)\n             202 LOAD_METHOD             11 (checkpoint)\n\n1315         204 LOAD_FAST               15 (create_custom_forward)\n             206 LOAD_FAST               14 (layer_module)\n             208 CALL_FUNCTION            1\n\n1316         210 LOAD_FAST                1 (hidden_states)\n\n1317         212 LOAD_FAST                2 (attention_mask)\n\n1318         214 LOAD_FAST                3 (head_mask)\n             216 LOAD_CONST               0 (None)\n             218 IS_OP                    1\n             220 POP_JUMP_IF_FALSE      115 (to 230)\n             222 LOAD_FAST                3 (head_mask)\n             224 LOAD_FAST               13 (idx)\n             226 BINARY_SUBSCR\n             228 JUMP_FORWARD             1 (to 232)\n         >>  230 LOAD_CONST               0 (None)\n\n1319     >>  232 LOAD_FAST                8 (is_index_masked)\n\n1320         234 LOAD_FAST                9 (is_index_global_attn)\n\n1314         236 CALL_METHOD              6\n             238 STORE_FAST              16 (layer_outputs)\n             240 JUMP_FORWARD            19 (to 280)\n\n1323     >>  242 LOAD_FAST               14 (layer_module)\n\n1324         244 LOAD_FAST                1 (hidden_states)\n\n1325         246 LOAD_FAST                2 (attention_mask)\n\n1326         248 LOAD_FAST                3 (head_mask)\n             250 LOAD_CONST               0 (None)\n             252 IS_OP                    1\n             254 POP_JUMP_IF_FALSE      132 (to 264)\n             256 LOAD_FAST                3 (head_mask)\n             258 LOAD_FAST               13 (idx)\n             260 BINARY_SUBSCR\n             262 JUMP_FORWARD             1 (to 266)\n         >>  264 LOAD_CONST               0 (None)\n\n1327     >>  266 LOAD_FAST                8 (is_index_masked)\n\n1328         268 LOAD_FAST                9 (is_index_global_attn)\n\n1329         270 LOAD_DEREF               0 (is_global_attn)\n\n1330         272 LOAD_DEREF               1 (output_attentions)\n\n1323         274 LOAD_CONST               8 (('attention_mask', 'layer_head_mask', 'is_index_masked', 'is_index_global_attn', 'is_global_attn', 'output_attentions'))\n             276 CALL_FUNCTION_KW         7\n             278 STORE_FAST              16 (layer_outputs)\n\n1332     >>  280 LOAD_FAST               16 (layer_outputs)\n             282 LOAD_CONST               1 (0)\n             284 BINARY_SUBSCR\n             286 STORE_FAST               1 (hidden_states)\n\n1334         288 LOAD_DEREF               1 (output_attentions)\n             290 POP_JUMP_IF_FALSE      170 (to 340)\n\n1336         292 LOAD_FAST               11 (all_attentions)\n             294 LOAD_FAST               16 (layer_outputs)\n             296 LOAD_CONST               9 (1)\n             298 BINARY_SUBSCR\n             300 LOAD_METHOD             12 (transpose)\n             302 LOAD_CONST               9 (1)\n             304 LOAD_CONST              10 (2)\n             306 CALL_METHOD              2\n             308 BUILD_TUPLE              1\n             310 BINARY_ADD\n             312 STORE_FAST              11 (all_attentions)\n\n1338         314 LOAD_DEREF               0 (is_global_attn)\n             316 POP_JUMP_IF_FALSE      170 (to 340)\n\n1340         318 LOAD_FAST               12 (all_global_attentions)\n             320 LOAD_FAST               16 (layer_outputs)\n             322 LOAD_CONST              10 (2)\n             324 BINARY_SUBSCR\n             326 LOAD_METHOD             12 (transpose)\n             328 LOAD_CONST              10 (2)\n             330 LOAD_CONST              11 (3)\n             332 CALL_METHOD              2\n             334 BUILD_TUPLE              1\n             336 BINARY_ADD\n             338 STORE_FAST              12 (all_global_attentions)\n         >>  340 JUMP_ABSOLUTE           74 (to 148)\n\n1343     >>  342 LOAD_FAST                6 (output_hidden_states)\n             344 POP_JUMP_IF_FALSE      178 (to 356)\n\n1344         346 LOAD_FAST               10 (all_hidden_states)\n             348 LOAD_FAST                1 (hidden_states)\n             350 BUILD_TUPLE              1\n             352 BINARY_ADD\n             354 STORE_FAST              10 (all_hidden_states)\n\n1348     >>  356 LOAD_FAST                1 (hidden_states)\n             358 LOAD_CONST               0 (None)\n             360 LOAD_CONST               0 (None)\n             362 BUILD_SLICE              2\n             364 LOAD_CONST               0 (None)\n             366 LOAD_FAST                1 (hidden_states)\n             368 LOAD_ATTR               13 (shape)\n             370 LOAD_CONST               9 (1)\n             372 BINARY_SUBSCR\n             374 LOAD_DEREF               2 (padding_len)\n             376 BINARY_SUBTRACT\n             378 BUILD_SLICE              2\n             380 BUILD_TUPLE              2\n             382 BINARY_SUBSCR\n             384 STORE_FAST               1 (hidden_states)\n\n1349         386 LOAD_FAST                6 (output_hidden_states)\n             388 POP_JUMP_IF_FALSE      206 (to 412)\n\n1350         390 LOAD_GLOBAL             14 (tuple)\n             392 LOAD_CLOSURE             2 (padding_len)\n             394 BUILD_TUPLE              1\n             396 LOAD_CONST              12 (<code object <listcomp> at 0x7fd55ec01f20, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1350>)\n             398 LOAD_CONST              13 ('LongformerEncoder.forward.<locals>.<listcomp>')\n             400 MAKE_FUNCTION            8 (closure)\n             402 LOAD_FAST               10 (all_hidden_states)\n             404 GET_ITER\n             406 CALL_FUNCTION            1\n             408 CALL_FUNCTION            1\n             410 STORE_FAST              10 (all_hidden_states)\n\n1352     >>  412 LOAD_DEREF               1 (output_attentions)\n             414 POP_JUMP_IF_FALSE      219 (to 438)\n\n1353         416 LOAD_GLOBAL             14 (tuple)\n             418 LOAD_CLOSURE             2 (padding_len)\n             420 BUILD_TUPLE              1\n             422 LOAD_CONST              14 (<code object <listcomp> at 0x7fd55ec01fd0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1353>)\n             424 LOAD_CONST              13 ('LongformerEncoder.forward.<locals>.<listcomp>')\n             426 MAKE_FUNCTION            8 (closure)\n             428 LOAD_FAST               11 (all_attentions)\n             430 GET_ITER\n             432 CALL_FUNCTION            1\n             434 CALL_FUNCTION            1\n             436 STORE_FAST              11 (all_attentions)\n\n1355     >>  438 LOAD_FAST                7 (return_dict)\n             440 POP_JUMP_IF_TRUE       234 (to 468)\n\n1356         442 LOAD_GLOBAL             14 (tuple)\n             444 LOAD_CONST              15 (<code object <genexpr> at 0x7fd55ec02080, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1356>)\n             446 LOAD_CONST              16 ('LongformerEncoder.forward.<locals>.<genexpr>')\n             448 MAKE_FUNCTION            0\n\n1357         450 LOAD_FAST                1 (hidden_states)\n             452 LOAD_FAST               10 (all_hidden_states)\n             454 LOAD_FAST               11 (all_attentions)\n             456 LOAD_FAST               12 (all_global_attentions)\n             458 BUILD_TUPLE              4\n\n1356         460 GET_ITER\n             462 CALL_FUNCTION            1\n             464 CALL_FUNCTION            1\n             466 RETURN_VALUE\n\n1359     >>  468 LOAD_GLOBAL             15 (LongformerBaseModelOutput)\n\n1360         470 LOAD_FAST                1 (hidden_states)\n\n1361         472 LOAD_FAST               10 (all_hidden_states)\n\n1362         474 LOAD_FAST               11 (all_attentions)\n\n1363         476 LOAD_FAST               12 (all_global_attentions)\n\n1359         478 LOAD_CONST              17 (('last_hidden_state', 'hidden_states', 'attentions', 'global_attentions'))\n             480 CALL_FUNCTION_KW         4\n             482 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py line 1291 \n1291           0 LOAD_FAST                0 (___stack0)\n               2 JUMP_ABSOLUTE           17 (to 34)\n               4 LOAD_FAST                3 (attention_mask)\n               6 LOAD_CONST               1 (0)\n               8 COMPARE_OP               0 (<)\n              10 STORE_FAST               7 (is_index_masked)\n              12 LOAD_FAST                3 (attention_mask)\n              14 LOAD_CONST               1 (0)\n              16 COMPARE_OP               4 (>)\n              18 STORE_FAST               8 (is_index_global_attn)\n              20 LOAD_FAST                8 (is_index_global_attn)\n              22 LOAD_ATTR                0 (flatten)\n              24 CALL_FUNCTION            0\n              26 LOAD_ATTR                1 (any)\n              28 CALL_FUNCTION            0\n              30 LOAD_ATTR                2 (item)\n              32 CALL_FUNCTION            0\n         >>   34 STORE_DEREF              0 (is_global_attn)\n\n1293          36 LOAD_FAST                5 (output_hidden_states)\n              38 POP_JUMP_IF_FALSE       22 (to 44)\n              40 LOAD_CONST               2 (())\n              42 JUMP_FORWARD             1 (to 46)\n         >>   44 LOAD_CONST               0 (None)\n         >>   46 STORE_FAST              11 (all_hidden_states)\n\n1294          48 LOAD_DEREF               1 (output_attentions)\n              50 POP_JUMP_IF_FALSE       28 (to 56)\n              52 LOAD_CONST               2 (())\n              54 JUMP_FORWARD             1 (to 58)\n         >>   56 LOAD_CONST               0 (None)\n         >>   58 STORE_FAST              12 (all_attentions)\n\n1295          60 LOAD_DEREF               1 (output_attentions)\n              62 POP_JUMP_IF_FALSE       36 (to 72)\n              64 LOAD_DEREF               0 (is_global_attn)\n              66 POP_JUMP_IF_FALSE       36 (to 72)\n              68 LOAD_CONST               2 (())\n              70 JUMP_FORWARD             1 (to 74)\n         >>   72 LOAD_CONST               0 (None)\n         >>   74 STORE_FAST              13 (all_global_attentions)\n\n1298          76 LOAD_FAST                4 (head_mask)\n              78 LOAD_CONST               0 (None)\n              80 IS_OP                    1\n              82 POP_JUMP_IF_FALSE       71 (to 142)\n\n1299          84 LOAD_FAST                4 (head_mask)\n              86 LOAD_ATTR                3 (size)\n              88 CALL_FUNCTION            0\n              90 LOAD_CONST               1 (0)\n              92 BINARY_SUBSCR\n\n1300          94 LOAD_GLOBAL              4 (len)\n              96 LOAD_FAST                1 (self)\n              98 LOAD_ATTR                5 (layer)\n             100 CALL_FUNCTION            1\n\n1299         102 COMPARE_OP               2 (==)\n             104 POP_JUMP_IF_TRUE        71 (to 142)\n             106 LOAD_ASSERTION_ERROR\n\n1301         108 LOAD_CONST               3 ('The head_mask should be specified for ')\n             110 LOAD_GLOBAL              4 (len)\n             112 LOAD_FAST                1 (self)\n             114 LOAD_ATTR                5 (layer)\n             116 CALL_FUNCTION            1\n             118 FORMAT_VALUE             0\n             120 LOAD_CONST               4 (' layers, but it is for ')\n             122 LOAD_FAST                4 (head_mask)\n             124 LOAD_ATTR                3 (size)\n             126 CALL_FUNCTION            0\n             128 LOAD_CONST               1 (0)\n             130 BINARY_SUBSCR\n             132 FORMAT_VALUE             0\n             134 LOAD_CONST               5 ('.')\n             136 BUILD_STRING             5\n\n1299         138 CALL_FUNCTION            1\n             140 RAISE_VARARGS            1\n\n1302     >>  142 LOAD_GLOBAL              6 (enumerate)\n             144 LOAD_FAST                1 (self)\n             146 LOAD_ATTR                5 (layer)\n             148 CALL_FUNCTION            1\n             150 GET_ITER\n         >>  152 FOR_ITER                96 (to 346)\n             154 UNPACK_SEQUENCE          2\n             156 STORE_FAST              14 (idx)\n             158 STORE_FAST              15 (layer_module)\n\n1303         160 LOAD_FAST                5 (output_hidden_states)\n             162 POP_JUMP_IF_FALSE       87 (to 174)\n\n1304         164 LOAD_FAST               11 (all_hidden_states)\n             166 LOAD_FAST                2 (hidden_states)\n             168 BUILD_TUPLE              1\n             170 BINARY_ADD\n             172 STORE_FAST              11 (all_hidden_states)\n\n1306     >>  174 LOAD_FAST                1 (self)\n             176 LOAD_ATTR                7 (gradient_checkpointing)\n             178 POP_JUMP_IF_FALSE      123 (to 246)\n             180 LOAD_FAST                1 (self)\n             182 LOAD_ATTR                8 (training)\n             184 POP_JUMP_IF_FALSE      123 (to 246)\n\n1308         186 LOAD_CLOSURE             0 (is_global_attn)\n             188 LOAD_CLOSURE             1 (output_attentions)\n             190 BUILD_TUPLE              2\n             192 LOAD_CONST               6 (<code object create_custom_forward at 0x7fd55ec01e70, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1308>)\n             194 LOAD_CONST               7 ('LongformerEncoder.forward.<locals>.create_custom_forward')\n             196 MAKE_FUNCTION            8 (closure)\n             198 STORE_FAST              16 (create_custom_forward)\n\n1314         200 LOAD_GLOBAL              9 (torch)\n             202 LOAD_ATTR               10 (utils)\n             204 LOAD_ATTR               11 (checkpoint)\n             206 LOAD_ATTR               11 (checkpoint)\n\n1315         208 LOAD_FAST               16 (create_custom_forward)\n             210 LOAD_FAST               15 (layer_module)\n             212 CALL_FUNCTION            1\n\n1316         214 LOAD_FAST                2 (hidden_states)\n\n1317         216 LOAD_FAST                3 (attention_mask)\n\n1318         218 LOAD_FAST                4 (head_mask)\n             220 LOAD_CONST               0 (None)\n             222 IS_OP                    1\n             224 POP_JUMP_IF_FALSE      117 (to 234)\n             226 LOAD_FAST                4 (head_mask)\n             228 LOAD_FAST               14 (idx)\n             230 BINARY_SUBSCR\n             232 JUMP_FORWARD             1 (to 236)\n         >>  234 LOAD_CONST               0 (None)\n\n1319     >>  236 LOAD_FAST                7 (is_index_masked)\n\n1320         238 LOAD_FAST                8 (is_index_global_attn)\n\n1314         240 CALL_FUNCTION            6\n             242 STORE_FAST              17 (layer_outputs)\n             244 JUMP_FORWARD            19 (to 284)\n\n1323     >>  246 LOAD_FAST               15 (layer_module)\n\n1324         248 LOAD_FAST                2 (hidden_states)\n\n1325         250 LOAD_FAST                3 (attention_mask)\n\n1326         252 LOAD_FAST                4 (head_mask)\n             254 LOAD_CONST               0 (None)\n             256 IS_OP                    1\n             258 POP_JUMP_IF_FALSE      134 (to 268)\n             260 LOAD_FAST                4 (head_mask)\n             262 LOAD_FAST               14 (idx)\n             264 BINARY_SUBSCR\n             266 JUMP_FORWARD             1 (to 270)\n         >>  268 LOAD_CONST               0 (None)\n\n1327     >>  270 LOAD_FAST                7 (is_index_masked)\n\n1328         272 LOAD_FAST                8 (is_index_global_attn)\n\n1329         274 LOAD_DEREF               0 (is_global_attn)\n\n1330         276 LOAD_DEREF               1 (output_attentions)\n\n1323         278 LOAD_CONST               8 (('attention_mask', 'layer_head_mask', 'is_index_masked', 'is_index_global_attn', 'is_global_attn', 'output_attentions'))\n             280 CALL_FUNCTION_KW         7\n             282 STORE_FAST              17 (layer_outputs)\n\n1332     >>  284 LOAD_FAST               17 (layer_outputs)\n             286 LOAD_CONST               1 (0)\n             288 BINARY_SUBSCR\n             290 STORE_FAST               2 (hidden_states)\n\n1334         292 LOAD_DEREF               1 (output_attentions)\n             294 POP_JUMP_IF_FALSE      172 (to 344)\n\n1336         296 LOAD_FAST               12 (all_attentions)\n             298 LOAD_FAST               17 (layer_outputs)\n             300 LOAD_CONST               9 (1)\n             302 BINARY_SUBSCR\n             304 LOAD_ATTR               12 (transpose)\n             306 LOAD_CONST               9 (1)\n             308 LOAD_CONST              10 (2)\n             310 CALL_FUNCTION            2\n             312 BUILD_TUPLE              1\n             314 BINARY_ADD\n             316 STORE_FAST              12 (all_attentions)\n\n1338         318 LOAD_DEREF               0 (is_global_attn)\n             320 POP_JUMP_IF_FALSE      172 (to 344)\n\n1340         322 LOAD_FAST               13 (all_global_attentions)\n             324 LOAD_FAST               17 (layer_outputs)\n             326 LOAD_CONST              10 (2)\n             328 BINARY_SUBSCR\n             330 LOAD_ATTR               12 (transpose)\n             332 LOAD_CONST              10 (2)\n             334 LOAD_CONST              11 (3)\n             336 CALL_FUNCTION            2\n             338 BUILD_TUPLE              1\n             340 BINARY_ADD\n             342 STORE_FAST              13 (all_global_attentions)\n         >>  344 JUMP_ABSOLUTE           76 (to 152)\n\n1343     >>  346 LOAD_FAST                5 (output_hidden_states)\n             348 POP_JUMP_IF_FALSE      180 (to 360)\n\n1344         350 LOAD_FAST               11 (all_hidden_states)\n             352 LOAD_FAST                2 (hidden_states)\n             354 BUILD_TUPLE              1\n             356 BINARY_ADD\n             358 STORE_FAST              11 (all_hidden_states)\n\n1348     >>  360 LOAD_FAST                2 (hidden_states)\n             362 LOAD_CONST               0 (None)\n             364 LOAD_CONST               0 (None)\n             366 BUILD_SLICE              2\n             368 LOAD_CONST               0 (None)\n             370 LOAD_FAST                2 (hidden_states)\n             372 LOAD_ATTR               13 (shape)\n             374 LOAD_CONST               9 (1)\n             376 BINARY_SUBSCR\n             378 LOAD_DEREF               2 (padding_len)\n             380 BINARY_SUBTRACT\n             382 BUILD_SLICE              2\n             384 BUILD_TUPLE              2\n             386 BINARY_SUBSCR\n             388 STORE_FAST               2 (hidden_states)\n\n1349         390 LOAD_FAST                5 (output_hidden_states)\n             392 POP_JUMP_IF_FALSE      208 (to 416)\n\n1350         394 LOAD_GLOBAL             14 (tuple)\n             396 LOAD_CLOSURE             2 (padding_len)\n             398 BUILD_TUPLE              1\n             400 LOAD_CONST              12 (<code object <listcomp> at 0x7fd55ec01f20, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1350>)\n             402 LOAD_CONST              13 ('LongformerEncoder.forward.<locals>.<listcomp>')\n             404 MAKE_FUNCTION            8 (closure)\n             406 LOAD_FAST               11 (all_hidden_states)\n             408 GET_ITER\n             410 CALL_FUNCTION            1\n             412 CALL_FUNCTION            1\n             414 STORE_FAST              11 (all_hidden_states)\n\n1352     >>  416 LOAD_DEREF               1 (output_attentions)\n             418 POP_JUMP_IF_FALSE      221 (to 442)\n\n1353         420 LOAD_GLOBAL             14 (tuple)\n             422 LOAD_CLOSURE             2 (padding_len)\n             424 BUILD_TUPLE              1\n             426 LOAD_CONST              14 (<code object <listcomp> at 0x7fd55ec01fd0, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1353>)\n             428 LOAD_CONST              13 ('LongformerEncoder.forward.<locals>.<listcomp>')\n             430 MAKE_FUNCTION            8 (closure)\n             432 LOAD_FAST               12 (all_attentions)\n             434 GET_ITER\n             436 CALL_FUNCTION            1\n             438 CALL_FUNCTION            1\n             440 STORE_FAST              12 (all_attentions)\n\n1355     >>  442 LOAD_FAST                6 (return_dict)\n             444 POP_JUMP_IF_TRUE       236 (to 472)\n\n1356         446 LOAD_GLOBAL             14 (tuple)\n             448 LOAD_CONST              15 (<code object <genexpr> at 0x7fd55ec02080, file \"/workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1356>)\n             450 LOAD_CONST              16 ('LongformerEncoder.forward.<locals>.<genexpr>')\n             452 MAKE_FUNCTION            0\n\n1357         454 LOAD_FAST                2 (hidden_states)\n             456 LOAD_FAST               11 (all_hidden_states)\n             458 LOAD_FAST               12 (all_attentions)\n             460 LOAD_FAST               13 (all_global_attentions)\n             462 BUILD_TUPLE              4\n\n1356         464 GET_ITER\n             466 CALL_FUNCTION            1\n             468 CALL_FUNCTION            1\n             470 RETURN_VALUE\n\n1359     >>  472 LOAD_GLOBAL             15 (LongformerBaseModelOutput)\n\n1360         474 LOAD_FAST                2 (hidden_states)\n\n1361         476 LOAD_FAST               11 (all_hidden_states)\n\n1362         478 LOAD_FAST               12 (all_attentions)\n\n1363         480 LOAD_FAST               13 (all_global_attentions)\n\n1359         482 LOAD_CONST              17 (('last_hidden_state', 'hidden_states', 'attentions', 'global_attentions'))\n             484 CALL_FUNCTION_KW         4\n             486 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py line 1843 \n1843           0 LOAD_FAST                0 (___stack0)\n               2 JUMP_ABSOLUTE           26 (to 52)\n               4 LOAD_FAST                3 (return_dict)\n               6 LOAD_CONST               1 (None)\n               8 IS_OP                    1\n              10 POP_JUMP_IF_FALSE        8 (to 16)\n              12 LOAD_FAST                3 (return_dict)\n              14 JUMP_FORWARD             3 (to 22)\n         >>   16 LOAD_FAST                1 (self)\n              18 LOAD_ATTR                0 (config)\n              20 LOAD_ATTR                1 (use_return_dict)\n         >>   22 STORE_FAST               3 (return_dict)\n              24 LOAD_FAST                1 (self)\n              26 LOAD_ATTR                2 (longformer)\n              28 LOAD_FAST                4 (input_ids)\n              30 LOAD_FAST                5 (attention_mask)\n              32 LOAD_FAST                6 (global_attention_mask)\n              34 LOAD_FAST                7 (head_mask)\n              36 LOAD_FAST                8 (token_type_ids)\n              38 LOAD_FAST                9 (position_ids)\n              40 LOAD_FAST               10 (inputs_embeds)\n              42 LOAD_FAST               11 (output_attentions)\n              44 LOAD_FAST               12 (output_hidden_states)\n              46 LOAD_FAST                3 (return_dict)\n              48 LOAD_CONST               2 (('attention_mask', 'global_attention_mask', 'head_mask', 'token_type_ids', 'position_ids', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              50 CALL_FUNCTION_KW        10\n         >>   52 STORE_FAST              13 (outputs)\n\n1855          54 LOAD_FAST               13 (outputs)\n              56 LOAD_CONST               3 (0)\n              58 BINARY_SUBSCR\n              60 STORE_FAST              14 (sequence_output)\n\n1856          62 LOAD_FAST                1 (self)\n              64 LOAD_ATTR                3 (lm_head)\n              66 LOAD_FAST               14 (sequence_output)\n              68 CALL_FUNCTION            1\n              70 STORE_FAST              15 (prediction_scores)\n\n1858          72 LOAD_CONST               1 (None)\n              74 STORE_FAST              16 (masked_lm_loss)\n\n1859          76 LOAD_FAST                2 (labels)\n              78 LOAD_CONST               1 (None)\n              80 IS_OP                    1\n              82 POP_JUMP_IF_FALSE       65 (to 130)\n\n1860          84 LOAD_GLOBAL              4 (CrossEntropyLoss)\n              86 CALL_FUNCTION            0\n              88 STORE_FAST              17 (loss_fct)\n\n1862          90 LOAD_FAST                2 (labels)\n              92 LOAD_ATTR                5 (to)\n              94 LOAD_FAST               15 (prediction_scores)\n              96 LOAD_ATTR                6 (device)\n              98 CALL_FUNCTION            1\n             100 STORE_FAST               2 (labels)\n\n1863         102 LOAD_FAST               17 (loss_fct)\n             104 LOAD_FAST               15 (prediction_scores)\n             106 LOAD_ATTR                7 (view)\n             108 LOAD_CONST               4 (-1)\n             110 LOAD_FAST                1 (self)\n             112 LOAD_ATTR                0 (config)\n             114 LOAD_ATTR                8 (vocab_size)\n             116 CALL_FUNCTION            2\n             118 LOAD_FAST                2 (labels)\n             120 LOAD_ATTR                7 (view)\n             122 LOAD_CONST               4 (-1)\n             124 CALL_FUNCTION            1\n             126 CALL_FUNCTION            2\n             128 STORE_FAST              16 (masked_lm_loss)\n\n1865     >>  130 LOAD_FAST                3 (return_dict)\n             132 POP_JUMP_IF_TRUE        87 (to 174)\n\n1866         134 LOAD_FAST               15 (prediction_scores)\n             136 BUILD_TUPLE              1\n             138 LOAD_FAST               13 (outputs)\n             140 LOAD_CONST               5 (2)\n             142 LOAD_CONST               1 (None)\n             144 BUILD_SLICE              2\n             146 BINARY_SUBSCR\n             148 BINARY_ADD\n             150 STORE_FAST              18 (output)\n\n1867         152 LOAD_FAST               16 (masked_lm_loss)\n             154 LOAD_CONST               1 (None)\n             156 IS_OP                    1\n             158 POP_JUMP_IF_FALSE       85 (to 170)\n             160 LOAD_FAST               16 (masked_lm_loss)\n             162 BUILD_TUPLE              1\n             164 LOAD_FAST               18 (output)\n             166 BINARY_ADD\n             168 RETURN_VALUE\n         >>  170 LOAD_FAST               18 (output)\n             172 RETURN_VALUE\n\n1869     >>  174 LOAD_GLOBAL              9 (LongformerMaskedLMOutput)\n\n1870         176 LOAD_FAST               16 (masked_lm_loss)\n\n1871         178 LOAD_FAST               15 (prediction_scores)\n\n1872         180 LOAD_FAST               13 (outputs)\n             182 LOAD_ATTR               10 (hidden_states)\n\n1873         184 LOAD_FAST               13 (outputs)\n             186 LOAD_ATTR               11 (attentions)\n\n1874         188 LOAD_FAST               13 (outputs)\n             190 LOAD_ATTR               12 (global_attentions)\n\n1869         192 LOAD_CONST               6 (('loss', 'logits', 'hidden_states', 'attentions', 'global_attentions'))\n             194 CALL_FUNCTION_KW         5\n             196 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 1326 \n1356           0 LOAD_FAST               12 (return_dict)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_FALSE        6 (to 12)\n               8 LOAD_FAST               12 (return_dict)\n              10 JUMP_FORWARD             3 (to 18)\n         >>   12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (use_return_dict)\n         >>   18 STORE_FAST              12 (return_dict)\n\n1358          20 LOAD_FAST                0 (self)\n              22 LOAD_ATTR                2 (bert)\n\n1359          24 LOAD_FAST                1 (input_ids)\n\n1360          26 LOAD_FAST                2 (attention_mask)\n\n1361          28 LOAD_FAST                3 (token_type_ids)\n\n1362          30 LOAD_FAST                4 (position_ids)\n\n1363          32 LOAD_FAST                5 (head_mask)\n\n1364          34 LOAD_FAST                6 (inputs_embeds)\n\n1365          36 LOAD_FAST                7 (encoder_hidden_states)\n\n1366          38 LOAD_FAST                8 (encoder_attention_mask)\n\n1367          40 LOAD_FAST               10 (output_attentions)\n\n1368          42 LOAD_FAST               11 (output_hidden_states)\n\n1369          44 LOAD_FAST               12 (return_dict)\n\n1358          46 LOAD_CONST               2 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'encoder_hidden_states', 'encoder_attention_mask', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              48 CALL_FUNCTION_KW        11\n              50 STORE_FAST              13 (outputs)\n\n1372          52 LOAD_FAST               13 (outputs)\n              54 LOAD_CONST               3 (0)\n              56 BINARY_SUBSCR\n              58 STORE_FAST              14 (sequence_output)\n\n1373          60 LOAD_FAST                0 (self)\n              62 LOAD_METHOD              3 (cls)\n              64 LOAD_FAST               14 (sequence_output)\n              66 CALL_METHOD              1\n              68 STORE_FAST              15 (prediction_scores)\n\n1375          70 LOAD_CONST               1 (None)\n              72 STORE_FAST              16 (masked_lm_loss)\n\n1376          74 LOAD_FAST                9 (labels)\n              76 LOAD_CONST               1 (None)\n              78 IS_OP                    1\n              80 POP_JUMP_IF_FALSE       58 (to 116)\n\n1377          82 LOAD_GLOBAL              4 (CrossEntropyLoss)\n              84 CALL_FUNCTION            0\n              86 STORE_FAST              17 (loss_fct)\n\n1378          88 LOAD_FAST               17 (loss_fct)\n              90 LOAD_FAST               15 (prediction_scores)\n              92 LOAD_METHOD              5 (view)\n              94 LOAD_CONST               4 (-1)\n              96 LOAD_FAST                0 (self)\n              98 LOAD_ATTR                0 (config)\n             100 LOAD_ATTR                6 (vocab_size)\n             102 CALL_METHOD              2\n             104 LOAD_FAST                9 (labels)\n             106 LOAD_METHOD              5 (view)\n             108 LOAD_CONST               4 (-1)\n             110 CALL_METHOD              1\n             112 CALL_FUNCTION            2\n             114 STORE_FAST              16 (masked_lm_loss)\n\n1380     >>  116 LOAD_FAST               12 (return_dict)\n             118 POP_JUMP_IF_TRUE        80 (to 160)\n\n1381         120 LOAD_FAST               15 (prediction_scores)\n             122 BUILD_TUPLE              1\n             124 LOAD_FAST               13 (outputs)\n             126 LOAD_CONST               5 (2)\n             128 LOAD_CONST               1 (None)\n             130 BUILD_SLICE              2\n             132 BINARY_SUBSCR\n             134 BINARY_ADD\n             136 STORE_FAST              18 (output)\n\n1382         138 LOAD_FAST               16 (masked_lm_loss)\n             140 LOAD_CONST               1 (None)\n             142 IS_OP                    1\n             144 POP_JUMP_IF_FALSE       78 (to 156)\n             146 LOAD_FAST               16 (masked_lm_loss)\n             148 BUILD_TUPLE              1\n             150 LOAD_FAST               18 (output)\n             152 BINARY_ADD\n             154 RETURN_VALUE\n         >>  156 LOAD_FAST               18 (output)\n             158 RETURN_VALUE\n\n1384     >>  160 LOAD_GLOBAL              7 (MaskedLMOutput)\n\n1385         162 LOAD_FAST               16 (masked_lm_loss)\n\n1386         164 LOAD_FAST               15 (prediction_scores)\n\n1387         166 LOAD_FAST               13 (outputs)\n             168 LOAD_ATTR                8 (hidden_states)\n\n1388         170 LOAD_FAST               13 (outputs)\n             172 LOAD_ATTR                9 (attentions)\n\n1384         174 LOAD_CONST               6 (('loss', 'logits', 'hidden_states', 'attentions'))\n             176 CALL_FUNCTION_KW         4\n             178 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 913 \n 955           0 LOAD_FAST               11 (output_attentions)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_FALSE        6 (to 12)\n               8 LOAD_FAST               11 (output_attentions)\n              10 JUMP_FORWARD             3 (to 18)\n         >>   12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (output_attentions)\n         >>   18 STORE_FAST              11 (output_attentions)\n\n 957          20 LOAD_FAST               12 (output_hidden_states)\n              22 LOAD_CONST               1 (None)\n              24 IS_OP                    1\n              26 POP_JUMP_IF_FALSE       16 (to 32)\n              28 LOAD_FAST               12 (output_hidden_states)\n              30 JUMP_FORWARD             3 (to 38)\n         >>   32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                0 (config)\n              36 LOAD_ATTR                2 (output_hidden_states)\n\n 956     >>   38 STORE_FAST              12 (output_hidden_states)\n\n 959          40 LOAD_FAST               13 (return_dict)\n              42 LOAD_CONST               1 (None)\n              44 IS_OP                    1\n              46 POP_JUMP_IF_FALSE       26 (to 52)\n              48 LOAD_FAST               13 (return_dict)\n              50 JUMP_FORWARD             3 (to 58)\n         >>   52 LOAD_FAST                0 (self)\n              54 LOAD_ATTR                0 (config)\n              56 LOAD_ATTR                3 (use_return_dict)\n         >>   58 STORE_FAST              13 (return_dict)\n\n 961          60 LOAD_FAST                0 (self)\n              62 LOAD_ATTR                0 (config)\n              64 LOAD_ATTR                4 (is_decoder)\n              66 POP_JUMP_IF_FALSE       45 (to 90)\n\n 962          68 LOAD_FAST               10 (use_cache)\n              70 LOAD_CONST               1 (None)\n              72 IS_OP                    1\n              74 POP_JUMP_IF_FALSE       40 (to 80)\n              76 LOAD_FAST               10 (use_cache)\n              78 JUMP_FORWARD             3 (to 86)\n         >>   80 LOAD_FAST                0 (self)\n              82 LOAD_ATTR                0 (config)\n              84 LOAD_ATTR                5 (use_cache)\n         >>   86 STORE_FAST              10 (use_cache)\n              88 JUMP_FORWARD             2 (to 94)\n\n 964     >>   90 LOAD_CONST               2 (False)\n              92 STORE_FAST              10 (use_cache)\n\n 966     >>   94 LOAD_FAST                1 (input_ids)\n              96 LOAD_CONST               1 (None)\n              98 IS_OP                    1\n             100 POP_JUMP_IF_FALSE       59 (to 118)\n             102 LOAD_FAST                6 (inputs_embeds)\n             104 LOAD_CONST               1 (None)\n             106 IS_OP                    1\n             108 POP_JUMP_IF_FALSE       59 (to 118)\n\n 967         110 LOAD_GLOBAL              6 (ValueError)\n             112 LOAD_CONST               3 ('You cannot specify both input_ids and inputs_embeds at the same time')\n             114 CALL_FUNCTION            1\n             116 RAISE_VARARGS            1\n\n 968     >>  118 LOAD_FAST                1 (input_ids)\n             120 LOAD_CONST               1 (None)\n             122 IS_OP                    1\n             124 POP_JUMP_IF_FALSE       74 (to 148)\n\n 969         126 LOAD_FAST                1 (input_ids)\n             128 LOAD_METHOD              7 (size)\n             130 CALL_METHOD              0\n             132 STORE_FAST              14 (input_shape)\n\n 970         134 LOAD_FAST                0 (self)\n             136 LOAD_METHOD              8 (warn_if_padding_and_no_attention_mask)\n             138 LOAD_FAST                1 (input_ids)\n             140 LOAD_FAST                2 (attention_mask)\n             142 CALL_METHOD              2\n             144 POP_TOP\n             146 JUMP_FORWARD            17 (to 182)\n\n 971     >>  148 LOAD_FAST                6 (inputs_embeds)\n             150 LOAD_CONST               1 (None)\n             152 IS_OP                    1\n             154 POP_JUMP_IF_FALSE       87 (to 174)\n\n 972         156 LOAD_FAST                6 (inputs_embeds)\n             158 LOAD_METHOD              7 (size)\n             160 CALL_METHOD              0\n             162 LOAD_CONST               1 (None)\n             164 LOAD_CONST               4 (-1)\n             166 BUILD_SLICE              2\n             168 BINARY_SUBSCR\n             170 STORE_FAST              14 (input_shape)\n             172 JUMP_FORWARD             4 (to 182)\n\n 974     >>  174 LOAD_GLOBAL              6 (ValueError)\n             176 LOAD_CONST               5 ('You have to specify either input_ids or inputs_embeds')\n             178 CALL_FUNCTION            1\n             180 RAISE_VARARGS            1\n\n 976     >>  182 LOAD_FAST               14 (input_shape)\n             184 UNPACK_SEQUENCE          2\n             186 STORE_FAST              15 (batch_size)\n             188 STORE_FAST              16 (seq_length)\n\n 977         190 LOAD_FAST                1 (input_ids)\n             192 LOAD_CONST               1 (None)\n             194 IS_OP                    1\n             196 POP_JUMP_IF_FALSE      102 (to 204)\n             198 LOAD_FAST                1 (input_ids)\n             200 LOAD_ATTR                9 (device)\n             202 JUMP_FORWARD             2 (to 208)\n         >>  204 LOAD_FAST                6 (inputs_embeds)\n             206 LOAD_ATTR                9 (device)\n         >>  208 STORE_FAST              17 (device)\n\n 980         210 LOAD_FAST                9 (past_key_values)\n             212 LOAD_CONST               1 (None)\n             214 IS_OP                    1\n             216 POP_JUMP_IF_FALSE      118 (to 236)\n             218 LOAD_FAST                9 (past_key_values)\n             220 LOAD_CONST               6 (0)\n             222 BINARY_SUBSCR\n             224 LOAD_CONST               6 (0)\n             226 BINARY_SUBSCR\n             228 LOAD_ATTR               10 (shape)\n             230 LOAD_CONST               7 (2)\n             232 BINARY_SUBSCR\n             234 JUMP_FORWARD             1 (to 238)\n         >>  236 LOAD_CONST               6 (0)\n         >>  238 STORE_FAST              18 (past_key_values_length)\n\n 982         240 LOAD_FAST                2 (attention_mask)\n             242 LOAD_CONST               1 (None)\n             244 IS_OP                    0\n             246 POP_JUMP_IF_FALSE      135 (to 270)\n\n 983         248 LOAD_GLOBAL             11 (torch)\n             250 LOAD_ATTR               12 (ones)\n             252 LOAD_FAST               15 (batch_size)\n             254 LOAD_FAST               16 (seq_length)\n             256 LOAD_FAST               18 (past_key_values_length)\n             258 BINARY_ADD\n             260 BUILD_TUPLE              2\n             262 LOAD_FAST               17 (device)\n             264 LOAD_CONST               8 (('device',))\n             266 CALL_FUNCTION_KW         2\n             268 STORE_FAST               2 (attention_mask)\n\n 985     >>  270 LOAD_FAST                3 (token_type_ids)\n             272 LOAD_CONST               1 (None)\n             274 IS_OP                    0\n             276 POP_JUMP_IF_FALSE      175 (to 350)\n\n 986         278 LOAD_GLOBAL             13 (hasattr)\n             280 LOAD_FAST                0 (self)\n             282 LOAD_ATTR               14 (embeddings)\n             284 LOAD_CONST               9 ('token_type_ids')\n             286 CALL_FUNCTION            2\n             288 POP_JUMP_IF_FALSE      166 (to 332)\n\n 987         290 LOAD_FAST                0 (self)\n             292 LOAD_ATTR               14 (embeddings)\n             294 LOAD_ATTR               15 (token_type_ids)\n             296 LOAD_CONST               1 (None)\n             298 LOAD_CONST               1 (None)\n             300 BUILD_SLICE              2\n             302 LOAD_CONST               1 (None)\n             304 LOAD_FAST               16 (seq_length)\n             306 BUILD_SLICE              2\n             308 BUILD_TUPLE              2\n             310 BINARY_SUBSCR\n             312 STORE_FAST              19 (buffered_token_type_ids)\n\n 988         314 LOAD_FAST               19 (buffered_token_type_ids)\n             316 LOAD_METHOD             16 (expand)\n             318 LOAD_FAST               15 (batch_size)\n             320 LOAD_FAST               16 (seq_length)\n             322 CALL_METHOD              2\n             324 STORE_FAST              20 (buffered_token_type_ids_expanded)\n\n 989         326 LOAD_FAST               20 (buffered_token_type_ids_expanded)\n             328 STORE_FAST               3 (token_type_ids)\n             330 JUMP_FORWARD             9 (to 350)\n\n 991     >>  332 LOAD_GLOBAL             11 (torch)\n             334 LOAD_ATTR               17 (zeros)\n             336 LOAD_FAST               14 (input_shape)\n             338 LOAD_GLOBAL             11 (torch)\n             340 LOAD_ATTR               18 (long)\n             342 LOAD_FAST               17 (device)\n             344 LOAD_CONST              10 (('dtype', 'device'))\n             346 CALL_FUNCTION_KW         3\n             348 STORE_FAST               3 (token_type_ids)\n\n 995     >>  350 LOAD_FAST                0 (self)\n             352 LOAD_METHOD             19 (get_extended_attention_mask)\n             354 LOAD_FAST                2 (attention_mask)\n             356 LOAD_FAST               14 (input_shape)\n             358 CALL_METHOD              2\n             360 STORE_FAST              21 (extended_attention_mask)\n\n 999         362 LOAD_FAST                0 (self)\n             364 LOAD_ATTR                0 (config)\n             366 LOAD_ATTR                4 (is_decoder)\n             368 POP_JUMP_IF_FALSE      217 (to 434)\n             370 LOAD_FAST                7 (encoder_hidden_states)\n             372 LOAD_CONST               1 (None)\n             374 IS_OP                    1\n             376 POP_JUMP_IF_FALSE      217 (to 434)\n\n1000         378 LOAD_FAST                7 (encoder_hidden_states)\n             380 LOAD_METHOD              7 (size)\n             382 CALL_METHOD              0\n             384 UNPACK_SEQUENCE          3\n             386 STORE_FAST              22 (encoder_batch_size)\n             388 STORE_FAST              23 (encoder_sequence_length)\n             390 STORE_FAST              24 (_)\n\n1001         392 LOAD_FAST               22 (encoder_batch_size)\n             394 LOAD_FAST               23 (encoder_sequence_length)\n             396 BUILD_TUPLE              2\n             398 STORE_FAST              25 (encoder_hidden_shape)\n\n1002         400 LOAD_FAST                8 (encoder_attention_mask)\n             402 LOAD_CONST               1 (None)\n             404 IS_OP                    0\n             406 POP_JUMP_IF_FALSE      211 (to 422)\n\n1003         408 LOAD_GLOBAL             11 (torch)\n             410 LOAD_ATTR               12 (ones)\n             412 LOAD_FAST               25 (encoder_hidden_shape)\n             414 LOAD_FAST               17 (device)\n             416 LOAD_CONST               8 (('device',))\n             418 CALL_FUNCTION_KW         2\n             420 STORE_FAST               8 (encoder_attention_mask)\n\n1004     >>  422 LOAD_FAST                0 (self)\n             424 LOAD_METHOD             20 (invert_attention_mask)\n             426 LOAD_FAST                8 (encoder_attention_mask)\n             428 CALL_METHOD              1\n             430 STORE_FAST              26 (encoder_extended_attention_mask)\n             432 JUMP_FORWARD             2 (to 438)\n\n1006     >>  434 LOAD_CONST               1 (None)\n             436 STORE_FAST              26 (encoder_extended_attention_mask)\n\n1013     >>  438 LOAD_FAST                0 (self)\n             440 LOAD_METHOD             21 (get_head_mask)\n             442 LOAD_FAST                5 (head_mask)\n             444 LOAD_FAST                0 (self)\n             446 LOAD_ATTR                0 (config)\n             448 LOAD_ATTR               22 (num_hidden_layers)\n             450 CALL_METHOD              2\n             452 STORE_FAST               5 (head_mask)\n\n1015         454 LOAD_FAST                0 (self)\n             456 LOAD_ATTR               14 (embeddings)\n\n1016         458 LOAD_FAST                1 (input_ids)\n\n1017         460 LOAD_FAST                4 (position_ids)\n\n1018         462 LOAD_FAST                3 (token_type_ids)\n\n1019         464 LOAD_FAST                6 (inputs_embeds)\n\n1020         466 LOAD_FAST               18 (past_key_values_length)\n\n1015         468 LOAD_CONST              11 (('input_ids', 'position_ids', 'token_type_ids', 'inputs_embeds', 'past_key_values_length'))\n             470 CALL_FUNCTION_KW         5\n             472 STORE_FAST              27 (embedding_output)\n\n1022         474 LOAD_FAST                0 (self)\n             476 LOAD_ATTR               23 (encoder)\n\n1023         478 LOAD_FAST               27 (embedding_output)\n\n1024         480 LOAD_FAST               21 (extended_attention_mask)\n\n1025         482 LOAD_FAST                5 (head_mask)\n\n1026         484 LOAD_FAST                7 (encoder_hidden_states)\n\n1027         486 LOAD_FAST               26 (encoder_extended_attention_mask)\n\n1028         488 LOAD_FAST                9 (past_key_values)\n\n1029         490 LOAD_FAST               10 (use_cache)\n\n1030         492 LOAD_FAST               11 (output_attentions)\n\n1031         494 LOAD_FAST               12 (output_hidden_states)\n\n1032         496 LOAD_FAST               13 (return_dict)\n\n1022         498 LOAD_CONST              12 (('attention_mask', 'head_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n             500 CALL_FUNCTION_KW        10\n             502 STORE_FAST              28 (encoder_outputs)\n\n1034         504 LOAD_FAST               28 (encoder_outputs)\n             506 LOAD_CONST               6 (0)\n             508 BINARY_SUBSCR\n             510 STORE_FAST              29 (sequence_output)\n\n1035         512 LOAD_FAST                0 (self)\n             514 LOAD_ATTR               24 (pooler)\n             516 LOAD_CONST               1 (None)\n             518 IS_OP                    1\n             520 EXTENDED_ARG             1\n             522 POP_JUMP_IF_FALSE      267 (to 534)\n             524 LOAD_FAST                0 (self)\n             526 LOAD_METHOD             24 (pooler)\n             528 LOAD_FAST               29 (sequence_output)\n             530 CALL_METHOD              1\n             532 JUMP_FORWARD             1 (to 536)\n         >>  534 LOAD_CONST               1 (None)\n         >>  536 STORE_FAST              30 (pooled_output)\n\n1037         538 LOAD_FAST               13 (return_dict)\n             540 EXTENDED_ARG             1\n             542 POP_JUMP_IF_TRUE       282 (to 564)\n\n1038         544 LOAD_FAST               29 (sequence_output)\n             546 LOAD_FAST               30 (pooled_output)\n             548 BUILD_TUPLE              2\n             550 LOAD_FAST               28 (encoder_outputs)\n             552 LOAD_CONST              13 (1)\n             554 LOAD_CONST               1 (None)\n             556 BUILD_SLICE              2\n             558 BINARY_SUBSCR\n             560 BINARY_ADD\n             562 RETURN_VALUE\n\n1040     >>  564 LOAD_GLOBAL             25 (BaseModelOutputWithPoolingAndCrossAttentions)\n\n1041         566 LOAD_FAST               29 (sequence_output)\n\n1042         568 LOAD_FAST               30 (pooled_output)\n\n1043         570 LOAD_FAST               28 (encoder_outputs)\n             572 LOAD_ATTR               26 (past_key_values)\n\n1044         574 LOAD_FAST               28 (encoder_outputs)\n             576 LOAD_ATTR               27 (hidden_states)\n\n1045         578 LOAD_FAST               28 (encoder_outputs)\n             580 LOAD_ATTR               28 (attentions)\n\n1046         582 LOAD_FAST               28 (encoder_outputs)\n             584 LOAD_ATTR               29 (cross_attentions)\n\n1040         586 LOAD_CONST              14 (('last_hidden_state', 'pooler_output', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n             588 CALL_FUNCTION_KW         6\n             590 RETURN_VALUE\n\n", "ORIGINAL BYTECODE warn_if_padding_and_no_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/modeling_utils.py line 3482 \n3486           0 LOAD_FAST                2 (attention_mask)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_TRUE        10 (to 20)\n               8 LOAD_FAST                0 (self)\n              10 LOAD_ATTR                0 (config)\n              12 LOAD_ATTR                1 (pad_token_id)\n              14 LOAD_CONST               1 (None)\n              16 IS_OP                    0\n              18 POP_JUMP_IF_FALSE       12 (to 24)\n\n3487     >>   20 LOAD_CONST               1 (None)\n              22 RETURN_VALUE\n\n3490     >>   24 LOAD_FAST                0 (self)\n              26 LOAD_ATTR                0 (config)\n              28 LOAD_ATTR                1 (pad_token_id)\n              30 LOAD_FAST                1 (input_ids)\n              32 LOAD_CONST               1 (None)\n              34 LOAD_CONST               1 (None)\n              36 BUILD_SLICE              2\n              38 LOAD_CONST               2 (-1)\n              40 LOAD_CONST               3 (0)\n              42 BUILD_LIST               2\n              44 BUILD_TUPLE              2\n              46 BINARY_SUBSCR\n              48 CONTAINS_OP              0\n              50 POP_JUMP_IF_FALSE      102 (to 204)\n\n3492          52 LOAD_CONST               4 ('We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.')\n\n3491          54 STORE_FAST               3 (warn_string)\n\n3500          56 LOAD_FAST                0 (self)\n              58 LOAD_ATTR                0 (config)\n              60 LOAD_ATTR                2 (bos_token_id)\n              62 LOAD_CONST               1 (None)\n              64 IS_OP                    1\n              66 POP_JUMP_IF_FALSE       42 (to 84)\n              68 LOAD_FAST                0 (self)\n              70 LOAD_ATTR                0 (config)\n              72 LOAD_ATTR                2 (bos_token_id)\n              74 LOAD_FAST                0 (self)\n              76 LOAD_ATTR                0 (config)\n              78 LOAD_ATTR                1 (pad_token_id)\n              80 COMPARE_OP               2 (==)\n              82 POP_JUMP_IF_TRUE        70 (to 140)\n\n3501     >>   84 LOAD_FAST                0 (self)\n              86 LOAD_ATTR                0 (config)\n              88 LOAD_ATTR                3 (eos_token_id)\n              90 LOAD_CONST               1 (None)\n              92 IS_OP                    1\n              94 POP_JUMP_IF_FALSE       56 (to 112)\n              96 LOAD_FAST                0 (self)\n              98 LOAD_ATTR                0 (config)\n             100 LOAD_ATTR                3 (eos_token_id)\n             102 LOAD_FAST                0 (self)\n             104 LOAD_ATTR                0 (config)\n             106 LOAD_ATTR                1 (pad_token_id)\n             108 COMPARE_OP               2 (==)\n             110 POP_JUMP_IF_TRUE        70 (to 140)\n\n3502     >>  112 LOAD_FAST                0 (self)\n             114 LOAD_ATTR                0 (config)\n             116 LOAD_ATTR                4 (sep_token_id)\n             118 LOAD_CONST               1 (None)\n             120 IS_OP                    1\n             122 POP_JUMP_IF_FALSE       95 (to 190)\n             124 LOAD_FAST                0 (self)\n             126 LOAD_ATTR                0 (config)\n             128 LOAD_ATTR                4 (sep_token_id)\n             130 LOAD_FAST                0 (self)\n             132 LOAD_ATTR                0 (config)\n             134 LOAD_ATTR                1 (pad_token_id)\n             136 COMPARE_OP               2 (==)\n             138 POP_JUMP_IF_FALSE       95 (to 190)\n\n3504     >>  140 LOAD_FAST                3 (warn_string)\n\n3505         142 LOAD_CONST               5 ('\\nYou may ignore this warning if your `pad_token_id` (')\n             144 LOAD_FAST                0 (self)\n             146 LOAD_ATTR                0 (config)\n             148 LOAD_ATTR                1 (pad_token_id)\n             150 FORMAT_VALUE             0\n             152 LOAD_CONST               6 (') is identical to the `bos_token_id` (')\n\n3506         154 LOAD_FAST                0 (self)\n             156 LOAD_ATTR                0 (config)\n             158 LOAD_ATTR                2 (bos_token_id)\n\n3505         160 FORMAT_VALUE             0\n             162 LOAD_CONST               7 ('), `eos_token_id` (')\n\n3506         164 LOAD_FAST                0 (self)\n             166 LOAD_ATTR                0 (config)\n             168 LOAD_ATTR                3 (eos_token_id)\n\n3505         170 FORMAT_VALUE             0\n             172 LOAD_CONST               8 ('), or the `sep_token_id` (')\n\n3507         174 LOAD_FAST                0 (self)\n             176 LOAD_ATTR                0 (config)\n             178 LOAD_ATTR                4 (sep_token_id)\n\n3505         180 FORMAT_VALUE             0\n             182 LOAD_CONST               9 ('), and your input is not padded.')\n             184 BUILD_STRING             9\n\n3504         186 INPLACE_ADD\n             188 STORE_FAST               3 (warn_string)\n\n3510     >>  190 LOAD_GLOBAL              5 (logger)\n             192 LOAD_METHOD              6 (warning_once)\n             194 LOAD_FAST                3 (warn_string)\n             196 CALL_METHOD              1\n             198 POP_TOP\n             200 LOAD_CONST               1 (None)\n             202 RETURN_VALUE\n\n3490     >>  204 LOAD_CONST               1 (None)\n             206 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 970 \n 970           0 LOAD_FAST                0 (___stack0)\n               2 JUMP_ABSOLUTE           74 (to 148)\n               4 LOAD_FAST               12 (output_attentions)\n               6 LOAD_CONST               1 (None)\n               8 IS_OP                    1\n              10 POP_JUMP_IF_FALSE        8 (to 16)\n              12 LOAD_FAST               12 (output_attentions)\n              14 JUMP_FORWARD             3 (to 22)\n         >>   16 LOAD_FAST                1 (self)\n              18 LOAD_ATTR                0 (config)\n              20 LOAD_ATTR                1 (output_attentions)\n         >>   22 STORE_FAST              12 (output_attentions)\n              24 LOAD_FAST               13 (output_hidden_states)\n              26 LOAD_CONST               1 (None)\n              28 IS_OP                    1\n              30 POP_JUMP_IF_FALSE       18 (to 36)\n              32 LOAD_FAST               13 (output_hidden_states)\n              34 JUMP_FORWARD             3 (to 42)\n         >>   36 LOAD_FAST                1 (self)\n              38 LOAD_ATTR                0 (config)\n              40 LOAD_ATTR                2 (output_hidden_states)\n         >>   42 STORE_FAST              13 (output_hidden_states)\n              44 LOAD_FAST               14 (return_dict)\n              46 LOAD_CONST               1 (None)\n              48 IS_OP                    1\n              50 POP_JUMP_IF_FALSE       28 (to 56)\n              52 LOAD_FAST               14 (return_dict)\n              54 JUMP_FORWARD             3 (to 62)\n         >>   56 LOAD_FAST                1 (self)\n              58 LOAD_ATTR                0 (config)\n              60 LOAD_ATTR                3 (use_return_dict)\n         >>   62 STORE_FAST              14 (return_dict)\n              64 LOAD_FAST                1 (self)\n              66 LOAD_ATTR                0 (config)\n              68 LOAD_ATTR                4 (is_decoder)\n              70 POP_JUMP_IF_FALSE       47 (to 94)\n              72 LOAD_FAST               11 (use_cache)\n              74 LOAD_CONST               1 (None)\n              76 IS_OP                    1\n              78 POP_JUMP_IF_FALSE       42 (to 84)\n              80 LOAD_FAST               11 (use_cache)\n              82 JUMP_FORWARD             3 (to 90)\n         >>   84 LOAD_FAST                1 (self)\n              86 LOAD_ATTR                0 (config)\n              88 LOAD_ATTR                5 (use_cache)\n         >>   90 STORE_FAST              11 (use_cache)\n              92 JUMP_FORWARD             2 (to 98)\n         >>   94 LOAD_CONST               2 (False)\n              96 STORE_FAST              11 (use_cache)\n         >>   98 LOAD_FAST                2 (input_ids)\n             100 LOAD_CONST               1 (None)\n             102 IS_OP                    1\n             104 POP_JUMP_IF_FALSE       61 (to 122)\n             106 LOAD_FAST                7 (inputs_embeds)\n             108 LOAD_CONST               1 (None)\n             110 IS_OP                    1\n             112 POP_JUMP_IF_FALSE       61 (to 122)\n             114 LOAD_GLOBAL              6 (ValueError)\n             116 LOAD_CONST               3 ('You cannot specify both input_ids and inputs_embeds at the same time')\n             118 CALL_FUNCTION            1\n             120 RAISE_VARARGS            1\n         >>  122 LOAD_FAST                2 (input_ids)\n             124 LOAD_CONST               1 (None)\n             126 IS_OP                    1\n             128 POP_JUMP_IF_FALSE       76 (to 152)\n             130 LOAD_FAST                2 (input_ids)\n             132 LOAD_ATTR                7 (size)\n             134 CALL_FUNCTION            0\n             136 STORE_FAST              15 (input_shape)\n             138 LOAD_FAST                1 (self)\n             140 LOAD_ATTR                8 (warn_if_padding_and_no_attention_mask)\n             142 LOAD_FAST                2 (input_ids)\n             144 LOAD_FAST                3 (attention_mask)\n             146 CALL_FUNCTION            2\n         >>  148 POP_TOP\n             150 JUMP_FORWARD            17 (to 186)\n\n 971     >>  152 LOAD_FAST                7 (inputs_embeds)\n             154 LOAD_CONST               1 (None)\n             156 IS_OP                    1\n             158 POP_JUMP_IF_FALSE       89 (to 178)\n\n 972         160 LOAD_FAST                7 (inputs_embeds)\n             162 LOAD_ATTR                7 (size)\n             164 CALL_FUNCTION            0\n             166 LOAD_CONST               1 (None)\n             168 LOAD_CONST               4 (-1)\n             170 BUILD_SLICE              2\n             172 BINARY_SUBSCR\n             174 STORE_FAST              15 (input_shape)\n             176 JUMP_FORWARD             4 (to 186)\n\n 974     >>  178 LOAD_GLOBAL              6 (ValueError)\n             180 LOAD_CONST               5 ('You have to specify either input_ids or inputs_embeds')\n             182 CALL_FUNCTION            1\n             184 RAISE_VARARGS            1\n\n 976     >>  186 LOAD_FAST               15 (input_shape)\n             188 UNPACK_SEQUENCE          2\n             190 STORE_FAST              16 (batch_size)\n             192 STORE_FAST              17 (seq_length)\n\n 977         194 LOAD_FAST                2 (input_ids)\n             196 LOAD_CONST               1 (None)\n             198 IS_OP                    1\n             200 POP_JUMP_IF_FALSE      104 (to 208)\n             202 LOAD_FAST                2 (input_ids)\n             204 LOAD_ATTR                9 (device)\n             206 JUMP_FORWARD             2 (to 212)\n         >>  208 LOAD_FAST                7 (inputs_embeds)\n             210 LOAD_ATTR                9 (device)\n         >>  212 STORE_FAST              18 (device)\n\n 980         214 LOAD_FAST               10 (past_key_values)\n             216 LOAD_CONST               1 (None)\n             218 IS_OP                    1\n             220 POP_JUMP_IF_FALSE      120 (to 240)\n             222 LOAD_FAST               10 (past_key_values)\n             224 LOAD_CONST               6 (0)\n             226 BINARY_SUBSCR\n             228 LOAD_CONST               6 (0)\n             230 BINARY_SUBSCR\n             232 LOAD_ATTR               10 (shape)\n             234 LOAD_CONST               7 (2)\n             236 BINARY_SUBSCR\n             238 JUMP_FORWARD             1 (to 242)\n         >>  240 LOAD_CONST               6 (0)\n         >>  242 STORE_FAST              19 (past_key_values_length)\n\n 982         244 LOAD_FAST                3 (attention_mask)\n             246 LOAD_CONST               1 (None)\n             248 IS_OP                    0\n             250 POP_JUMP_IF_FALSE      137 (to 274)\n\n 983         252 LOAD_GLOBAL             11 (torch)\n             254 LOAD_ATTR               12 (ones)\n             256 LOAD_FAST               16 (batch_size)\n             258 LOAD_FAST               17 (seq_length)\n             260 LOAD_FAST               19 (past_key_values_length)\n             262 BINARY_ADD\n             264 BUILD_TUPLE              2\n             266 LOAD_FAST               18 (device)\n             268 LOAD_CONST               8 (('device',))\n             270 CALL_FUNCTION_KW         2\n             272 STORE_FAST               3 (attention_mask)\n\n 985     >>  274 LOAD_FAST                4 (token_type_ids)\n             276 LOAD_CONST               1 (None)\n             278 IS_OP                    0\n             280 POP_JUMP_IF_FALSE      177 (to 354)\n\n 986         282 LOAD_GLOBAL             13 (hasattr)\n             284 LOAD_FAST                1 (self)\n             286 LOAD_ATTR               14 (embeddings)\n             288 LOAD_CONST               9 ('token_type_ids')\n             290 CALL_FUNCTION            2\n             292 POP_JUMP_IF_FALSE      168 (to 336)\n\n 987         294 LOAD_FAST                1 (self)\n             296 LOAD_ATTR               14 (embeddings)\n             298 LOAD_ATTR               15 (token_type_ids)\n             300 LOAD_CONST               1 (None)\n             302 LOAD_CONST               1 (None)\n             304 BUILD_SLICE              2\n             306 LOAD_CONST               1 (None)\n             308 LOAD_FAST               17 (seq_length)\n             310 BUILD_SLICE              2\n             312 BUILD_TUPLE              2\n             314 BINARY_SUBSCR\n             316 STORE_FAST              20 (buffered_token_type_ids)\n\n 988         318 LOAD_FAST               20 (buffered_token_type_ids)\n             320 LOAD_ATTR               16 (expand)\n             322 LOAD_FAST               16 (batch_size)\n             324 LOAD_FAST               17 (seq_length)\n             326 CALL_FUNCTION            2\n             328 STORE_FAST              21 (buffered_token_type_ids_expanded)\n\n 989         330 LOAD_FAST               21 (buffered_token_type_ids_expanded)\n             332 STORE_FAST               4 (token_type_ids)\n             334 JUMP_FORWARD             9 (to 354)\n\n 991     >>  336 LOAD_GLOBAL             11 (torch)\n             338 LOAD_ATTR               17 (zeros)\n             340 LOAD_FAST               15 (input_shape)\n             342 LOAD_GLOBAL             11 (torch)\n             344 LOAD_ATTR               18 (long)\n             346 LOAD_FAST               18 (device)\n             348 LOAD_CONST              10 (('dtype', 'device'))\n             350 CALL_FUNCTION_KW         3\n             352 STORE_FAST               4 (token_type_ids)\n\n 995     >>  354 LOAD_FAST                1 (self)\n             356 LOAD_ATTR               19 (get_extended_attention_mask)\n             358 LOAD_FAST                3 (attention_mask)\n             360 LOAD_FAST               15 (input_shape)\n             362 CALL_FUNCTION            2\n             364 STORE_FAST              22 (extended_attention_mask)\n\n 999         366 LOAD_FAST                1 (self)\n             368 LOAD_ATTR                0 (config)\n             370 LOAD_ATTR                4 (is_decoder)\n             372 POP_JUMP_IF_FALSE      219 (to 438)\n             374 LOAD_FAST                8 (encoder_hidden_states)\n             376 LOAD_CONST               1 (None)\n             378 IS_OP                    1\n             380 POP_JUMP_IF_FALSE      219 (to 438)\n\n1000         382 LOAD_FAST                8 (encoder_hidden_states)\n             384 LOAD_ATTR                7 (size)\n             386 CALL_FUNCTION            0\n             388 UNPACK_SEQUENCE          3\n             390 STORE_FAST              23 (encoder_batch_size)\n             392 STORE_FAST              24 (encoder_sequence_length)\n             394 STORE_FAST              25 (_)\n\n1001         396 LOAD_FAST               23 (encoder_batch_size)\n             398 LOAD_FAST               24 (encoder_sequence_length)\n             400 BUILD_TUPLE              2\n             402 STORE_FAST              26 (encoder_hidden_shape)\n\n1002         404 LOAD_FAST                9 (encoder_attention_mask)\n             406 LOAD_CONST               1 (None)\n             408 IS_OP                    0\n             410 POP_JUMP_IF_FALSE      213 (to 426)\n\n1003         412 LOAD_GLOBAL             11 (torch)\n             414 LOAD_ATTR               12 (ones)\n             416 LOAD_FAST               26 (encoder_hidden_shape)\n             418 LOAD_FAST               18 (device)\n             420 LOAD_CONST               8 (('device',))\n             422 CALL_FUNCTION_KW         2\n             424 STORE_FAST               9 (encoder_attention_mask)\n\n1004     >>  426 LOAD_FAST                1 (self)\n             428 LOAD_ATTR               20 (invert_attention_mask)\n             430 LOAD_FAST                9 (encoder_attention_mask)\n             432 CALL_FUNCTION            1\n             434 STORE_FAST              27 (encoder_extended_attention_mask)\n             436 JUMP_FORWARD             2 (to 442)\n\n1006     >>  438 LOAD_CONST               1 (None)\n             440 STORE_FAST              27 (encoder_extended_attention_mask)\n\n1013     >>  442 LOAD_FAST                1 (self)\n             444 LOAD_ATTR               21 (get_head_mask)\n             446 LOAD_FAST                6 (head_mask)\n             448 LOAD_FAST                1 (self)\n             450 LOAD_ATTR                0 (config)\n             452 LOAD_ATTR               22 (num_hidden_layers)\n             454 CALL_FUNCTION            2\n             456 STORE_FAST               6 (head_mask)\n\n1015         458 LOAD_FAST                1 (self)\n             460 LOAD_ATTR               14 (embeddings)\n\n1016         462 LOAD_FAST                2 (input_ids)\n\n1017         464 LOAD_FAST                5 (position_ids)\n\n1018         466 LOAD_FAST                4 (token_type_ids)\n\n1019         468 LOAD_FAST                7 (inputs_embeds)\n\n1020         470 LOAD_FAST               19 (past_key_values_length)\n\n1015         472 LOAD_CONST              11 (('input_ids', 'position_ids', 'token_type_ids', 'inputs_embeds', 'past_key_values_length'))\n             474 CALL_FUNCTION_KW         5\n             476 STORE_FAST              28 (embedding_output)\n\n1022         478 LOAD_FAST                1 (self)\n             480 LOAD_ATTR               23 (encoder)\n\n1023         482 LOAD_FAST               28 (embedding_output)\n\n1024         484 LOAD_FAST               22 (extended_attention_mask)\n\n1025         486 LOAD_FAST                6 (head_mask)\n\n1026         488 LOAD_FAST                8 (encoder_hidden_states)\n\n1027         490 LOAD_FAST               27 (encoder_extended_attention_mask)\n\n1028         492 LOAD_FAST               10 (past_key_values)\n\n1029         494 LOAD_FAST               11 (use_cache)\n\n1030         496 LOAD_FAST               12 (output_attentions)\n\n1031         498 LOAD_FAST               13 (output_hidden_states)\n\n1032         500 LOAD_FAST               14 (return_dict)\n\n1022         502 LOAD_CONST              12 (('attention_mask', 'head_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n             504 CALL_FUNCTION_KW        10\n             506 STORE_FAST              29 (encoder_outputs)\n\n1034         508 LOAD_FAST               29 (encoder_outputs)\n             510 LOAD_CONST               6 (0)\n             512 BINARY_SUBSCR\n             514 STORE_FAST              30 (sequence_output)\n\n1035         516 LOAD_FAST                1 (self)\n             518 LOAD_ATTR               24 (pooler)\n             520 LOAD_CONST               1 (None)\n             522 IS_OP                    1\n             524 EXTENDED_ARG             1\n             526 POP_JUMP_IF_FALSE      269 (to 538)\n             528 LOAD_FAST                1 (self)\n             530 LOAD_ATTR               24 (pooler)\n             532 LOAD_FAST               30 (sequence_output)\n             534 CALL_FUNCTION            1\n             536 JUMP_FORWARD             1 (to 540)\n         >>  538 LOAD_CONST               1 (None)\n         >>  540 STORE_FAST              31 (pooled_output)\n\n1037         542 LOAD_FAST               14 (return_dict)\n             544 EXTENDED_ARG             1\n             546 POP_JUMP_IF_TRUE       284 (to 568)\n\n1038         548 LOAD_FAST               30 (sequence_output)\n             550 LOAD_FAST               31 (pooled_output)\n             552 BUILD_TUPLE              2\n             554 LOAD_FAST               29 (encoder_outputs)\n             556 LOAD_CONST              13 (1)\n             558 LOAD_CONST               1 (None)\n             560 BUILD_SLICE              2\n             562 BINARY_SUBSCR\n             564 BINARY_ADD\n             566 RETURN_VALUE\n\n1040     >>  568 LOAD_GLOBAL             25 (BaseModelOutputWithPoolingAndCrossAttentions)\n\n1041         570 LOAD_FAST               30 (sequence_output)\n\n1042         572 LOAD_FAST               31 (pooled_output)\n\n1043         574 LOAD_FAST               29 (encoder_outputs)\n             576 LOAD_ATTR               26 (past_key_values)\n\n1044         578 LOAD_FAST               29 (encoder_outputs)\n             580 LOAD_ATTR               27 (hidden_states)\n\n1045         582 LOAD_FAST               29 (encoder_outputs)\n             584 LOAD_ATTR               28 (attentions)\n\n1046         586 LOAD_FAST               29 (encoder_outputs)\n             588 LOAD_ATTR               29 (cross_attentions)\n\n1040         590 LOAD_CONST              14 (('last_hidden_state', 'pooler_output', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n             592 CALL_FUNCTION_KW         6\n             594 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 1358 \n1358           0 LOAD_FAST                0 (___stack0)\n               2 JUMP_ABSOLUTE           27 (to 54)\n               4 LOAD_FAST                3 (return_dict)\n               6 LOAD_CONST               1 (None)\n               8 IS_OP                    1\n              10 POP_JUMP_IF_FALSE        8 (to 16)\n              12 LOAD_FAST                3 (return_dict)\n              14 JUMP_FORWARD             3 (to 22)\n         >>   16 LOAD_FAST                1 (self)\n              18 LOAD_ATTR                0 (config)\n              20 LOAD_ATTR                1 (use_return_dict)\n         >>   22 STORE_FAST               3 (return_dict)\n              24 LOAD_FAST                1 (self)\n              26 LOAD_ATTR                2 (bert)\n              28 LOAD_FAST                4 (input_ids)\n              30 LOAD_FAST                5 (attention_mask)\n              32 LOAD_FAST                6 (token_type_ids)\n              34 LOAD_FAST                7 (position_ids)\n              36 LOAD_FAST                8 (head_mask)\n              38 LOAD_FAST                9 (inputs_embeds)\n              40 LOAD_FAST               10 (encoder_hidden_states)\n              42 LOAD_FAST               11 (encoder_attention_mask)\n              44 LOAD_FAST               12 (output_attentions)\n              46 LOAD_FAST               13 (output_hidden_states)\n              48 LOAD_FAST                3 (return_dict)\n              50 LOAD_CONST               2 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'encoder_hidden_states', 'encoder_attention_mask', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              52 CALL_FUNCTION_KW        11\n         >>   54 STORE_FAST              14 (outputs)\n\n1372          56 LOAD_FAST               14 (outputs)\n              58 LOAD_CONST               3 (0)\n              60 BINARY_SUBSCR\n              62 STORE_FAST              15 (sequence_output)\n\n1373          64 LOAD_FAST                1 (self)\n              66 LOAD_ATTR                3 (cls)\n              68 LOAD_FAST               15 (sequence_output)\n              70 CALL_FUNCTION            1\n              72 STORE_FAST              16 (prediction_scores)\n\n1375          74 LOAD_CONST               1 (None)\n              76 STORE_FAST              17 (masked_lm_loss)\n\n1376          78 LOAD_FAST                2 (labels)\n              80 LOAD_CONST               1 (None)\n              82 IS_OP                    1\n              84 POP_JUMP_IF_FALSE       60 (to 120)\n\n1377          86 LOAD_GLOBAL              4 (CrossEntropyLoss)\n              88 CALL_FUNCTION            0\n              90 STORE_FAST              18 (loss_fct)\n\n1378          92 LOAD_FAST               18 (loss_fct)\n              94 LOAD_FAST               16 (prediction_scores)\n              96 LOAD_ATTR                5 (view)\n              98 LOAD_CONST               4 (-1)\n             100 LOAD_FAST                1 (self)\n             102 LOAD_ATTR                0 (config)\n             104 LOAD_ATTR                6 (vocab_size)\n             106 CALL_FUNCTION            2\n             108 LOAD_FAST                2 (labels)\n             110 LOAD_ATTR                5 (view)\n             112 LOAD_CONST               4 (-1)\n             114 CALL_FUNCTION            1\n             116 CALL_FUNCTION            2\n             118 STORE_FAST              17 (masked_lm_loss)\n\n1380     >>  120 LOAD_FAST                3 (return_dict)\n             122 POP_JUMP_IF_TRUE        82 (to 164)\n\n1381         124 LOAD_FAST               16 (prediction_scores)\n             126 BUILD_TUPLE              1\n             128 LOAD_FAST               14 (outputs)\n             130 LOAD_CONST               5 (2)\n             132 LOAD_CONST               1 (None)\n             134 BUILD_SLICE              2\n             136 BINARY_SUBSCR\n             138 BINARY_ADD\n             140 STORE_FAST              19 (output)\n\n1382         142 LOAD_FAST               17 (masked_lm_loss)\n             144 LOAD_CONST               1 (None)\n             146 IS_OP                    1\n             148 POP_JUMP_IF_FALSE       80 (to 160)\n             150 LOAD_FAST               17 (masked_lm_loss)\n             152 BUILD_TUPLE              1\n             154 LOAD_FAST               19 (output)\n             156 BINARY_ADD\n             158 RETURN_VALUE\n         >>  160 LOAD_FAST               19 (output)\n             162 RETURN_VALUE\n\n1384     >>  164 LOAD_GLOBAL              7 (MaskedLMOutput)\n\n1385         166 LOAD_FAST               17 (masked_lm_loss)\n\n1386         168 LOAD_FAST               16 (prediction_scores)\n\n1387         170 LOAD_FAST               14 (outputs)\n             172 LOAD_ATTR                8 (hidden_states)\n\n1388         174 LOAD_FAST               14 (outputs)\n             176 LOAD_ATTR                9 (attentions)\n\n1384         178 LOAD_CONST               6 (('loss', 'logits', 'hidden_states', 'attentions'))\n             180 CALL_FUNCTION_KW         4\n             182 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 1808 \n1842           0 LOAD_FAST               11 (return_dict)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_FALSE        6 (to 12)\n               8 LOAD_FAST               11 (return_dict)\n              10 JUMP_FORWARD             3 (to 18)\n         >>   12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (use_return_dict)\n         >>   18 STORE_FAST              11 (return_dict)\n\n1844          20 LOAD_FAST                0 (self)\n              22 LOAD_ATTR                2 (bert)\n\n1845          24 LOAD_FAST                1 (input_ids)\n\n1846          26 LOAD_FAST                2 (attention_mask)\n\n1847          28 LOAD_FAST                3 (token_type_ids)\n\n1848          30 LOAD_FAST                4 (position_ids)\n\n1849          32 LOAD_FAST                5 (head_mask)\n\n1850          34 LOAD_FAST                6 (inputs_embeds)\n\n1851          36 LOAD_FAST                9 (output_attentions)\n\n1852          38 LOAD_FAST               10 (output_hidden_states)\n\n1853          40 LOAD_FAST               11 (return_dict)\n\n1844          42 LOAD_CONST               2 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              44 CALL_FUNCTION_KW         9\n              46 STORE_FAST              12 (outputs)\n\n1856          48 LOAD_FAST               12 (outputs)\n              50 LOAD_CONST               3 (0)\n              52 BINARY_SUBSCR\n              54 STORE_FAST              13 (sequence_output)\n\n1858          56 LOAD_FAST                0 (self)\n              58 LOAD_METHOD              3 (qa_outputs)\n              60 LOAD_FAST               13 (sequence_output)\n              62 CALL_METHOD              1\n              64 STORE_FAST              14 (logits)\n\n1859          66 LOAD_FAST               14 (logits)\n              68 LOAD_ATTR                4 (split)\n              70 LOAD_CONST               4 (1)\n              72 LOAD_CONST               5 (-1)\n              74 LOAD_CONST               6 (('dim',))\n              76 CALL_FUNCTION_KW         2\n              78 UNPACK_SEQUENCE          2\n              80 STORE_FAST              15 (start_logits)\n              82 STORE_FAST              16 (end_logits)\n\n1860          84 LOAD_FAST               15 (start_logits)\n              86 LOAD_METHOD              5 (squeeze)\n              88 LOAD_CONST               5 (-1)\n              90 CALL_METHOD              1\n              92 LOAD_METHOD              6 (contiguous)\n              94 CALL_METHOD              0\n              96 STORE_FAST              15 (start_logits)\n\n1861          98 LOAD_FAST               16 (end_logits)\n             100 LOAD_METHOD              5 (squeeze)\n             102 LOAD_CONST               5 (-1)\n             104 CALL_METHOD              1\n             106 LOAD_METHOD              6 (contiguous)\n             108 CALL_METHOD              0\n             110 STORE_FAST              16 (end_logits)\n\n1863         112 LOAD_CONST               1 (None)\n             114 STORE_FAST              17 (total_loss)\n\n1864         116 LOAD_FAST                7 (start_positions)\n             118 LOAD_CONST               1 (None)\n             120 IS_OP                    1\n             122 POP_JUMP_IF_FALSE      130 (to 260)\n             124 LOAD_FAST                8 (end_positions)\n             126 LOAD_CONST               1 (None)\n             128 IS_OP                    1\n             130 POP_JUMP_IF_FALSE      130 (to 260)\n\n1866         132 LOAD_GLOBAL              7 (len)\n             134 LOAD_FAST                7 (start_positions)\n             136 LOAD_METHOD              8 (size)\n             138 CALL_METHOD              0\n             140 CALL_FUNCTION            1\n             142 LOAD_CONST               4 (1)\n             144 COMPARE_OP               4 (>)\n             146 POP_JUMP_IF_FALSE       79 (to 158)\n\n1867         148 LOAD_FAST                7 (start_positions)\n             150 LOAD_METHOD              5 (squeeze)\n             152 LOAD_CONST               5 (-1)\n             154 CALL_METHOD              1\n             156 STORE_FAST               7 (start_positions)\n\n1868     >>  158 LOAD_GLOBAL              7 (len)\n             160 LOAD_FAST                8 (end_positions)\n             162 LOAD_METHOD              8 (size)\n             164 CALL_METHOD              0\n             166 CALL_FUNCTION            1\n             168 LOAD_CONST               4 (1)\n             170 COMPARE_OP               4 (>)\n             172 POP_JUMP_IF_FALSE       92 (to 184)\n\n1869         174 LOAD_FAST                8 (end_positions)\n             176 LOAD_METHOD              5 (squeeze)\n             178 LOAD_CONST               5 (-1)\n             180 CALL_METHOD              1\n             182 STORE_FAST               8 (end_positions)\n\n1871     >>  184 LOAD_FAST               15 (start_logits)\n             186 LOAD_METHOD              8 (size)\n             188 LOAD_CONST               4 (1)\n             190 CALL_METHOD              1\n             192 STORE_FAST              18 (ignored_index)\n\n1872         194 LOAD_FAST                7 (start_positions)\n             196 LOAD_METHOD              9 (clamp)\n             198 LOAD_CONST               3 (0)\n             200 LOAD_FAST               18 (ignored_index)\n             202 CALL_METHOD              2\n             204 STORE_FAST               7 (start_positions)\n\n1873         206 LOAD_FAST                8 (end_positions)\n             208 LOAD_METHOD              9 (clamp)\n             210 LOAD_CONST               3 (0)\n             212 LOAD_FAST               18 (ignored_index)\n             214 CALL_METHOD              2\n             216 STORE_FAST               8 (end_positions)\n\n1875         218 LOAD_GLOBAL             10 (CrossEntropyLoss)\n             220 LOAD_FAST               18 (ignored_index)\n             222 LOAD_CONST               7 (('ignore_index',))\n             224 CALL_FUNCTION_KW         1\n             226 STORE_FAST              19 (loss_fct)\n\n1876         228 LOAD_FAST               19 (loss_fct)\n             230 LOAD_FAST               15 (start_logits)\n             232 LOAD_FAST                7 (start_positions)\n             234 CALL_FUNCTION            2\n             236 STORE_FAST              20 (start_loss)\n\n1877         238 LOAD_FAST               19 (loss_fct)\n             240 LOAD_FAST               16 (end_logits)\n             242 LOAD_FAST                8 (end_positions)\n             244 CALL_FUNCTION            2\n             246 STORE_FAST              21 (end_loss)\n\n1878         248 LOAD_FAST               20 (start_loss)\n             250 LOAD_FAST               21 (end_loss)\n             252 BINARY_ADD\n             254 LOAD_CONST               8 (2)\n             256 BINARY_TRUE_DIVIDE\n             258 STORE_FAST              17 (total_loss)\n\n1880     >>  260 LOAD_FAST               11 (return_dict)\n             262 POP_JUMP_IF_TRUE       153 (to 306)\n\n1881         264 LOAD_FAST               15 (start_logits)\n             266 LOAD_FAST               16 (end_logits)\n             268 BUILD_TUPLE              2\n             270 LOAD_FAST               12 (outputs)\n             272 LOAD_CONST               8 (2)\n             274 LOAD_CONST               1 (None)\n             276 BUILD_SLICE              2\n             278 BINARY_SUBSCR\n             280 BINARY_ADD\n             282 STORE_FAST              22 (output)\n\n1882         284 LOAD_FAST               17 (total_loss)\n             286 LOAD_CONST               1 (None)\n             288 IS_OP                    1\n             290 POP_JUMP_IF_FALSE      151 (to 302)\n             292 LOAD_FAST               17 (total_loss)\n             294 BUILD_TUPLE              1\n             296 LOAD_FAST               22 (output)\n             298 BINARY_ADD\n             300 RETURN_VALUE\n         >>  302 LOAD_FAST               22 (output)\n             304 RETURN_VALUE\n\n1884     >>  306 LOAD_GLOBAL             11 (QuestionAnsweringModelOutput)\n\n1885         308 LOAD_FAST               17 (total_loss)\n\n1886         310 LOAD_FAST               15 (start_logits)\n\n1887         312 LOAD_FAST               16 (end_logits)\n\n1888         314 LOAD_FAST               12 (outputs)\n             316 LOAD_ATTR               12 (hidden_states)\n\n1889         318 LOAD_FAST               12 (outputs)\n             320 LOAD_ATTR               13 (attentions)\n\n1884         322 LOAD_CONST               9 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n             324 CALL_FUNCTION_KW         5\n             326 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 913 \n 955           0 LOAD_FAST               11 (output_attentions)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_FALSE        6 (to 12)\n               8 LOAD_FAST               11 (output_attentions)\n              10 JUMP_FORWARD             3 (to 18)\n         >>   12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (output_attentions)\n         >>   18 STORE_FAST              11 (output_attentions)\n\n 957          20 LOAD_FAST               12 (output_hidden_states)\n              22 LOAD_CONST               1 (None)\n              24 IS_OP                    1\n              26 POP_JUMP_IF_FALSE       16 (to 32)\n              28 LOAD_FAST               12 (output_hidden_states)\n              30 JUMP_FORWARD             3 (to 38)\n         >>   32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                0 (config)\n              36 LOAD_ATTR                2 (output_hidden_states)\n\n 956     >>   38 STORE_FAST              12 (output_hidden_states)\n\n 959          40 LOAD_FAST               13 (return_dict)\n              42 LOAD_CONST               1 (None)\n              44 IS_OP                    1\n              46 POP_JUMP_IF_FALSE       26 (to 52)\n              48 LOAD_FAST               13 (return_dict)\n              50 JUMP_FORWARD             3 (to 58)\n         >>   52 LOAD_FAST                0 (self)\n              54 LOAD_ATTR                0 (config)\n              56 LOAD_ATTR                3 (use_return_dict)\n         >>   58 STORE_FAST              13 (return_dict)\n\n 961          60 LOAD_FAST                0 (self)\n              62 LOAD_ATTR                0 (config)\n              64 LOAD_ATTR                4 (is_decoder)\n              66 POP_JUMP_IF_FALSE       45 (to 90)\n\n 962          68 LOAD_FAST               10 (use_cache)\n              70 LOAD_CONST               1 (None)\n              72 IS_OP                    1\n              74 POP_JUMP_IF_FALSE       40 (to 80)\n              76 LOAD_FAST               10 (use_cache)\n              78 JUMP_FORWARD             3 (to 86)\n         >>   80 LOAD_FAST                0 (self)\n              82 LOAD_ATTR                0 (config)\n              84 LOAD_ATTR                5 (use_cache)\n         >>   86 STORE_FAST              10 (use_cache)\n              88 JUMP_FORWARD             2 (to 94)\n\n 964     >>   90 LOAD_CONST               2 (False)\n              92 STORE_FAST              10 (use_cache)\n\n 966     >>   94 LOAD_FAST                1 (input_ids)\n              96 LOAD_CONST               1 (None)\n              98 IS_OP                    1\n             100 POP_JUMP_IF_FALSE       59 (to 118)\n             102 LOAD_FAST                6 (inputs_embeds)\n             104 LOAD_CONST               1 (None)\n             106 IS_OP                    1\n             108 POP_JUMP_IF_FALSE       59 (to 118)\n\n 967         110 LOAD_GLOBAL              6 (ValueError)\n             112 LOAD_CONST               3 ('You cannot specify both input_ids and inputs_embeds at the same time')\n             114 CALL_FUNCTION            1\n             116 RAISE_VARARGS            1\n\n 968     >>  118 LOAD_FAST                1 (input_ids)\n             120 LOAD_CONST               1 (None)\n             122 IS_OP                    1\n             124 POP_JUMP_IF_FALSE       74 (to 148)\n\n 969         126 LOAD_FAST                1 (input_ids)\n             128 LOAD_METHOD              7 (size)\n             130 CALL_METHOD              0\n             132 STORE_FAST              14 (input_shape)\n\n 970         134 LOAD_FAST                0 (self)\n             136 LOAD_METHOD              8 (warn_if_padding_and_no_attention_mask)\n             138 LOAD_FAST                1 (input_ids)\n             140 LOAD_FAST                2 (attention_mask)\n             142 CALL_METHOD              2\n             144 POP_TOP\n             146 JUMP_FORWARD            17 (to 182)\n\n 971     >>  148 LOAD_FAST                6 (inputs_embeds)\n             150 LOAD_CONST               1 (None)\n             152 IS_OP                    1\n             154 POP_JUMP_IF_FALSE       87 (to 174)\n\n 972         156 LOAD_FAST                6 (inputs_embeds)\n             158 LOAD_METHOD              7 (size)\n             160 CALL_METHOD              0\n             162 LOAD_CONST               1 (None)\n             164 LOAD_CONST               4 (-1)\n             166 BUILD_SLICE              2\n             168 BINARY_SUBSCR\n             170 STORE_FAST              14 (input_shape)\n             172 JUMP_FORWARD             4 (to 182)\n\n 974     >>  174 LOAD_GLOBAL              6 (ValueError)\n             176 LOAD_CONST               5 ('You have to specify either input_ids or inputs_embeds')\n             178 CALL_FUNCTION            1\n             180 RAISE_VARARGS            1\n\n 976     >>  182 LOAD_FAST               14 (input_shape)\n             184 UNPACK_SEQUENCE          2\n             186 STORE_FAST              15 (batch_size)\n             188 STORE_FAST              16 (seq_length)\n\n 977         190 LOAD_FAST                1 (input_ids)\n             192 LOAD_CONST               1 (None)\n             194 IS_OP                    1\n             196 POP_JUMP_IF_FALSE      102 (to 204)\n             198 LOAD_FAST                1 (input_ids)\n             200 LOAD_ATTR                9 (device)\n             202 JUMP_FORWARD             2 (to 208)\n         >>  204 LOAD_FAST                6 (inputs_embeds)\n             206 LOAD_ATTR                9 (device)\n         >>  208 STORE_FAST              17 (device)\n\n 980         210 LOAD_FAST                9 (past_key_values)\n             212 LOAD_CONST               1 (None)\n             214 IS_OP                    1\n             216 POP_JUMP_IF_FALSE      118 (to 236)\n             218 LOAD_FAST                9 (past_key_values)\n             220 LOAD_CONST               6 (0)\n             222 BINARY_SUBSCR\n             224 LOAD_CONST               6 (0)\n             226 BINARY_SUBSCR\n             228 LOAD_ATTR               10 (shape)\n             230 LOAD_CONST               7 (2)\n             232 BINARY_SUBSCR\n             234 JUMP_FORWARD             1 (to 238)\n         >>  236 LOAD_CONST               6 (0)\n         >>  238 STORE_FAST              18 (past_key_values_length)\n\n 982         240 LOAD_FAST                2 (attention_mask)\n             242 LOAD_CONST               1 (None)\n             244 IS_OP                    0\n             246 POP_JUMP_IF_FALSE      135 (to 270)\n\n 983         248 LOAD_GLOBAL             11 (torch)\n             250 LOAD_ATTR               12 (ones)\n             252 LOAD_FAST               15 (batch_size)\n             254 LOAD_FAST               16 (seq_length)\n             256 LOAD_FAST               18 (past_key_values_length)\n             258 BINARY_ADD\n             260 BUILD_TUPLE              2\n             262 LOAD_FAST               17 (device)\n             264 LOAD_CONST               8 (('device',))\n             266 CALL_FUNCTION_KW         2\n             268 STORE_FAST               2 (attention_mask)\n\n 985     >>  270 LOAD_FAST                3 (token_type_ids)\n             272 LOAD_CONST               1 (None)\n             274 IS_OP                    0\n             276 POP_JUMP_IF_FALSE      175 (to 350)\n\n 986         278 LOAD_GLOBAL             13 (hasattr)\n             280 LOAD_FAST                0 (self)\n             282 LOAD_ATTR               14 (embeddings)\n             284 LOAD_CONST               9 ('token_type_ids')\n             286 CALL_FUNCTION            2\n             288 POP_JUMP_IF_FALSE      166 (to 332)\n\n 987         290 LOAD_FAST                0 (self)\n             292 LOAD_ATTR               14 (embeddings)\n             294 LOAD_ATTR               15 (token_type_ids)\n             296 LOAD_CONST               1 (None)\n             298 LOAD_CONST               1 (None)\n             300 BUILD_SLICE              2\n             302 LOAD_CONST               1 (None)\n             304 LOAD_FAST               16 (seq_length)\n             306 BUILD_SLICE              2\n             308 BUILD_TUPLE              2\n             310 BINARY_SUBSCR\n             312 STORE_FAST              19 (buffered_token_type_ids)\n\n 988         314 LOAD_FAST               19 (buffered_token_type_ids)\n             316 LOAD_METHOD             16 (expand)\n             318 LOAD_FAST               15 (batch_size)\n             320 LOAD_FAST               16 (seq_length)\n             322 CALL_METHOD              2\n             324 STORE_FAST              20 (buffered_token_type_ids_expanded)\n\n 989         326 LOAD_FAST               20 (buffered_token_type_ids_expanded)\n             328 STORE_FAST               3 (token_type_ids)\n             330 JUMP_FORWARD             9 (to 350)\n\n 991     >>  332 LOAD_GLOBAL             11 (torch)\n             334 LOAD_ATTR               17 (zeros)\n             336 LOAD_FAST               14 (input_shape)\n             338 LOAD_GLOBAL             11 (torch)\n             340 LOAD_ATTR               18 (long)\n             342 LOAD_FAST               17 (device)\n             344 LOAD_CONST              10 (('dtype', 'device'))\n             346 CALL_FUNCTION_KW         3\n             348 STORE_FAST               3 (token_type_ids)\n\n 995     >>  350 LOAD_FAST                0 (self)\n             352 LOAD_METHOD             19 (get_extended_attention_mask)\n             354 LOAD_FAST                2 (attention_mask)\n             356 LOAD_FAST               14 (input_shape)\n             358 CALL_METHOD              2\n             360 STORE_FAST              21 (extended_attention_mask)\n\n 999         362 LOAD_FAST                0 (self)\n             364 LOAD_ATTR                0 (config)\n             366 LOAD_ATTR                4 (is_decoder)\n             368 POP_JUMP_IF_FALSE      217 (to 434)\n             370 LOAD_FAST                7 (encoder_hidden_states)\n             372 LOAD_CONST               1 (None)\n             374 IS_OP                    1\n             376 POP_JUMP_IF_FALSE      217 (to 434)\n\n1000         378 LOAD_FAST                7 (encoder_hidden_states)\n             380 LOAD_METHOD              7 (size)\n             382 CALL_METHOD              0\n             384 UNPACK_SEQUENCE          3\n             386 STORE_FAST              22 (encoder_batch_size)\n             388 STORE_FAST              23 (encoder_sequence_length)\n             390 STORE_FAST              24 (_)\n\n1001         392 LOAD_FAST               22 (encoder_batch_size)\n             394 LOAD_FAST               23 (encoder_sequence_length)\n             396 BUILD_TUPLE              2\n             398 STORE_FAST              25 (encoder_hidden_shape)\n\n1002         400 LOAD_FAST                8 (encoder_attention_mask)\n             402 LOAD_CONST               1 (None)\n             404 IS_OP                    0\n             406 POP_JUMP_IF_FALSE      211 (to 422)\n\n1003         408 LOAD_GLOBAL             11 (torch)\n             410 LOAD_ATTR               12 (ones)\n             412 LOAD_FAST               25 (encoder_hidden_shape)\n             414 LOAD_FAST               17 (device)\n             416 LOAD_CONST               8 (('device',))\n             418 CALL_FUNCTION_KW         2\n             420 STORE_FAST               8 (encoder_attention_mask)\n\n1004     >>  422 LOAD_FAST                0 (self)\n             424 LOAD_METHOD             20 (invert_attention_mask)\n             426 LOAD_FAST                8 (encoder_attention_mask)\n             428 CALL_METHOD              1\n             430 STORE_FAST              26 (encoder_extended_attention_mask)\n             432 JUMP_FORWARD             2 (to 438)\n\n1006     >>  434 LOAD_CONST               1 (None)\n             436 STORE_FAST              26 (encoder_extended_attention_mask)\n\n1013     >>  438 LOAD_FAST                0 (self)\n             440 LOAD_METHOD             21 (get_head_mask)\n             442 LOAD_FAST                5 (head_mask)\n             444 LOAD_FAST                0 (self)\n             446 LOAD_ATTR                0 (config)\n             448 LOAD_ATTR               22 (num_hidden_layers)\n             450 CALL_METHOD              2\n             452 STORE_FAST               5 (head_mask)\n\n1015         454 LOAD_FAST                0 (self)\n             456 LOAD_ATTR               14 (embeddings)\n\n1016         458 LOAD_FAST                1 (input_ids)\n\n1017         460 LOAD_FAST                4 (position_ids)\n\n1018         462 LOAD_FAST                3 (token_type_ids)\n\n1019         464 LOAD_FAST                6 (inputs_embeds)\n\n1020         466 LOAD_FAST               18 (past_key_values_length)\n\n1015         468 LOAD_CONST              11 (('input_ids', 'position_ids', 'token_type_ids', 'inputs_embeds', 'past_key_values_length'))\n             470 CALL_FUNCTION_KW         5\n             472 STORE_FAST              27 (embedding_output)\n\n1022         474 LOAD_FAST                0 (self)\n             476 LOAD_ATTR               23 (encoder)\n\n1023         478 LOAD_FAST               27 (embedding_output)\n\n1024         480 LOAD_FAST               21 (extended_attention_mask)\n\n1025         482 LOAD_FAST                5 (head_mask)\n\n1026         484 LOAD_FAST                7 (encoder_hidden_states)\n\n1027         486 LOAD_FAST               26 (encoder_extended_attention_mask)\n\n1028         488 LOAD_FAST                9 (past_key_values)\n\n1029         490 LOAD_FAST               10 (use_cache)\n\n1030         492 LOAD_FAST               11 (output_attentions)\n\n1031         494 LOAD_FAST               12 (output_hidden_states)\n\n1032         496 LOAD_FAST               13 (return_dict)\n\n1022         498 LOAD_CONST              12 (('attention_mask', 'head_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n             500 CALL_FUNCTION_KW        10\n             502 STORE_FAST              28 (encoder_outputs)\n\n1034         504 LOAD_FAST               28 (encoder_outputs)\n             506 LOAD_CONST               6 (0)\n             508 BINARY_SUBSCR\n             510 STORE_FAST              29 (sequence_output)\n\n1035         512 LOAD_FAST                0 (self)\n             514 LOAD_ATTR               24 (pooler)\n             516 LOAD_CONST               1 (None)\n             518 IS_OP                    1\n             520 EXTENDED_ARG             1\n             522 POP_JUMP_IF_FALSE      267 (to 534)\n             524 LOAD_FAST                0 (self)\n             526 LOAD_METHOD             24 (pooler)\n             528 LOAD_FAST               29 (sequence_output)\n             530 CALL_METHOD              1\n             532 JUMP_FORWARD             1 (to 536)\n         >>  534 LOAD_CONST               1 (None)\n         >>  536 STORE_FAST              30 (pooled_output)\n\n1037         538 LOAD_FAST               13 (return_dict)\n             540 EXTENDED_ARG             1\n             542 POP_JUMP_IF_TRUE       282 (to 564)\n\n1038         544 LOAD_FAST               29 (sequence_output)\n             546 LOAD_FAST               30 (pooled_output)\n             548 BUILD_TUPLE              2\n             550 LOAD_FAST               28 (encoder_outputs)\n             552 LOAD_CONST              13 (1)\n             554 LOAD_CONST               1 (None)\n             556 BUILD_SLICE              2\n             558 BINARY_SUBSCR\n             560 BINARY_ADD\n             562 RETURN_VALUE\n\n1040     >>  564 LOAD_GLOBAL             25 (BaseModelOutputWithPoolingAndCrossAttentions)\n\n1041         566 LOAD_FAST               29 (sequence_output)\n\n1042         568 LOAD_FAST               30 (pooled_output)\n\n1043         570 LOAD_FAST               28 (encoder_outputs)\n             572 LOAD_ATTR               26 (past_key_values)\n\n1044         574 LOAD_FAST               28 (encoder_outputs)\n             576 LOAD_ATTR               27 (hidden_states)\n\n1045         578 LOAD_FAST               28 (encoder_outputs)\n             580 LOAD_ATTR               28 (attentions)\n\n1046         582 LOAD_FAST               28 (encoder_outputs)\n             584 LOAD_ATTR               29 (cross_attentions)\n\n1040         586 LOAD_CONST              14 (('last_hidden_state', 'pooler_output', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n             588 CALL_FUNCTION_KW         6\n             590 RETURN_VALUE\n\n", "ORIGINAL BYTECODE warn_if_padding_and_no_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/modeling_utils.py line 3482 \n3486           0 LOAD_FAST                2 (attention_mask)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_TRUE        10 (to 20)\n               8 LOAD_FAST                0 (self)\n              10 LOAD_ATTR                0 (config)\n              12 LOAD_ATTR                1 (pad_token_id)\n              14 LOAD_CONST               1 (None)\n              16 IS_OP                    0\n              18 POP_JUMP_IF_FALSE       12 (to 24)\n\n3487     >>   20 LOAD_CONST               1 (None)\n              22 RETURN_VALUE\n\n3490     >>   24 LOAD_FAST                0 (self)\n              26 LOAD_ATTR                0 (config)\n              28 LOAD_ATTR                1 (pad_token_id)\n              30 LOAD_FAST                1 (input_ids)\n              32 LOAD_CONST               1 (None)\n              34 LOAD_CONST               1 (None)\n              36 BUILD_SLICE              2\n              38 LOAD_CONST               2 (-1)\n              40 LOAD_CONST               3 (0)\n              42 BUILD_LIST               2\n              44 BUILD_TUPLE              2\n              46 BINARY_SUBSCR\n              48 CONTAINS_OP              0\n              50 POP_JUMP_IF_FALSE      102 (to 204)\n\n3492          52 LOAD_CONST               4 ('We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.')\n\n3491          54 STORE_FAST               3 (warn_string)\n\n3500          56 LOAD_FAST                0 (self)\n              58 LOAD_ATTR                0 (config)\n              60 LOAD_ATTR                2 (bos_token_id)\n              62 LOAD_CONST               1 (None)\n              64 IS_OP                    1\n              66 POP_JUMP_IF_FALSE       42 (to 84)\n              68 LOAD_FAST                0 (self)\n              70 LOAD_ATTR                0 (config)\n              72 LOAD_ATTR                2 (bos_token_id)\n              74 LOAD_FAST                0 (self)\n              76 LOAD_ATTR                0 (config)\n              78 LOAD_ATTR                1 (pad_token_id)\n              80 COMPARE_OP               2 (==)\n              82 POP_JUMP_IF_TRUE        70 (to 140)\n\n3501     >>   84 LOAD_FAST                0 (self)\n              86 LOAD_ATTR                0 (config)\n              88 LOAD_ATTR                3 (eos_token_id)\n              90 LOAD_CONST               1 (None)\n              92 IS_OP                    1\n              94 POP_JUMP_IF_FALSE       56 (to 112)\n              96 LOAD_FAST                0 (self)\n              98 LOAD_ATTR                0 (config)\n             100 LOAD_ATTR                3 (eos_token_id)\n             102 LOAD_FAST                0 (self)\n             104 LOAD_ATTR                0 (config)\n             106 LOAD_ATTR                1 (pad_token_id)\n             108 COMPARE_OP               2 (==)\n             110 POP_JUMP_IF_TRUE        70 (to 140)\n\n3502     >>  112 LOAD_FAST                0 (self)\n             114 LOAD_ATTR                0 (config)\n             116 LOAD_ATTR                4 (sep_token_id)\n             118 LOAD_CONST               1 (None)\n             120 IS_OP                    1\n             122 POP_JUMP_IF_FALSE       95 (to 190)\n             124 LOAD_FAST                0 (self)\n             126 LOAD_ATTR                0 (config)\n             128 LOAD_ATTR                4 (sep_token_id)\n             130 LOAD_FAST                0 (self)\n             132 LOAD_ATTR                0 (config)\n             134 LOAD_ATTR                1 (pad_token_id)\n             136 COMPARE_OP               2 (==)\n             138 POP_JUMP_IF_FALSE       95 (to 190)\n\n3504     >>  140 LOAD_FAST                3 (warn_string)\n\n3505         142 LOAD_CONST               5 ('\\nYou may ignore this warning if your `pad_token_id` (')\n             144 LOAD_FAST                0 (self)\n             146 LOAD_ATTR                0 (config)\n             148 LOAD_ATTR                1 (pad_token_id)\n             150 FORMAT_VALUE             0\n             152 LOAD_CONST               6 (') is identical to the `bos_token_id` (')\n\n3506         154 LOAD_FAST                0 (self)\n             156 LOAD_ATTR                0 (config)\n             158 LOAD_ATTR                2 (bos_token_id)\n\n3505         160 FORMAT_VALUE             0\n             162 LOAD_CONST               7 ('), `eos_token_id` (')\n\n3506         164 LOAD_FAST                0 (self)\n             166 LOAD_ATTR                0 (config)\n             168 LOAD_ATTR                3 (eos_token_id)\n\n3505         170 FORMAT_VALUE             0\n             172 LOAD_CONST               8 ('), or the `sep_token_id` (')\n\n3507         174 LOAD_FAST                0 (self)\n             176 LOAD_ATTR                0 (config)\n             178 LOAD_ATTR                4 (sep_token_id)\n\n3505         180 FORMAT_VALUE             0\n             182 LOAD_CONST               9 ('), and your input is not padded.')\n             184 BUILD_STRING             9\n\n3504         186 INPLACE_ADD\n             188 STORE_FAST               3 (warn_string)\n\n3510     >>  190 LOAD_GLOBAL              5 (logger)\n             192 LOAD_METHOD              6 (warning_once)\n             194 LOAD_FAST                3 (warn_string)\n             196 CALL_METHOD              1\n             198 POP_TOP\n             200 LOAD_CONST               1 (None)\n             202 RETURN_VALUE\n\n3490     >>  204 LOAD_CONST               1 (None)\n             206 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 970 \n 970           0 LOAD_FAST                0 (___stack0)\n               2 JUMP_ABSOLUTE           74 (to 148)\n               4 LOAD_FAST               12 (output_attentions)\n               6 LOAD_CONST               1 (None)\n               8 IS_OP                    1\n              10 POP_JUMP_IF_FALSE        8 (to 16)\n              12 LOAD_FAST               12 (output_attentions)\n              14 JUMP_FORWARD             3 (to 22)\n         >>   16 LOAD_FAST                1 (self)\n              18 LOAD_ATTR                0 (config)\n              20 LOAD_ATTR                1 (output_attentions)\n         >>   22 STORE_FAST              12 (output_attentions)\n              24 LOAD_FAST               13 (output_hidden_states)\n              26 LOAD_CONST               1 (None)\n              28 IS_OP                    1\n              30 POP_JUMP_IF_FALSE       18 (to 36)\n              32 LOAD_FAST               13 (output_hidden_states)\n              34 JUMP_FORWARD             3 (to 42)\n         >>   36 LOAD_FAST                1 (self)\n              38 LOAD_ATTR                0 (config)\n              40 LOAD_ATTR                2 (output_hidden_states)\n         >>   42 STORE_FAST              13 (output_hidden_states)\n              44 LOAD_FAST               14 (return_dict)\n              46 LOAD_CONST               1 (None)\n              48 IS_OP                    1\n              50 POP_JUMP_IF_FALSE       28 (to 56)\n              52 LOAD_FAST               14 (return_dict)\n              54 JUMP_FORWARD             3 (to 62)\n         >>   56 LOAD_FAST                1 (self)\n              58 LOAD_ATTR                0 (config)\n              60 LOAD_ATTR                3 (use_return_dict)\n         >>   62 STORE_FAST              14 (return_dict)\n              64 LOAD_FAST                1 (self)\n              66 LOAD_ATTR                0 (config)\n              68 LOAD_ATTR                4 (is_decoder)\n              70 POP_JUMP_IF_FALSE       47 (to 94)\n              72 LOAD_FAST               11 (use_cache)\n              74 LOAD_CONST               1 (None)\n              76 IS_OP                    1\n              78 POP_JUMP_IF_FALSE       42 (to 84)\n              80 LOAD_FAST               11 (use_cache)\n              82 JUMP_FORWARD             3 (to 90)\n         >>   84 LOAD_FAST                1 (self)\n              86 LOAD_ATTR                0 (config)\n              88 LOAD_ATTR                5 (use_cache)\n         >>   90 STORE_FAST              11 (use_cache)\n              92 JUMP_FORWARD             2 (to 98)\n         >>   94 LOAD_CONST               2 (False)\n              96 STORE_FAST              11 (use_cache)\n         >>   98 LOAD_FAST                2 (input_ids)\n             100 LOAD_CONST               1 (None)\n             102 IS_OP                    1\n             104 POP_JUMP_IF_FALSE       61 (to 122)\n             106 LOAD_FAST                7 (inputs_embeds)\n             108 LOAD_CONST               1 (None)\n             110 IS_OP                    1\n             112 POP_JUMP_IF_FALSE       61 (to 122)\n             114 LOAD_GLOBAL              6 (ValueError)\n             116 LOAD_CONST               3 ('You cannot specify both input_ids and inputs_embeds at the same time')\n             118 CALL_FUNCTION            1\n             120 RAISE_VARARGS            1\n         >>  122 LOAD_FAST                2 (input_ids)\n             124 LOAD_CONST               1 (None)\n             126 IS_OP                    1\n             128 POP_JUMP_IF_FALSE       76 (to 152)\n             130 LOAD_FAST                2 (input_ids)\n             132 LOAD_ATTR                7 (size)\n             134 CALL_FUNCTION            0\n             136 STORE_FAST              15 (input_shape)\n             138 LOAD_FAST                1 (self)\n             140 LOAD_ATTR                8 (warn_if_padding_and_no_attention_mask)\n             142 LOAD_FAST                2 (input_ids)\n             144 LOAD_FAST                3 (attention_mask)\n             146 CALL_FUNCTION            2\n         >>  148 POP_TOP\n             150 JUMP_FORWARD            17 (to 186)\n\n 971     >>  152 LOAD_FAST                7 (inputs_embeds)\n             154 LOAD_CONST               1 (None)\n             156 IS_OP                    1\n             158 POP_JUMP_IF_FALSE       89 (to 178)\n\n 972         160 LOAD_FAST                7 (inputs_embeds)\n             162 LOAD_ATTR                7 (size)\n             164 CALL_FUNCTION            0\n             166 LOAD_CONST               1 (None)\n             168 LOAD_CONST               4 (-1)\n             170 BUILD_SLICE              2\n             172 BINARY_SUBSCR\n             174 STORE_FAST              15 (input_shape)\n             176 JUMP_FORWARD             4 (to 186)\n\n 974     >>  178 LOAD_GLOBAL              6 (ValueError)\n             180 LOAD_CONST               5 ('You have to specify either input_ids or inputs_embeds')\n             182 CALL_FUNCTION            1\n             184 RAISE_VARARGS            1\n\n 976     >>  186 LOAD_FAST               15 (input_shape)\n             188 UNPACK_SEQUENCE          2\n             190 STORE_FAST              16 (batch_size)\n             192 STORE_FAST              17 (seq_length)\n\n 977         194 LOAD_FAST                2 (input_ids)\n             196 LOAD_CONST               1 (None)\n             198 IS_OP                    1\n             200 POP_JUMP_IF_FALSE      104 (to 208)\n             202 LOAD_FAST                2 (input_ids)\n             204 LOAD_ATTR                9 (device)\n             206 JUMP_FORWARD             2 (to 212)\n         >>  208 LOAD_FAST                7 (inputs_embeds)\n             210 LOAD_ATTR                9 (device)\n         >>  212 STORE_FAST              18 (device)\n\n 980         214 LOAD_FAST               10 (past_key_values)\n             216 LOAD_CONST               1 (None)\n             218 IS_OP                    1\n             220 POP_JUMP_IF_FALSE      120 (to 240)\n             222 LOAD_FAST               10 (past_key_values)\n             224 LOAD_CONST               6 (0)\n             226 BINARY_SUBSCR\n             228 LOAD_CONST               6 (0)\n             230 BINARY_SUBSCR\n             232 LOAD_ATTR               10 (shape)\n             234 LOAD_CONST               7 (2)\n             236 BINARY_SUBSCR\n             238 JUMP_FORWARD             1 (to 242)\n         >>  240 LOAD_CONST               6 (0)\n         >>  242 STORE_FAST              19 (past_key_values_length)\n\n 982         244 LOAD_FAST                3 (attention_mask)\n             246 LOAD_CONST               1 (None)\n             248 IS_OP                    0\n             250 POP_JUMP_IF_FALSE      137 (to 274)\n\n 983         252 LOAD_GLOBAL             11 (torch)\n             254 LOAD_ATTR               12 (ones)\n             256 LOAD_FAST               16 (batch_size)\n             258 LOAD_FAST               17 (seq_length)\n             260 LOAD_FAST               19 (past_key_values_length)\n             262 BINARY_ADD\n             264 BUILD_TUPLE              2\n             266 LOAD_FAST               18 (device)\n             268 LOAD_CONST               8 (('device',))\n             270 CALL_FUNCTION_KW         2\n             272 STORE_FAST               3 (attention_mask)\n\n 985     >>  274 LOAD_FAST                4 (token_type_ids)\n             276 LOAD_CONST               1 (None)\n             278 IS_OP                    0\n             280 POP_JUMP_IF_FALSE      177 (to 354)\n\n 986         282 LOAD_GLOBAL             13 (hasattr)\n             284 LOAD_FAST                1 (self)\n             286 LOAD_ATTR               14 (embeddings)\n             288 LOAD_CONST               9 ('token_type_ids')\n             290 CALL_FUNCTION            2\n             292 POP_JUMP_IF_FALSE      168 (to 336)\n\n 987         294 LOAD_FAST                1 (self)\n             296 LOAD_ATTR               14 (embeddings)\n             298 LOAD_ATTR               15 (token_type_ids)\n             300 LOAD_CONST               1 (None)\n             302 LOAD_CONST               1 (None)\n             304 BUILD_SLICE              2\n             306 LOAD_CONST               1 (None)\n             308 LOAD_FAST               17 (seq_length)\n             310 BUILD_SLICE              2\n             312 BUILD_TUPLE              2\n             314 BINARY_SUBSCR\n             316 STORE_FAST              20 (buffered_token_type_ids)\n\n 988         318 LOAD_FAST               20 (buffered_token_type_ids)\n             320 LOAD_ATTR               16 (expand)\n             322 LOAD_FAST               16 (batch_size)\n             324 LOAD_FAST               17 (seq_length)\n             326 CALL_FUNCTION            2\n             328 STORE_FAST              21 (buffered_token_type_ids_expanded)\n\n 989         330 LOAD_FAST               21 (buffered_token_type_ids_expanded)\n             332 STORE_FAST               4 (token_type_ids)\n             334 JUMP_FORWARD             9 (to 354)\n\n 991     >>  336 LOAD_GLOBAL             11 (torch)\n             338 LOAD_ATTR               17 (zeros)\n             340 LOAD_FAST               15 (input_shape)\n             342 LOAD_GLOBAL             11 (torch)\n             344 LOAD_ATTR               18 (long)\n             346 LOAD_FAST               18 (device)\n             348 LOAD_CONST              10 (('dtype', 'device'))\n             350 CALL_FUNCTION_KW         3\n             352 STORE_FAST               4 (token_type_ids)\n\n 995     >>  354 LOAD_FAST                1 (self)\n             356 LOAD_ATTR               19 (get_extended_attention_mask)\n             358 LOAD_FAST                3 (attention_mask)\n             360 LOAD_FAST               15 (input_shape)\n             362 CALL_FUNCTION            2\n             364 STORE_FAST              22 (extended_attention_mask)\n\n 999         366 LOAD_FAST                1 (self)\n             368 LOAD_ATTR                0 (config)\n             370 LOAD_ATTR                4 (is_decoder)\n             372 POP_JUMP_IF_FALSE      219 (to 438)\n             374 LOAD_FAST                8 (encoder_hidden_states)\n             376 LOAD_CONST               1 (None)\n             378 IS_OP                    1\n             380 POP_JUMP_IF_FALSE      219 (to 438)\n\n1000         382 LOAD_FAST                8 (encoder_hidden_states)\n             384 LOAD_ATTR                7 (size)\n             386 CALL_FUNCTION            0\n             388 UNPACK_SEQUENCE          3\n             390 STORE_FAST              23 (encoder_batch_size)\n             392 STORE_FAST              24 (encoder_sequence_length)\n             394 STORE_FAST              25 (_)\n\n1001         396 LOAD_FAST               23 (encoder_batch_size)\n             398 LOAD_FAST               24 (encoder_sequence_length)\n             400 BUILD_TUPLE              2\n             402 STORE_FAST              26 (encoder_hidden_shape)\n\n1002         404 LOAD_FAST                9 (encoder_attention_mask)\n             406 LOAD_CONST               1 (None)\n             408 IS_OP                    0\n             410 POP_JUMP_IF_FALSE      213 (to 426)\n\n1003         412 LOAD_GLOBAL             11 (torch)\n             414 LOAD_ATTR               12 (ones)\n             416 LOAD_FAST               26 (encoder_hidden_shape)\n             418 LOAD_FAST               18 (device)\n             420 LOAD_CONST               8 (('device',))\n             422 CALL_FUNCTION_KW         2\n             424 STORE_FAST               9 (encoder_attention_mask)\n\n1004     >>  426 LOAD_FAST                1 (self)\n             428 LOAD_ATTR               20 (invert_attention_mask)\n             430 LOAD_FAST                9 (encoder_attention_mask)\n             432 CALL_FUNCTION            1\n             434 STORE_FAST              27 (encoder_extended_attention_mask)\n             436 JUMP_FORWARD             2 (to 442)\n\n1006     >>  438 LOAD_CONST               1 (None)\n             440 STORE_FAST              27 (encoder_extended_attention_mask)\n\n1013     >>  442 LOAD_FAST                1 (self)\n             444 LOAD_ATTR               21 (get_head_mask)\n             446 LOAD_FAST                6 (head_mask)\n             448 LOAD_FAST                1 (self)\n             450 LOAD_ATTR                0 (config)\n             452 LOAD_ATTR               22 (num_hidden_layers)\n             454 CALL_FUNCTION            2\n             456 STORE_FAST               6 (head_mask)\n\n1015         458 LOAD_FAST                1 (self)\n             460 LOAD_ATTR               14 (embeddings)\n\n1016         462 LOAD_FAST                2 (input_ids)\n\n1017         464 LOAD_FAST                5 (position_ids)\n\n1018         466 LOAD_FAST                4 (token_type_ids)\n\n1019         468 LOAD_FAST                7 (inputs_embeds)\n\n1020         470 LOAD_FAST               19 (past_key_values_length)\n\n1015         472 LOAD_CONST              11 (('input_ids', 'position_ids', 'token_type_ids', 'inputs_embeds', 'past_key_values_length'))\n             474 CALL_FUNCTION_KW         5\n             476 STORE_FAST              28 (embedding_output)\n\n1022         478 LOAD_FAST                1 (self)\n             480 LOAD_ATTR               23 (encoder)\n\n1023         482 LOAD_FAST               28 (embedding_output)\n\n1024         484 LOAD_FAST               22 (extended_attention_mask)\n\n1025         486 LOAD_FAST                6 (head_mask)\n\n1026         488 LOAD_FAST                8 (encoder_hidden_states)\n\n1027         490 LOAD_FAST               27 (encoder_extended_attention_mask)\n\n1028         492 LOAD_FAST               10 (past_key_values)\n\n1029         494 LOAD_FAST               11 (use_cache)\n\n1030         496 LOAD_FAST               12 (output_attentions)\n\n1031         498 LOAD_FAST               13 (output_hidden_states)\n\n1032         500 LOAD_FAST               14 (return_dict)\n\n1022         502 LOAD_CONST              12 (('attention_mask', 'head_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n             504 CALL_FUNCTION_KW        10\n             506 STORE_FAST              29 (encoder_outputs)\n\n1034         508 LOAD_FAST               29 (encoder_outputs)\n             510 LOAD_CONST               6 (0)\n             512 BINARY_SUBSCR\n             514 STORE_FAST              30 (sequence_output)\n\n1035         516 LOAD_FAST                1 (self)\n             518 LOAD_ATTR               24 (pooler)\n             520 LOAD_CONST               1 (None)\n             522 IS_OP                    1\n             524 EXTENDED_ARG             1\n             526 POP_JUMP_IF_FALSE      269 (to 538)\n             528 LOAD_FAST                1 (self)\n             530 LOAD_ATTR               24 (pooler)\n             532 LOAD_FAST               30 (sequence_output)\n             534 CALL_FUNCTION            1\n             536 JUMP_FORWARD             1 (to 540)\n         >>  538 LOAD_CONST               1 (None)\n         >>  540 STORE_FAST              31 (pooled_output)\n\n1037         542 LOAD_FAST               14 (return_dict)\n             544 EXTENDED_ARG             1\n             546 POP_JUMP_IF_TRUE       284 (to 568)\n\n1038         548 LOAD_FAST               30 (sequence_output)\n             550 LOAD_FAST               31 (pooled_output)\n             552 BUILD_TUPLE              2\n             554 LOAD_FAST               29 (encoder_outputs)\n             556 LOAD_CONST              13 (1)\n             558 LOAD_CONST               1 (None)\n             560 BUILD_SLICE              2\n             562 BINARY_SUBSCR\n             564 BINARY_ADD\n             566 RETURN_VALUE\n\n1040     >>  568 LOAD_GLOBAL             25 (BaseModelOutputWithPoolingAndCrossAttentions)\n\n1041         570 LOAD_FAST               30 (sequence_output)\n\n1042         572 LOAD_FAST               31 (pooled_output)\n\n1043         574 LOAD_FAST               29 (encoder_outputs)\n             576 LOAD_ATTR               26 (past_key_values)\n\n1044         578 LOAD_FAST               29 (encoder_outputs)\n             580 LOAD_ATTR               27 (hidden_states)\n\n1045         582 LOAD_FAST               29 (encoder_outputs)\n             584 LOAD_ATTR               28 (attentions)\n\n1046         586 LOAD_FAST               29 (encoder_outputs)\n             588 LOAD_ATTR               29 (cross_attentions)\n\n1040         590 LOAD_CONST              14 (('last_hidden_state', 'pooler_output', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n             592 CALL_FUNCTION_KW         6\n             594 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py line 1844 \n1844           0 LOAD_FAST                0 (___stack0)\n               2 JUMP_ABSOLUTE           25 (to 50)\n               4 LOAD_FAST                4 (return_dict)\n               6 LOAD_CONST               1 (None)\n               8 IS_OP                    1\n              10 POP_JUMP_IF_FALSE        8 (to 16)\n              12 LOAD_FAST                4 (return_dict)\n              14 JUMP_FORWARD             3 (to 22)\n         >>   16 LOAD_FAST                1 (self)\n              18 LOAD_ATTR                0 (config)\n              20 LOAD_ATTR                1 (use_return_dict)\n         >>   22 STORE_FAST               4 (return_dict)\n              24 LOAD_FAST                1 (self)\n              26 LOAD_ATTR                2 (bert)\n              28 LOAD_FAST                5 (input_ids)\n              30 LOAD_FAST                6 (attention_mask)\n              32 LOAD_FAST                7 (token_type_ids)\n              34 LOAD_FAST                8 (position_ids)\n              36 LOAD_FAST                9 (head_mask)\n              38 LOAD_FAST               10 (inputs_embeds)\n              40 LOAD_FAST               11 (output_attentions)\n              42 LOAD_FAST               12 (output_hidden_states)\n              44 LOAD_FAST                4 (return_dict)\n              46 LOAD_CONST               2 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              48 CALL_FUNCTION_KW         9\n         >>   50 STORE_FAST              13 (outputs)\n\n1856          52 LOAD_FAST               13 (outputs)\n              54 LOAD_CONST               3 (0)\n              56 BINARY_SUBSCR\n              58 STORE_FAST              14 (sequence_output)\n\n1858          60 LOAD_FAST                1 (self)\n              62 LOAD_ATTR                3 (qa_outputs)\n              64 LOAD_FAST               14 (sequence_output)\n              66 CALL_FUNCTION            1\n              68 STORE_FAST              15 (logits)\n\n1859          70 LOAD_FAST               15 (logits)\n              72 LOAD_ATTR                4 (split)\n              74 LOAD_CONST               4 (1)\n              76 LOAD_CONST               5 (-1)\n              78 LOAD_CONST               6 (('dim',))\n              80 CALL_FUNCTION_KW         2\n              82 UNPACK_SEQUENCE          2\n              84 STORE_FAST              16 (start_logits)\n              86 STORE_FAST              17 (end_logits)\n\n1860          88 LOAD_FAST               16 (start_logits)\n              90 LOAD_ATTR                5 (squeeze)\n              92 LOAD_CONST               5 (-1)\n              94 CALL_FUNCTION            1\n              96 LOAD_ATTR                6 (contiguous)\n              98 CALL_FUNCTION            0\n             100 STORE_FAST              16 (start_logits)\n\n1861         102 LOAD_FAST               17 (end_logits)\n             104 LOAD_ATTR                5 (squeeze)\n             106 LOAD_CONST               5 (-1)\n             108 CALL_FUNCTION            1\n             110 LOAD_ATTR                6 (contiguous)\n             112 CALL_FUNCTION            0\n             114 STORE_FAST              17 (end_logits)\n\n1863         116 LOAD_CONST               1 (None)\n             118 STORE_FAST              18 (total_loss)\n\n1864         120 LOAD_FAST                2 (start_positions)\n             122 LOAD_CONST               1 (None)\n             124 IS_OP                    1\n             126 POP_JUMP_IF_FALSE      132 (to 264)\n             128 LOAD_FAST                3 (end_positions)\n             130 LOAD_CONST               1 (None)\n             132 IS_OP                    1\n             134 POP_JUMP_IF_FALSE      132 (to 264)\n\n1866         136 LOAD_GLOBAL              7 (len)\n             138 LOAD_FAST                2 (start_positions)\n             140 LOAD_ATTR                8 (size)\n             142 CALL_FUNCTION            0\n             144 CALL_FUNCTION            1\n             146 LOAD_CONST               4 (1)\n             148 COMPARE_OP               4 (>)\n             150 POP_JUMP_IF_FALSE       81 (to 162)\n\n1867         152 LOAD_FAST                2 (start_positions)\n             154 LOAD_ATTR                5 (squeeze)\n             156 LOAD_CONST               5 (-1)\n             158 CALL_FUNCTION            1\n             160 STORE_FAST               2 (start_positions)\n\n1868     >>  162 LOAD_GLOBAL              7 (len)\n             164 LOAD_FAST                3 (end_positions)\n             166 LOAD_ATTR                8 (size)\n             168 CALL_FUNCTION            0\n             170 CALL_FUNCTION            1\n             172 LOAD_CONST               4 (1)\n             174 COMPARE_OP               4 (>)\n             176 POP_JUMP_IF_FALSE       94 (to 188)\n\n1869         178 LOAD_FAST                3 (end_positions)\n             180 LOAD_ATTR                5 (squeeze)\n             182 LOAD_CONST               5 (-1)\n             184 CALL_FUNCTION            1\n             186 STORE_FAST               3 (end_positions)\n\n1871     >>  188 LOAD_FAST               16 (start_logits)\n             190 LOAD_ATTR                8 (size)\n             192 LOAD_CONST               4 (1)\n             194 CALL_FUNCTION            1\n             196 STORE_FAST              19 (ignored_index)\n\n1872         198 LOAD_FAST                2 (start_positions)\n             200 LOAD_ATTR                9 (clamp)\n             202 LOAD_CONST               3 (0)\n             204 LOAD_FAST               19 (ignored_index)\n             206 CALL_FUNCTION            2\n             208 STORE_FAST               2 (start_positions)\n\n1873         210 LOAD_FAST                3 (end_positions)\n             212 LOAD_ATTR                9 (clamp)\n             214 LOAD_CONST               3 (0)\n             216 LOAD_FAST               19 (ignored_index)\n             218 CALL_FUNCTION            2\n             220 STORE_FAST               3 (end_positions)\n\n1875         222 LOAD_GLOBAL             10 (CrossEntropyLoss)\n             224 LOAD_FAST               19 (ignored_index)\n             226 LOAD_CONST               7 (('ignore_index',))\n             228 CALL_FUNCTION_KW         1\n             230 STORE_FAST              20 (loss_fct)\n\n1876         232 LOAD_FAST               20 (loss_fct)\n             234 LOAD_FAST               16 (start_logits)\n             236 LOAD_FAST                2 (start_positions)\n             238 CALL_FUNCTION            2\n             240 STORE_FAST              21 (start_loss)\n\n1877         242 LOAD_FAST               20 (loss_fct)\n             244 LOAD_FAST               17 (end_logits)\n             246 LOAD_FAST                3 (end_positions)\n             248 CALL_FUNCTION            2\n             250 STORE_FAST              22 (end_loss)\n\n1878         252 LOAD_FAST               21 (start_loss)\n             254 LOAD_FAST               22 (end_loss)\n             256 BINARY_ADD\n             258 LOAD_CONST               8 (2)\n             260 BINARY_TRUE_DIVIDE\n             262 STORE_FAST              18 (total_loss)\n\n1880     >>  264 LOAD_FAST                4 (return_dict)\n             266 POP_JUMP_IF_TRUE       155 (to 310)\n\n1881         268 LOAD_FAST               16 (start_logits)\n             270 LOAD_FAST               17 (end_logits)\n             272 BUILD_TUPLE              2\n             274 LOAD_FAST               13 (outputs)\n             276 LOAD_CONST               8 (2)\n             278 LOAD_CONST               1 (None)\n             280 BUILD_SLICE              2\n             282 BINARY_SUBSCR\n             284 BINARY_ADD\n             286 STORE_FAST              23 (output)\n\n1882         288 LOAD_FAST               18 (total_loss)\n             290 LOAD_CONST               1 (None)\n             292 IS_OP                    1\n             294 POP_JUMP_IF_FALSE      153 (to 306)\n             296 LOAD_FAST               18 (total_loss)\n             298 BUILD_TUPLE              1\n             300 LOAD_FAST               23 (output)\n             302 BINARY_ADD\n             304 RETURN_VALUE\n         >>  306 LOAD_FAST               23 (output)\n             308 RETURN_VALUE\n\n1884     >>  310 LOAD_GLOBAL             11 (QuestionAnsweringModelOutput)\n\n1885         312 LOAD_FAST               18 (total_loss)\n\n1886         314 LOAD_FAST               16 (start_logits)\n\n1887         316 LOAD_FAST               17 (end_logits)\n\n1888         318 LOAD_FAST               13 (outputs)\n             320 LOAD_ATTR               12 (hidden_states)\n\n1889         322 LOAD_FAST               13 (outputs)\n             324 LOAD_ATTR               13 (attentions)\n\n1884         326 LOAD_CONST               9 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n             328 CALL_FUNCTION_KW         5\n             330 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py line 954 \n 986           0 LOAD_FAST               12 (return_dict)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_FALSE        6 (to 12)\n               8 LOAD_FAST               12 (return_dict)\n              10 JUMP_FORWARD             3 (to 18)\n         >>   12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (use_return_dict)\n         >>   18 STORE_FAST              12 (return_dict)\n\n 988          20 LOAD_FAST                0 (self)\n              22 LOAD_ATTR                2 (roberta)\n\n 989          24 LOAD_FAST                1 (input_ids)\n\n 990          26 LOAD_FAST                2 (attention_mask)\n\n 991          28 LOAD_FAST                3 (token_type_ids)\n\n 992          30 LOAD_FAST                4 (position_ids)\n\n 993          32 LOAD_FAST                5 (head_mask)\n\n 994          34 LOAD_FAST                6 (inputs_embeds)\n\n 995          36 LOAD_FAST                7 (encoder_hidden_states)\n\n 996          38 LOAD_FAST                8 (encoder_attention_mask)\n\n 997          40 LOAD_FAST               10 (output_attentions)\n\n 998          42 LOAD_FAST               11 (output_hidden_states)\n\n 999          44 LOAD_FAST               12 (return_dict)\n\n 988          46 LOAD_CONST               2 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'encoder_hidden_states', 'encoder_attention_mask', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              48 CALL_FUNCTION_KW        11\n              50 STORE_FAST              13 (outputs)\n\n1001          52 LOAD_FAST               13 (outputs)\n              54 LOAD_CONST               3 (0)\n              56 BINARY_SUBSCR\n              58 STORE_FAST              14 (sequence_output)\n\n1002          60 LOAD_FAST                0 (self)\n              62 LOAD_METHOD              3 (lm_head)\n              64 LOAD_FAST               14 (sequence_output)\n              66 CALL_METHOD              1\n              68 STORE_FAST              15 (prediction_scores)\n\n1004          70 LOAD_CONST               1 (None)\n              72 STORE_FAST              16 (masked_lm_loss)\n\n1005          74 LOAD_FAST                9 (labels)\n              76 LOAD_CONST               1 (None)\n              78 IS_OP                    1\n              80 POP_JUMP_IF_FALSE       64 (to 128)\n\n1007          82 LOAD_FAST                9 (labels)\n              84 LOAD_METHOD              4 (to)\n              86 LOAD_FAST               15 (prediction_scores)\n              88 LOAD_ATTR                5 (device)\n              90 CALL_METHOD              1\n              92 STORE_FAST               9 (labels)\n\n1008          94 LOAD_GLOBAL              6 (CrossEntropyLoss)\n              96 CALL_FUNCTION            0\n              98 STORE_FAST              17 (loss_fct)\n\n1009         100 LOAD_FAST               17 (loss_fct)\n             102 LOAD_FAST               15 (prediction_scores)\n             104 LOAD_METHOD              7 (view)\n             106 LOAD_CONST               4 (-1)\n             108 LOAD_FAST                0 (self)\n             110 LOAD_ATTR                0 (config)\n             112 LOAD_ATTR                8 (vocab_size)\n             114 CALL_METHOD              2\n             116 LOAD_FAST                9 (labels)\n             118 LOAD_METHOD              7 (view)\n             120 LOAD_CONST               4 (-1)\n             122 CALL_METHOD              1\n             124 CALL_FUNCTION            2\n             126 STORE_FAST              16 (masked_lm_loss)\n\n1011     >>  128 LOAD_FAST               12 (return_dict)\n             130 POP_JUMP_IF_TRUE        86 (to 172)\n\n1012         132 LOAD_FAST               15 (prediction_scores)\n             134 BUILD_TUPLE              1\n             136 LOAD_FAST               13 (outputs)\n             138 LOAD_CONST               5 (2)\n             140 LOAD_CONST               1 (None)\n             142 BUILD_SLICE              2\n             144 BINARY_SUBSCR\n             146 BINARY_ADD\n             148 STORE_FAST              18 (output)\n\n1013         150 LOAD_FAST               16 (masked_lm_loss)\n             152 LOAD_CONST               1 (None)\n             154 IS_OP                    1\n             156 POP_JUMP_IF_FALSE       84 (to 168)\n             158 LOAD_FAST               16 (masked_lm_loss)\n             160 BUILD_TUPLE              1\n             162 LOAD_FAST               18 (output)\n             164 BINARY_ADD\n             166 RETURN_VALUE\n         >>  168 LOAD_FAST               18 (output)\n             170 RETURN_VALUE\n\n1015     >>  172 LOAD_GLOBAL              9 (MaskedLMOutput)\n\n1016         174 LOAD_FAST               16 (masked_lm_loss)\n\n1017         176 LOAD_FAST               15 (prediction_scores)\n\n1018         178 LOAD_FAST               13 (outputs)\n             180 LOAD_ATTR               10 (hidden_states)\n\n1019         182 LOAD_FAST               13 (outputs)\n             184 LOAD_ATTR               11 (attentions)\n\n1015         186 LOAD_CONST               6 (('loss', 'logits', 'hidden_states', 'attentions'))\n             188 CALL_FUNCTION_KW         4\n             190 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py line 787 \n830           0 LOAD_FAST               11 (output_attentions)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    1\n              6 POP_JUMP_IF_FALSE        6 (to 12)\n              8 LOAD_FAST               11 (output_attentions)\n             10 JUMP_FORWARD             3 (to 18)\n        >>   12 LOAD_FAST                0 (self)\n             14 LOAD_ATTR                0 (config)\n             16 LOAD_ATTR                1 (output_attentions)\n        >>   18 STORE_FAST              11 (output_attentions)\n\n832          20 LOAD_FAST               12 (output_hidden_states)\n             22 LOAD_CONST               1 (None)\n             24 IS_OP                    1\n             26 POP_JUMP_IF_FALSE       16 (to 32)\n             28 LOAD_FAST               12 (output_hidden_states)\n             30 JUMP_FORWARD             3 (to 38)\n        >>   32 LOAD_FAST                0 (self)\n             34 LOAD_ATTR                0 (config)\n             36 LOAD_ATTR                2 (output_hidden_states)\n\n831     >>   38 STORE_FAST              12 (output_hidden_states)\n\n834          40 LOAD_FAST               13 (return_dict)\n             42 LOAD_CONST               1 (None)\n             44 IS_OP                    1\n             46 POP_JUMP_IF_FALSE       26 (to 52)\n             48 LOAD_FAST               13 (return_dict)\n             50 JUMP_FORWARD             3 (to 58)\n        >>   52 LOAD_FAST                0 (self)\n             54 LOAD_ATTR                0 (config)\n             56 LOAD_ATTR                3 (use_return_dict)\n        >>   58 STORE_FAST              13 (return_dict)\n\n836          60 LOAD_FAST                0 (self)\n             62 LOAD_ATTR                0 (config)\n             64 LOAD_ATTR                4 (is_decoder)\n             66 POP_JUMP_IF_FALSE       45 (to 90)\n\n837          68 LOAD_FAST               10 (use_cache)\n             70 LOAD_CONST               1 (None)\n             72 IS_OP                    1\n             74 POP_JUMP_IF_FALSE       40 (to 80)\n             76 LOAD_FAST               10 (use_cache)\n             78 JUMP_FORWARD             3 (to 86)\n        >>   80 LOAD_FAST                0 (self)\n             82 LOAD_ATTR                0 (config)\n             84 LOAD_ATTR                5 (use_cache)\n        >>   86 STORE_FAST              10 (use_cache)\n             88 JUMP_FORWARD             2 (to 94)\n\n839     >>   90 LOAD_CONST               2 (False)\n             92 STORE_FAST              10 (use_cache)\n\n841     >>   94 LOAD_FAST                1 (input_ids)\n             96 LOAD_CONST               1 (None)\n             98 IS_OP                    1\n            100 POP_JUMP_IF_FALSE       59 (to 118)\n            102 LOAD_FAST                6 (inputs_embeds)\n            104 LOAD_CONST               1 (None)\n            106 IS_OP                    1\n            108 POP_JUMP_IF_FALSE       59 (to 118)\n\n842         110 LOAD_GLOBAL              6 (ValueError)\n            112 LOAD_CONST               3 ('You cannot specify both input_ids and inputs_embeds at the same time')\n            114 CALL_FUNCTION            1\n            116 RAISE_VARARGS            1\n\n843     >>  118 LOAD_FAST                1 (input_ids)\n            120 LOAD_CONST               1 (None)\n            122 IS_OP                    1\n            124 POP_JUMP_IF_FALSE       74 (to 148)\n\n844         126 LOAD_FAST                1 (input_ids)\n            128 LOAD_METHOD              7 (size)\n            130 CALL_METHOD              0\n            132 STORE_FAST              14 (input_shape)\n\n845         134 LOAD_FAST                0 (self)\n            136 LOAD_METHOD              8 (warn_if_padding_and_no_attention_mask)\n            138 LOAD_FAST                1 (input_ids)\n            140 LOAD_FAST                2 (attention_mask)\n            142 CALL_METHOD              2\n            144 POP_TOP\n            146 JUMP_FORWARD            17 (to 182)\n\n846     >>  148 LOAD_FAST                6 (inputs_embeds)\n            150 LOAD_CONST               1 (None)\n            152 IS_OP                    1\n            154 POP_JUMP_IF_FALSE       87 (to 174)\n\n847         156 LOAD_FAST                6 (inputs_embeds)\n            158 LOAD_METHOD              7 (size)\n            160 CALL_METHOD              0\n            162 LOAD_CONST               1 (None)\n            164 LOAD_CONST               4 (-1)\n            166 BUILD_SLICE              2\n            168 BINARY_SUBSCR\n            170 STORE_FAST              14 (input_shape)\n            172 JUMP_FORWARD             4 (to 182)\n\n849     >>  174 LOAD_GLOBAL              6 (ValueError)\n            176 LOAD_CONST               5 ('You have to specify either input_ids or inputs_embeds')\n            178 CALL_FUNCTION            1\n            180 RAISE_VARARGS            1\n\n851     >>  182 LOAD_FAST               14 (input_shape)\n            184 UNPACK_SEQUENCE          2\n            186 STORE_FAST              15 (batch_size)\n            188 STORE_FAST              16 (seq_length)\n\n852         190 LOAD_FAST                1 (input_ids)\n            192 LOAD_CONST               1 (None)\n            194 IS_OP                    1\n            196 POP_JUMP_IF_FALSE      102 (to 204)\n            198 LOAD_FAST                1 (input_ids)\n            200 LOAD_ATTR                9 (device)\n            202 JUMP_FORWARD             2 (to 208)\n        >>  204 LOAD_FAST                6 (inputs_embeds)\n            206 LOAD_ATTR                9 (device)\n        >>  208 STORE_FAST              17 (device)\n\n855         210 LOAD_FAST                9 (past_key_values)\n            212 LOAD_CONST               1 (None)\n            214 IS_OP                    1\n            216 POP_JUMP_IF_FALSE      118 (to 236)\n            218 LOAD_FAST                9 (past_key_values)\n            220 LOAD_CONST               6 (0)\n            222 BINARY_SUBSCR\n            224 LOAD_CONST               6 (0)\n            226 BINARY_SUBSCR\n            228 LOAD_ATTR               10 (shape)\n            230 LOAD_CONST               7 (2)\n            232 BINARY_SUBSCR\n            234 JUMP_FORWARD             1 (to 238)\n        >>  236 LOAD_CONST               6 (0)\n        >>  238 STORE_FAST              18 (past_key_values_length)\n\n857         240 LOAD_FAST                2 (attention_mask)\n            242 LOAD_CONST               1 (None)\n            244 IS_OP                    0\n            246 POP_JUMP_IF_FALSE      135 (to 270)\n\n858         248 LOAD_GLOBAL             11 (torch)\n            250 LOAD_ATTR               12 (ones)\n            252 LOAD_FAST               15 (batch_size)\n            254 LOAD_FAST               16 (seq_length)\n            256 LOAD_FAST               18 (past_key_values_length)\n            258 BINARY_ADD\n            260 BUILD_TUPLE              2\n            262 LOAD_FAST               17 (device)\n            264 LOAD_CONST               8 (('device',))\n            266 CALL_FUNCTION_KW         2\n            268 STORE_FAST               2 (attention_mask)\n\n860     >>  270 LOAD_FAST                3 (token_type_ids)\n            272 LOAD_CONST               1 (None)\n            274 IS_OP                    0\n            276 POP_JUMP_IF_FALSE      175 (to 350)\n\n861         278 LOAD_GLOBAL             13 (hasattr)\n            280 LOAD_FAST                0 (self)\n            282 LOAD_ATTR               14 (embeddings)\n            284 LOAD_CONST               9 ('token_type_ids')\n            286 CALL_FUNCTION            2\n            288 POP_JUMP_IF_FALSE      166 (to 332)\n\n862         290 LOAD_FAST                0 (self)\n            292 LOAD_ATTR               14 (embeddings)\n            294 LOAD_ATTR               15 (token_type_ids)\n            296 LOAD_CONST               1 (None)\n            298 LOAD_CONST               1 (None)\n            300 BUILD_SLICE              2\n            302 LOAD_CONST               1 (None)\n            304 LOAD_FAST               16 (seq_length)\n            306 BUILD_SLICE              2\n            308 BUILD_TUPLE              2\n            310 BINARY_SUBSCR\n            312 STORE_FAST              19 (buffered_token_type_ids)\n\n863         314 LOAD_FAST               19 (buffered_token_type_ids)\n            316 LOAD_METHOD             16 (expand)\n            318 LOAD_FAST               15 (batch_size)\n            320 LOAD_FAST               16 (seq_length)\n            322 CALL_METHOD              2\n            324 STORE_FAST              20 (buffered_token_type_ids_expanded)\n\n864         326 LOAD_FAST               20 (buffered_token_type_ids_expanded)\n            328 STORE_FAST               3 (token_type_ids)\n            330 JUMP_FORWARD             9 (to 350)\n\n866     >>  332 LOAD_GLOBAL             11 (torch)\n            334 LOAD_ATTR               17 (zeros)\n            336 LOAD_FAST               14 (input_shape)\n            338 LOAD_GLOBAL             11 (torch)\n            340 LOAD_ATTR               18 (long)\n            342 LOAD_FAST               17 (device)\n            344 LOAD_CONST              10 (('dtype', 'device'))\n            346 CALL_FUNCTION_KW         3\n            348 STORE_FAST               3 (token_type_ids)\n\n870     >>  350 LOAD_FAST                0 (self)\n            352 LOAD_METHOD             19 (get_extended_attention_mask)\n            354 LOAD_FAST                2 (attention_mask)\n            356 LOAD_FAST               14 (input_shape)\n            358 CALL_METHOD              2\n            360 STORE_FAST              21 (extended_attention_mask)\n\n874         362 LOAD_FAST                0 (self)\n            364 LOAD_ATTR                0 (config)\n            366 LOAD_ATTR                4 (is_decoder)\n            368 POP_JUMP_IF_FALSE      217 (to 434)\n            370 LOAD_FAST                7 (encoder_hidden_states)\n            372 LOAD_CONST               1 (None)\n            374 IS_OP                    1\n            376 POP_JUMP_IF_FALSE      217 (to 434)\n\n875         378 LOAD_FAST                7 (encoder_hidden_states)\n            380 LOAD_METHOD              7 (size)\n            382 CALL_METHOD              0\n            384 UNPACK_SEQUENCE          3\n            386 STORE_FAST              22 (encoder_batch_size)\n            388 STORE_FAST              23 (encoder_sequence_length)\n            390 STORE_FAST              24 (_)\n\n876         392 LOAD_FAST               22 (encoder_batch_size)\n            394 LOAD_FAST               23 (encoder_sequence_length)\n            396 BUILD_TUPLE              2\n            398 STORE_FAST              25 (encoder_hidden_shape)\n\n877         400 LOAD_FAST                8 (encoder_attention_mask)\n            402 LOAD_CONST               1 (None)\n            404 IS_OP                    0\n            406 POP_JUMP_IF_FALSE      211 (to 422)\n\n878         408 LOAD_GLOBAL             11 (torch)\n            410 LOAD_ATTR               12 (ones)\n            412 LOAD_FAST               25 (encoder_hidden_shape)\n            414 LOAD_FAST               17 (device)\n            416 LOAD_CONST               8 (('device',))\n            418 CALL_FUNCTION_KW         2\n            420 STORE_FAST               8 (encoder_attention_mask)\n\n879     >>  422 LOAD_FAST                0 (self)\n            424 LOAD_METHOD             20 (invert_attention_mask)\n            426 LOAD_FAST                8 (encoder_attention_mask)\n            428 CALL_METHOD              1\n            430 STORE_FAST              26 (encoder_extended_attention_mask)\n            432 JUMP_FORWARD             2 (to 438)\n\n881     >>  434 LOAD_CONST               1 (None)\n            436 STORE_FAST              26 (encoder_extended_attention_mask)\n\n888     >>  438 LOAD_FAST                0 (self)\n            440 LOAD_METHOD             21 (get_head_mask)\n            442 LOAD_FAST                5 (head_mask)\n            444 LOAD_FAST                0 (self)\n            446 LOAD_ATTR                0 (config)\n            448 LOAD_ATTR               22 (num_hidden_layers)\n            450 CALL_METHOD              2\n            452 STORE_FAST               5 (head_mask)\n\n890         454 LOAD_FAST                0 (self)\n            456 LOAD_ATTR               14 (embeddings)\n\n891         458 LOAD_FAST                1 (input_ids)\n\n892         460 LOAD_FAST                4 (position_ids)\n\n893         462 LOAD_FAST                3 (token_type_ids)\n\n894         464 LOAD_FAST                6 (inputs_embeds)\n\n895         466 LOAD_FAST               18 (past_key_values_length)\n\n890         468 LOAD_CONST              11 (('input_ids', 'position_ids', 'token_type_ids', 'inputs_embeds', 'past_key_values_length'))\n            470 CALL_FUNCTION_KW         5\n            472 STORE_FAST              27 (embedding_output)\n\n897         474 LOAD_FAST                0 (self)\n            476 LOAD_ATTR               23 (encoder)\n\n898         478 LOAD_FAST               27 (embedding_output)\n\n899         480 LOAD_FAST               21 (extended_attention_mask)\n\n900         482 LOAD_FAST                5 (head_mask)\n\n901         484 LOAD_FAST                7 (encoder_hidden_states)\n\n902         486 LOAD_FAST               26 (encoder_extended_attention_mask)\n\n903         488 LOAD_FAST                9 (past_key_values)\n\n904         490 LOAD_FAST               10 (use_cache)\n\n905         492 LOAD_FAST               11 (output_attentions)\n\n906         494 LOAD_FAST               12 (output_hidden_states)\n\n907         496 LOAD_FAST               13 (return_dict)\n\n897         498 LOAD_CONST              12 (('attention_mask', 'head_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n            500 CALL_FUNCTION_KW        10\n            502 STORE_FAST              28 (encoder_outputs)\n\n909         504 LOAD_FAST               28 (encoder_outputs)\n            506 LOAD_CONST               6 (0)\n            508 BINARY_SUBSCR\n            510 STORE_FAST              29 (sequence_output)\n\n910         512 LOAD_FAST                0 (self)\n            514 LOAD_ATTR               24 (pooler)\n            516 LOAD_CONST               1 (None)\n            518 IS_OP                    1\n            520 EXTENDED_ARG             1\n            522 POP_JUMP_IF_FALSE      267 (to 534)\n            524 LOAD_FAST                0 (self)\n            526 LOAD_METHOD             24 (pooler)\n            528 LOAD_FAST               29 (sequence_output)\n            530 CALL_METHOD              1\n            532 JUMP_FORWARD             1 (to 536)\n        >>  534 LOAD_CONST               1 (None)\n        >>  536 STORE_FAST              30 (pooled_output)\n\n912         538 LOAD_FAST               13 (return_dict)\n            540 EXTENDED_ARG             1\n            542 POP_JUMP_IF_TRUE       282 (to 564)\n\n913         544 LOAD_FAST               29 (sequence_output)\n            546 LOAD_FAST               30 (pooled_output)\n            548 BUILD_TUPLE              2\n            550 LOAD_FAST               28 (encoder_outputs)\n            552 LOAD_CONST              13 (1)\n            554 LOAD_CONST               1 (None)\n            556 BUILD_SLICE              2\n            558 BINARY_SUBSCR\n            560 BINARY_ADD\n            562 RETURN_VALUE\n\n915     >>  564 LOAD_GLOBAL             25 (BaseModelOutputWithPoolingAndCrossAttentions)\n\n916         566 LOAD_FAST               29 (sequence_output)\n\n917         568 LOAD_FAST               30 (pooled_output)\n\n918         570 LOAD_FAST               28 (encoder_outputs)\n            572 LOAD_ATTR               26 (past_key_values)\n\n919         574 LOAD_FAST               28 (encoder_outputs)\n            576 LOAD_ATTR               27 (hidden_states)\n\n920         578 LOAD_FAST               28 (encoder_outputs)\n            580 LOAD_ATTR               28 (attentions)\n\n921         582 LOAD_FAST               28 (encoder_outputs)\n            584 LOAD_ATTR               29 (cross_attentions)\n\n915         586 LOAD_CONST              14 (('last_hidden_state', 'pooler_output', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n            588 CALL_FUNCTION_KW         6\n            590 RETURN_VALUE\n\n", "ORIGINAL BYTECODE warn_if_padding_and_no_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/modeling_utils.py line 3482 \n3486           0 LOAD_FAST                2 (attention_mask)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_TRUE        10 (to 20)\n               8 LOAD_FAST                0 (self)\n              10 LOAD_ATTR                0 (config)\n              12 LOAD_ATTR                1 (pad_token_id)\n              14 LOAD_CONST               1 (None)\n              16 IS_OP                    0\n              18 POP_JUMP_IF_FALSE       12 (to 24)\n\n3487     >>   20 LOAD_CONST               1 (None)\n              22 RETURN_VALUE\n\n3490     >>   24 LOAD_FAST                0 (self)\n              26 LOAD_ATTR                0 (config)\n              28 LOAD_ATTR                1 (pad_token_id)\n              30 LOAD_FAST                1 (input_ids)\n              32 LOAD_CONST               1 (None)\n              34 LOAD_CONST               1 (None)\n              36 BUILD_SLICE              2\n              38 LOAD_CONST               2 (-1)\n              40 LOAD_CONST               3 (0)\n              42 BUILD_LIST               2\n              44 BUILD_TUPLE              2\n              46 BINARY_SUBSCR\n              48 CONTAINS_OP              0\n              50 POP_JUMP_IF_FALSE      102 (to 204)\n\n3492          52 LOAD_CONST               4 ('We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.')\n\n3491          54 STORE_FAST               3 (warn_string)\n\n3500          56 LOAD_FAST                0 (self)\n              58 LOAD_ATTR                0 (config)\n              60 LOAD_ATTR                2 (bos_token_id)\n              62 LOAD_CONST               1 (None)\n              64 IS_OP                    1\n              66 POP_JUMP_IF_FALSE       42 (to 84)\n              68 LOAD_FAST                0 (self)\n              70 LOAD_ATTR                0 (config)\n              72 LOAD_ATTR                2 (bos_token_id)\n              74 LOAD_FAST                0 (self)\n              76 LOAD_ATTR                0 (config)\n              78 LOAD_ATTR                1 (pad_token_id)\n              80 COMPARE_OP               2 (==)\n              82 POP_JUMP_IF_TRUE        70 (to 140)\n\n3501     >>   84 LOAD_FAST                0 (self)\n              86 LOAD_ATTR                0 (config)\n              88 LOAD_ATTR                3 (eos_token_id)\n              90 LOAD_CONST               1 (None)\n              92 IS_OP                    1\n              94 POP_JUMP_IF_FALSE       56 (to 112)\n              96 LOAD_FAST                0 (self)\n              98 LOAD_ATTR                0 (config)\n             100 LOAD_ATTR                3 (eos_token_id)\n             102 LOAD_FAST                0 (self)\n             104 LOAD_ATTR                0 (config)\n             106 LOAD_ATTR                1 (pad_token_id)\n             108 COMPARE_OP               2 (==)\n             110 POP_JUMP_IF_TRUE        70 (to 140)\n\n3502     >>  112 LOAD_FAST                0 (self)\n             114 LOAD_ATTR                0 (config)\n             116 LOAD_ATTR                4 (sep_token_id)\n             118 LOAD_CONST               1 (None)\n             120 IS_OP                    1\n             122 POP_JUMP_IF_FALSE       95 (to 190)\n             124 LOAD_FAST                0 (self)\n             126 LOAD_ATTR                0 (config)\n             128 LOAD_ATTR                4 (sep_token_id)\n             130 LOAD_FAST                0 (self)\n             132 LOAD_ATTR                0 (config)\n             134 LOAD_ATTR                1 (pad_token_id)\n             136 COMPARE_OP               2 (==)\n             138 POP_JUMP_IF_FALSE       95 (to 190)\n\n3504     >>  140 LOAD_FAST                3 (warn_string)\n\n3505         142 LOAD_CONST               5 ('\\nYou may ignore this warning if your `pad_token_id` (')\n             144 LOAD_FAST                0 (self)\n             146 LOAD_ATTR                0 (config)\n             148 LOAD_ATTR                1 (pad_token_id)\n             150 FORMAT_VALUE             0\n             152 LOAD_CONST               6 (') is identical to the `bos_token_id` (')\n\n3506         154 LOAD_FAST                0 (self)\n             156 LOAD_ATTR                0 (config)\n             158 LOAD_ATTR                2 (bos_token_id)\n\n3505         160 FORMAT_VALUE             0\n             162 LOAD_CONST               7 ('), `eos_token_id` (')\n\n3506         164 LOAD_FAST                0 (self)\n             166 LOAD_ATTR                0 (config)\n             168 LOAD_ATTR                3 (eos_token_id)\n\n3505         170 FORMAT_VALUE             0\n             172 LOAD_CONST               8 ('), or the `sep_token_id` (')\n\n3507         174 LOAD_FAST                0 (self)\n             176 LOAD_ATTR                0 (config)\n             178 LOAD_ATTR                4 (sep_token_id)\n\n3505         180 FORMAT_VALUE             0\n             182 LOAD_CONST               9 ('), and your input is not padded.')\n             184 BUILD_STRING             9\n\n3504         186 INPLACE_ADD\n             188 STORE_FAST               3 (warn_string)\n\n3510     >>  190 LOAD_GLOBAL              5 (logger)\n             192 LOAD_METHOD              6 (warning_once)\n             194 LOAD_FAST                3 (warn_string)\n             196 CALL_METHOD              1\n             198 POP_TOP\n             200 LOAD_CONST               1 (None)\n             202 RETURN_VALUE\n\n3490     >>  204 LOAD_CONST               1 (None)\n             206 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py line 845 \n845           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           74 (to 148)\n              4 LOAD_FAST               12 (output_attentions)\n              6 LOAD_CONST               1 (None)\n              8 IS_OP                    1\n             10 POP_JUMP_IF_FALSE        8 (to 16)\n             12 LOAD_FAST               12 (output_attentions)\n             14 JUMP_FORWARD             3 (to 22)\n        >>   16 LOAD_FAST                1 (self)\n             18 LOAD_ATTR                0 (config)\n             20 LOAD_ATTR                1 (output_attentions)\n        >>   22 STORE_FAST              12 (output_attentions)\n             24 LOAD_FAST               13 (output_hidden_states)\n             26 LOAD_CONST               1 (None)\n             28 IS_OP                    1\n             30 POP_JUMP_IF_FALSE       18 (to 36)\n             32 LOAD_FAST               13 (output_hidden_states)\n             34 JUMP_FORWARD             3 (to 42)\n        >>   36 LOAD_FAST                1 (self)\n             38 LOAD_ATTR                0 (config)\n             40 LOAD_ATTR                2 (output_hidden_states)\n        >>   42 STORE_FAST              13 (output_hidden_states)\n             44 LOAD_FAST               14 (return_dict)\n             46 LOAD_CONST               1 (None)\n             48 IS_OP                    1\n             50 POP_JUMP_IF_FALSE       28 (to 56)\n             52 LOAD_FAST               14 (return_dict)\n             54 JUMP_FORWARD             3 (to 62)\n        >>   56 LOAD_FAST                1 (self)\n             58 LOAD_ATTR                0 (config)\n             60 LOAD_ATTR                3 (use_return_dict)\n        >>   62 STORE_FAST              14 (return_dict)\n             64 LOAD_FAST                1 (self)\n             66 LOAD_ATTR                0 (config)\n             68 LOAD_ATTR                4 (is_decoder)\n             70 POP_JUMP_IF_FALSE       47 (to 94)\n             72 LOAD_FAST               11 (use_cache)\n             74 LOAD_CONST               1 (None)\n             76 IS_OP                    1\n             78 POP_JUMP_IF_FALSE       42 (to 84)\n             80 LOAD_FAST               11 (use_cache)\n             82 JUMP_FORWARD             3 (to 90)\n        >>   84 LOAD_FAST                1 (self)\n             86 LOAD_ATTR                0 (config)\n             88 LOAD_ATTR                5 (use_cache)\n        >>   90 STORE_FAST              11 (use_cache)\n             92 JUMP_FORWARD             2 (to 98)\n        >>   94 LOAD_CONST               2 (False)\n             96 STORE_FAST              11 (use_cache)\n        >>   98 LOAD_FAST                2 (input_ids)\n            100 LOAD_CONST               1 (None)\n            102 IS_OP                    1\n            104 POP_JUMP_IF_FALSE       61 (to 122)\n            106 LOAD_FAST                7 (inputs_embeds)\n            108 LOAD_CONST               1 (None)\n            110 IS_OP                    1\n            112 POP_JUMP_IF_FALSE       61 (to 122)\n            114 LOAD_GLOBAL              6 (ValueError)\n            116 LOAD_CONST               3 ('You cannot specify both input_ids and inputs_embeds at the same time')\n            118 CALL_FUNCTION            1\n            120 RAISE_VARARGS            1\n        >>  122 LOAD_FAST                2 (input_ids)\n            124 LOAD_CONST               1 (None)\n            126 IS_OP                    1\n            128 POP_JUMP_IF_FALSE       76 (to 152)\n            130 LOAD_FAST                2 (input_ids)\n            132 LOAD_ATTR                7 (size)\n            134 CALL_FUNCTION            0\n            136 STORE_FAST              15 (input_shape)\n            138 LOAD_FAST                1 (self)\n            140 LOAD_ATTR                8 (warn_if_padding_and_no_attention_mask)\n            142 LOAD_FAST                2 (input_ids)\n            144 LOAD_FAST                3 (attention_mask)\n            146 CALL_FUNCTION            2\n        >>  148 POP_TOP\n            150 JUMP_FORWARD            17 (to 186)\n\n846     >>  152 LOAD_FAST                7 (inputs_embeds)\n            154 LOAD_CONST               1 (None)\n            156 IS_OP                    1\n            158 POP_JUMP_IF_FALSE       89 (to 178)\n\n847         160 LOAD_FAST                7 (inputs_embeds)\n            162 LOAD_ATTR                7 (size)\n            164 CALL_FUNCTION            0\n            166 LOAD_CONST               1 (None)\n            168 LOAD_CONST               4 (-1)\n            170 BUILD_SLICE              2\n            172 BINARY_SUBSCR\n            174 STORE_FAST              15 (input_shape)\n            176 JUMP_FORWARD             4 (to 186)\n\n849     >>  178 LOAD_GLOBAL              6 (ValueError)\n            180 LOAD_CONST               5 ('You have to specify either input_ids or inputs_embeds')\n            182 CALL_FUNCTION            1\n            184 RAISE_VARARGS            1\n\n851     >>  186 LOAD_FAST               15 (input_shape)\n            188 UNPACK_SEQUENCE          2\n            190 STORE_FAST              16 (batch_size)\n            192 STORE_FAST              17 (seq_length)\n\n852         194 LOAD_FAST                2 (input_ids)\n            196 LOAD_CONST               1 (None)\n            198 IS_OP                    1\n            200 POP_JUMP_IF_FALSE      104 (to 208)\n            202 LOAD_FAST                2 (input_ids)\n            204 LOAD_ATTR                9 (device)\n            206 JUMP_FORWARD             2 (to 212)\n        >>  208 LOAD_FAST                7 (inputs_embeds)\n            210 LOAD_ATTR                9 (device)\n        >>  212 STORE_FAST              18 (device)\n\n855         214 LOAD_FAST               10 (past_key_values)\n            216 LOAD_CONST               1 (None)\n            218 IS_OP                    1\n            220 POP_JUMP_IF_FALSE      120 (to 240)\n            222 LOAD_FAST               10 (past_key_values)\n            224 LOAD_CONST               6 (0)\n            226 BINARY_SUBSCR\n            228 LOAD_CONST               6 (0)\n            230 BINARY_SUBSCR\n            232 LOAD_ATTR               10 (shape)\n            234 LOAD_CONST               7 (2)\n            236 BINARY_SUBSCR\n            238 JUMP_FORWARD             1 (to 242)\n        >>  240 LOAD_CONST               6 (0)\n        >>  242 STORE_FAST              19 (past_key_values_length)\n\n857         244 LOAD_FAST                3 (attention_mask)\n            246 LOAD_CONST               1 (None)\n            248 IS_OP                    0\n            250 POP_JUMP_IF_FALSE      137 (to 274)\n\n858         252 LOAD_GLOBAL             11 (torch)\n            254 LOAD_ATTR               12 (ones)\n            256 LOAD_FAST               16 (batch_size)\n            258 LOAD_FAST               17 (seq_length)\n            260 LOAD_FAST               19 (past_key_values_length)\n            262 BINARY_ADD\n            264 BUILD_TUPLE              2\n            266 LOAD_FAST               18 (device)\n            268 LOAD_CONST               8 (('device',))\n            270 CALL_FUNCTION_KW         2\n            272 STORE_FAST               3 (attention_mask)\n\n860     >>  274 LOAD_FAST                4 (token_type_ids)\n            276 LOAD_CONST               1 (None)\n            278 IS_OP                    0\n            280 POP_JUMP_IF_FALSE      177 (to 354)\n\n861         282 LOAD_GLOBAL             13 (hasattr)\n            284 LOAD_FAST                1 (self)\n            286 LOAD_ATTR               14 (embeddings)\n            288 LOAD_CONST               9 ('token_type_ids')\n            290 CALL_FUNCTION            2\n            292 POP_JUMP_IF_FALSE      168 (to 336)\n\n862         294 LOAD_FAST                1 (self)\n            296 LOAD_ATTR               14 (embeddings)\n            298 LOAD_ATTR               15 (token_type_ids)\n            300 LOAD_CONST               1 (None)\n            302 LOAD_CONST               1 (None)\n            304 BUILD_SLICE              2\n            306 LOAD_CONST               1 (None)\n            308 LOAD_FAST               17 (seq_length)\n            310 BUILD_SLICE              2\n            312 BUILD_TUPLE              2\n            314 BINARY_SUBSCR\n            316 STORE_FAST              20 (buffered_token_type_ids)\n\n863         318 LOAD_FAST               20 (buffered_token_type_ids)\n            320 LOAD_ATTR               16 (expand)\n            322 LOAD_FAST               16 (batch_size)\n            324 LOAD_FAST               17 (seq_length)\n            326 CALL_FUNCTION            2\n            328 STORE_FAST              21 (buffered_token_type_ids_expanded)\n\n864         330 LOAD_FAST               21 (buffered_token_type_ids_expanded)\n            332 STORE_FAST               4 (token_type_ids)\n            334 JUMP_FORWARD             9 (to 354)\n\n866     >>  336 LOAD_GLOBAL             11 (torch)\n            338 LOAD_ATTR               17 (zeros)\n            340 LOAD_FAST               15 (input_shape)\n            342 LOAD_GLOBAL             11 (torch)\n            344 LOAD_ATTR               18 (long)\n            346 LOAD_FAST               18 (device)\n            348 LOAD_CONST              10 (('dtype', 'device'))\n            350 CALL_FUNCTION_KW         3\n            352 STORE_FAST               4 (token_type_ids)\n\n870     >>  354 LOAD_FAST                1 (self)\n            356 LOAD_ATTR               19 (get_extended_attention_mask)\n            358 LOAD_FAST                3 (attention_mask)\n            360 LOAD_FAST               15 (input_shape)\n            362 CALL_FUNCTION            2\n            364 STORE_FAST              22 (extended_attention_mask)\n\n874         366 LOAD_FAST                1 (self)\n            368 LOAD_ATTR                0 (config)\n            370 LOAD_ATTR                4 (is_decoder)\n            372 POP_JUMP_IF_FALSE      219 (to 438)\n            374 LOAD_FAST                8 (encoder_hidden_states)\n            376 LOAD_CONST               1 (None)\n            378 IS_OP                    1\n            380 POP_JUMP_IF_FALSE      219 (to 438)\n\n875         382 LOAD_FAST                8 (encoder_hidden_states)\n            384 LOAD_ATTR                7 (size)\n            386 CALL_FUNCTION            0\n            388 UNPACK_SEQUENCE          3\n            390 STORE_FAST              23 (encoder_batch_size)\n            392 STORE_FAST              24 (encoder_sequence_length)\n            394 STORE_FAST              25 (_)\n\n876         396 LOAD_FAST               23 (encoder_batch_size)\n            398 LOAD_FAST               24 (encoder_sequence_length)\n            400 BUILD_TUPLE              2\n            402 STORE_FAST              26 (encoder_hidden_shape)\n\n877         404 LOAD_FAST                9 (encoder_attention_mask)\n            406 LOAD_CONST               1 (None)\n            408 IS_OP                    0\n            410 POP_JUMP_IF_FALSE      213 (to 426)\n\n878         412 LOAD_GLOBAL             11 (torch)\n            414 LOAD_ATTR               12 (ones)\n            416 LOAD_FAST               26 (encoder_hidden_shape)\n            418 LOAD_FAST               18 (device)\n            420 LOAD_CONST               8 (('device',))\n            422 CALL_FUNCTION_KW         2\n            424 STORE_FAST               9 (encoder_attention_mask)\n\n879     >>  426 LOAD_FAST                1 (self)\n            428 LOAD_ATTR               20 (invert_attention_mask)\n            430 LOAD_FAST                9 (encoder_attention_mask)\n            432 CALL_FUNCTION            1\n            434 STORE_FAST              27 (encoder_extended_attention_mask)\n            436 JUMP_FORWARD             2 (to 442)\n\n881     >>  438 LOAD_CONST               1 (None)\n            440 STORE_FAST              27 (encoder_extended_attention_mask)\n\n888     >>  442 LOAD_FAST                1 (self)\n            444 LOAD_ATTR               21 (get_head_mask)\n            446 LOAD_FAST                6 (head_mask)\n            448 LOAD_FAST                1 (self)\n            450 LOAD_ATTR                0 (config)\n            452 LOAD_ATTR               22 (num_hidden_layers)\n            454 CALL_FUNCTION            2\n            456 STORE_FAST               6 (head_mask)\n\n890         458 LOAD_FAST                1 (self)\n            460 LOAD_ATTR               14 (embeddings)\n\n891         462 LOAD_FAST                2 (input_ids)\n\n892         464 LOAD_FAST                5 (position_ids)\n\n893         466 LOAD_FAST                4 (token_type_ids)\n\n894         468 LOAD_FAST                7 (inputs_embeds)\n\n895         470 LOAD_FAST               19 (past_key_values_length)\n\n890         472 LOAD_CONST              11 (('input_ids', 'position_ids', 'token_type_ids', 'inputs_embeds', 'past_key_values_length'))\n            474 CALL_FUNCTION_KW         5\n            476 STORE_FAST              28 (embedding_output)\n\n897         478 LOAD_FAST                1 (self)\n            480 LOAD_ATTR               23 (encoder)\n\n898         482 LOAD_FAST               28 (embedding_output)\n\n899         484 LOAD_FAST               22 (extended_attention_mask)\n\n900         486 LOAD_FAST                6 (head_mask)\n\n901         488 LOAD_FAST                8 (encoder_hidden_states)\n\n902         490 LOAD_FAST               27 (encoder_extended_attention_mask)\n\n903         492 LOAD_FAST               10 (past_key_values)\n\n904         494 LOAD_FAST               11 (use_cache)\n\n905         496 LOAD_FAST               12 (output_attentions)\n\n906         498 LOAD_FAST               13 (output_hidden_states)\n\n907         500 LOAD_FAST               14 (return_dict)\n\n897         502 LOAD_CONST              12 (('attention_mask', 'head_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n            504 CALL_FUNCTION_KW        10\n            506 STORE_FAST              29 (encoder_outputs)\n\n909         508 LOAD_FAST               29 (encoder_outputs)\n            510 LOAD_CONST               6 (0)\n            512 BINARY_SUBSCR\n            514 STORE_FAST              30 (sequence_output)\n\n910         516 LOAD_FAST                1 (self)\n            518 LOAD_ATTR               24 (pooler)\n            520 LOAD_CONST               1 (None)\n            522 IS_OP                    1\n            524 EXTENDED_ARG             1\n            526 POP_JUMP_IF_FALSE      269 (to 538)\n            528 LOAD_FAST                1 (self)\n            530 LOAD_ATTR               24 (pooler)\n            532 LOAD_FAST               30 (sequence_output)\n            534 CALL_FUNCTION            1\n            536 JUMP_FORWARD             1 (to 540)\n        >>  538 LOAD_CONST               1 (None)\n        >>  540 STORE_FAST              31 (pooled_output)\n\n912         542 LOAD_FAST               14 (return_dict)\n            544 EXTENDED_ARG             1\n            546 POP_JUMP_IF_TRUE       284 (to 568)\n\n913         548 LOAD_FAST               30 (sequence_output)\n            550 LOAD_FAST               31 (pooled_output)\n            552 BUILD_TUPLE              2\n            554 LOAD_FAST               29 (encoder_outputs)\n            556 LOAD_CONST              13 (1)\n            558 LOAD_CONST               1 (None)\n            560 BUILD_SLICE              2\n            562 BINARY_SUBSCR\n            564 BINARY_ADD\n            566 RETURN_VALUE\n\n915     >>  568 LOAD_GLOBAL             25 (BaseModelOutputWithPoolingAndCrossAttentions)\n\n916         570 LOAD_FAST               30 (sequence_output)\n\n917         572 LOAD_FAST               31 (pooled_output)\n\n918         574 LOAD_FAST               29 (encoder_outputs)\n            576 LOAD_ATTR               26 (past_key_values)\n\n919         578 LOAD_FAST               29 (encoder_outputs)\n            580 LOAD_ATTR               27 (hidden_states)\n\n920         582 LOAD_FAST               29 (encoder_outputs)\n            584 LOAD_ATTR               28 (attentions)\n\n921         586 LOAD_FAST               29 (encoder_outputs)\n            588 LOAD_ATTR               29 (cross_attentions)\n\n915         590 LOAD_CONST              14 (('last_hidden_state', 'pooler_output', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n            592 CALL_FUNCTION_KW         6\n            594 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py line 988 \n 988           0 LOAD_FAST                0 (___stack0)\n               2 JUMP_ABSOLUTE           27 (to 54)\n               4 LOAD_FAST                3 (return_dict)\n               6 LOAD_CONST               1 (None)\n               8 IS_OP                    1\n              10 POP_JUMP_IF_FALSE        8 (to 16)\n              12 LOAD_FAST                3 (return_dict)\n              14 JUMP_FORWARD             3 (to 22)\n         >>   16 LOAD_FAST                1 (self)\n              18 LOAD_ATTR                0 (config)\n              20 LOAD_ATTR                1 (use_return_dict)\n         >>   22 STORE_FAST               3 (return_dict)\n              24 LOAD_FAST                1 (self)\n              26 LOAD_ATTR                2 (roberta)\n              28 LOAD_FAST                4 (input_ids)\n              30 LOAD_FAST                5 (attention_mask)\n              32 LOAD_FAST                6 (token_type_ids)\n              34 LOAD_FAST                7 (position_ids)\n              36 LOAD_FAST                8 (head_mask)\n              38 LOAD_FAST                9 (inputs_embeds)\n              40 LOAD_FAST               10 (encoder_hidden_states)\n              42 LOAD_FAST               11 (encoder_attention_mask)\n              44 LOAD_FAST               12 (output_attentions)\n              46 LOAD_FAST               13 (output_hidden_states)\n              48 LOAD_FAST                3 (return_dict)\n              50 LOAD_CONST               2 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'encoder_hidden_states', 'encoder_attention_mask', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              52 CALL_FUNCTION_KW        11\n         >>   54 STORE_FAST              14 (outputs)\n\n1001          56 LOAD_FAST               14 (outputs)\n              58 LOAD_CONST               3 (0)\n              60 BINARY_SUBSCR\n              62 STORE_FAST              15 (sequence_output)\n\n1002          64 LOAD_FAST                1 (self)\n              66 LOAD_ATTR                3 (lm_head)\n              68 LOAD_FAST               15 (sequence_output)\n              70 CALL_FUNCTION            1\n              72 STORE_FAST              16 (prediction_scores)\n\n1004          74 LOAD_CONST               1 (None)\n              76 STORE_FAST              17 (masked_lm_loss)\n\n1005          78 LOAD_FAST                2 (labels)\n              80 LOAD_CONST               1 (None)\n              82 IS_OP                    1\n              84 POP_JUMP_IF_FALSE       66 (to 132)\n\n1007          86 LOAD_FAST                2 (labels)\n              88 LOAD_ATTR                4 (to)\n              90 LOAD_FAST               16 (prediction_scores)\n              92 LOAD_ATTR                5 (device)\n              94 CALL_FUNCTION            1\n              96 STORE_FAST               2 (labels)\n\n1008          98 LOAD_GLOBAL              6 (CrossEntropyLoss)\n             100 CALL_FUNCTION            0\n             102 STORE_FAST              18 (loss_fct)\n\n1009         104 LOAD_FAST               18 (loss_fct)\n             106 LOAD_FAST               16 (prediction_scores)\n             108 LOAD_ATTR                7 (view)\n             110 LOAD_CONST               4 (-1)\n             112 LOAD_FAST                1 (self)\n             114 LOAD_ATTR                0 (config)\n             116 LOAD_ATTR                8 (vocab_size)\n             118 CALL_FUNCTION            2\n             120 LOAD_FAST                2 (labels)\n             122 LOAD_ATTR                7 (view)\n             124 LOAD_CONST               4 (-1)\n             126 CALL_FUNCTION            1\n             128 CALL_FUNCTION            2\n             130 STORE_FAST              17 (masked_lm_loss)\n\n1011     >>  132 LOAD_FAST                3 (return_dict)\n             134 POP_JUMP_IF_TRUE        88 (to 176)\n\n1012         136 LOAD_FAST               16 (prediction_scores)\n             138 BUILD_TUPLE              1\n             140 LOAD_FAST               14 (outputs)\n             142 LOAD_CONST               5 (2)\n             144 LOAD_CONST               1 (None)\n             146 BUILD_SLICE              2\n             148 BINARY_SUBSCR\n             150 BINARY_ADD\n             152 STORE_FAST              19 (output)\n\n1013         154 LOAD_FAST               17 (masked_lm_loss)\n             156 LOAD_CONST               1 (None)\n             158 IS_OP                    1\n             160 POP_JUMP_IF_FALSE       86 (to 172)\n             162 LOAD_FAST               17 (masked_lm_loss)\n             164 BUILD_TUPLE              1\n             166 LOAD_FAST               19 (output)\n             168 BINARY_ADD\n             170 RETURN_VALUE\n         >>  172 LOAD_FAST               19 (output)\n             174 RETURN_VALUE\n\n1015     >>  176 LOAD_GLOBAL              9 (MaskedLMOutput)\n\n1016         178 LOAD_FAST               17 (masked_lm_loss)\n\n1017         180 LOAD_FAST               16 (prediction_scores)\n\n1018         182 LOAD_FAST               14 (outputs)\n             184 LOAD_ATTR               10 (hidden_states)\n\n1019         186 LOAD_FAST               14 (outputs)\n             188 LOAD_ATTR               11 (attentions)\n\n1015         190 LOAD_CONST               6 (('loss', 'logits', 'hidden_states', 'attentions'))\n             192 CALL_FUNCTION_KW         4\n             194 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 896 \n 958           0 LOAD_FAST               14 (return_dict)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_FALSE        6 (to 12)\n               8 LOAD_FAST               14 (return_dict)\n              10 JUMP_FORWARD             3 (to 18)\n         >>   12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (use_return_dict)\n         >>   18 STORE_FAST              14 (return_dict)\n\n 959          20 LOAD_FAST                9 (labels)\n              22 LOAD_CONST               1 (None)\n              24 IS_OP                    1\n              26 POP_JUMP_IF_FALSE       16 (to 32)\n\n 960          28 LOAD_CONST               2 (False)\n              30 STORE_FAST              11 (use_cache)\n\n 962     >>   32 LOAD_FAST                0 (self)\n              34 LOAD_ATTR                2 (roberta)\n\n 963          36 LOAD_FAST                1 (input_ids)\n\n 964          38 LOAD_FAST                2 (attention_mask)\n\n 965          40 LOAD_FAST                3 (token_type_ids)\n\n 966          42 LOAD_FAST                4 (position_ids)\n\n 967          44 LOAD_FAST                5 (head_mask)\n\n 968          46 LOAD_FAST                6 (inputs_embeds)\n\n 969          48 LOAD_FAST                7 (encoder_hidden_states)\n\n 970          50 LOAD_FAST                8 (encoder_attention_mask)\n\n 971          52 LOAD_FAST               10 (past_key_values)\n\n 972          54 LOAD_FAST               11 (use_cache)\n\n 973          56 LOAD_FAST               12 (output_attentions)\n\n 974          58 LOAD_FAST               13 (output_hidden_states)\n\n 975          60 LOAD_FAST               14 (return_dict)\n\n 962          62 LOAD_CONST               3 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              64 CALL_FUNCTION_KW        13\n              66 STORE_FAST              15 (outputs)\n\n 978          68 LOAD_FAST               15 (outputs)\n              70 LOAD_CONST               4 (0)\n              72 BINARY_SUBSCR\n              74 STORE_FAST              16 (sequence_output)\n\n 979          76 LOAD_FAST                0 (self)\n              78 LOAD_METHOD              3 (lm_head)\n              80 LOAD_FAST               16 (sequence_output)\n              82 CALL_METHOD              1\n              84 STORE_FAST              17 (prediction_scores)\n\n 981          86 LOAD_CONST               1 (None)\n              88 STORE_FAST              18 (lm_loss)\n\n 982          90 LOAD_FAST                9 (labels)\n              92 LOAD_CONST               1 (None)\n              94 IS_OP                    1\n              96 POP_JUMP_IF_FALSE       99 (to 198)\n\n 984          98 LOAD_FAST                9 (labels)\n             100 LOAD_METHOD              4 (to)\n             102 LOAD_FAST               17 (prediction_scores)\n             104 LOAD_ATTR                5 (device)\n             106 CALL_METHOD              1\n             108 STORE_FAST               9 (labels)\n\n 986         110 LOAD_FAST               17 (prediction_scores)\n             112 LOAD_CONST               1 (None)\n             114 LOAD_CONST               1 (None)\n             116 BUILD_SLICE              2\n             118 LOAD_CONST               1 (None)\n             120 LOAD_CONST               5 (-1)\n             122 BUILD_SLICE              2\n             124 LOAD_CONST               1 (None)\n             126 LOAD_CONST               1 (None)\n             128 BUILD_SLICE              2\n             130 BUILD_TUPLE              3\n             132 BINARY_SUBSCR\n             134 LOAD_METHOD              6 (contiguous)\n             136 CALL_METHOD              0\n             138 STORE_FAST              19 (shifted_prediction_scores)\n\n 987         140 LOAD_FAST                9 (labels)\n             142 LOAD_CONST               1 (None)\n             144 LOAD_CONST               1 (None)\n             146 BUILD_SLICE              2\n             148 LOAD_CONST               6 (1)\n             150 LOAD_CONST               1 (None)\n             152 BUILD_SLICE              2\n             154 BUILD_TUPLE              2\n             156 BINARY_SUBSCR\n             158 LOAD_METHOD              6 (contiguous)\n             160 CALL_METHOD              0\n             162 STORE_FAST               9 (labels)\n\n 988         164 LOAD_GLOBAL              7 (CrossEntropyLoss)\n             166 CALL_FUNCTION            0\n             168 STORE_FAST              20 (loss_fct)\n\n 989         170 LOAD_FAST               20 (loss_fct)\n             172 LOAD_FAST               19 (shifted_prediction_scores)\n             174 LOAD_METHOD              8 (view)\n             176 LOAD_CONST               5 (-1)\n             178 LOAD_FAST                0 (self)\n             180 LOAD_ATTR                0 (config)\n             182 LOAD_ATTR                9 (vocab_size)\n             184 CALL_METHOD              2\n             186 LOAD_FAST                9 (labels)\n             188 LOAD_METHOD              8 (view)\n             190 LOAD_CONST               5 (-1)\n             192 CALL_METHOD              1\n             194 CALL_FUNCTION            2\n             196 STORE_FAST              18 (lm_loss)\n\n 991     >>  198 LOAD_FAST               14 (return_dict)\n             200 POP_JUMP_IF_TRUE       121 (to 242)\n\n 992         202 LOAD_FAST               17 (prediction_scores)\n             204 BUILD_TUPLE              1\n             206 LOAD_FAST               15 (outputs)\n             208 LOAD_CONST               7 (2)\n             210 LOAD_CONST               1 (None)\n             212 BUILD_SLICE              2\n             214 BINARY_SUBSCR\n             216 BINARY_ADD\n             218 STORE_FAST              21 (output)\n\n 993         220 LOAD_FAST               18 (lm_loss)\n             222 LOAD_CONST               1 (None)\n             224 IS_OP                    1\n             226 POP_JUMP_IF_FALSE      119 (to 238)\n             228 LOAD_FAST               18 (lm_loss)\n             230 BUILD_TUPLE              1\n             232 LOAD_FAST               21 (output)\n             234 BINARY_ADD\n             236 RETURN_VALUE\n         >>  238 LOAD_FAST               21 (output)\n             240 RETURN_VALUE\n\n 995     >>  242 LOAD_GLOBAL             10 (CausalLMOutputWithCrossAttentions)\n\n 996         244 LOAD_FAST               18 (lm_loss)\n\n 997         246 LOAD_FAST               17 (prediction_scores)\n\n 998         248 LOAD_FAST               15 (outputs)\n             250 LOAD_ATTR               11 (past_key_values)\n\n 999         252 LOAD_FAST               15 (outputs)\n             254 LOAD_ATTR               12 (hidden_states)\n\n1000         256 LOAD_FAST               15 (outputs)\n             258 LOAD_ATTR               13 (attentions)\n\n1001         260 LOAD_FAST               15 (outputs)\n             262 LOAD_ATTR               14 (cross_attentions)\n\n 995         264 LOAD_CONST               8 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n             266 CALL_FUNCTION_KW         6\n             268 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 734 \n777           0 LOAD_FAST               11 (output_attentions)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    1\n              6 POP_JUMP_IF_FALSE        6 (to 12)\n              8 LOAD_FAST               11 (output_attentions)\n             10 JUMP_FORWARD             3 (to 18)\n        >>   12 LOAD_FAST                0 (self)\n             14 LOAD_ATTR                0 (config)\n             16 LOAD_ATTR                1 (output_attentions)\n        >>   18 STORE_FAST              11 (output_attentions)\n\n779          20 LOAD_FAST               12 (output_hidden_states)\n             22 LOAD_CONST               1 (None)\n             24 IS_OP                    1\n             26 POP_JUMP_IF_FALSE       16 (to 32)\n             28 LOAD_FAST               12 (output_hidden_states)\n             30 JUMP_FORWARD             3 (to 38)\n        >>   32 LOAD_FAST                0 (self)\n             34 LOAD_ATTR                0 (config)\n             36 LOAD_ATTR                2 (output_hidden_states)\n\n778     >>   38 STORE_FAST              12 (output_hidden_states)\n\n781          40 LOAD_FAST               13 (return_dict)\n             42 LOAD_CONST               1 (None)\n             44 IS_OP                    1\n             46 POP_JUMP_IF_FALSE       26 (to 52)\n             48 LOAD_FAST               13 (return_dict)\n             50 JUMP_FORWARD             3 (to 58)\n        >>   52 LOAD_FAST                0 (self)\n             54 LOAD_ATTR                0 (config)\n             56 LOAD_ATTR                3 (use_return_dict)\n        >>   58 STORE_FAST              13 (return_dict)\n\n783          60 LOAD_FAST                0 (self)\n             62 LOAD_ATTR                0 (config)\n             64 LOAD_ATTR                4 (is_decoder)\n             66 POP_JUMP_IF_FALSE       45 (to 90)\n\n784          68 LOAD_FAST               10 (use_cache)\n             70 LOAD_CONST               1 (None)\n             72 IS_OP                    1\n             74 POP_JUMP_IF_FALSE       40 (to 80)\n             76 LOAD_FAST               10 (use_cache)\n             78 JUMP_FORWARD             3 (to 86)\n        >>   80 LOAD_FAST                0 (self)\n             82 LOAD_ATTR                0 (config)\n             84 LOAD_ATTR                5 (use_cache)\n        >>   86 STORE_FAST              10 (use_cache)\n             88 JUMP_FORWARD             2 (to 94)\n\n786     >>   90 LOAD_CONST               2 (False)\n             92 STORE_FAST              10 (use_cache)\n\n788     >>   94 LOAD_FAST                1 (input_ids)\n             96 LOAD_CONST               1 (None)\n             98 IS_OP                    1\n            100 POP_JUMP_IF_FALSE       59 (to 118)\n            102 LOAD_FAST                6 (inputs_embeds)\n            104 LOAD_CONST               1 (None)\n            106 IS_OP                    1\n            108 POP_JUMP_IF_FALSE       59 (to 118)\n\n789         110 LOAD_GLOBAL              6 (ValueError)\n            112 LOAD_CONST               3 ('You cannot specify both input_ids and inputs_embeds at the same time')\n            114 CALL_FUNCTION            1\n            116 RAISE_VARARGS            1\n\n790     >>  118 LOAD_FAST                1 (input_ids)\n            120 LOAD_CONST               1 (None)\n            122 IS_OP                    1\n            124 POP_JUMP_IF_FALSE       74 (to 148)\n\n791         126 LOAD_FAST                1 (input_ids)\n            128 LOAD_METHOD              7 (size)\n            130 CALL_METHOD              0\n            132 STORE_FAST              14 (input_shape)\n\n792         134 LOAD_FAST                0 (self)\n            136 LOAD_METHOD              8 (warn_if_padding_and_no_attention_mask)\n            138 LOAD_FAST                1 (input_ids)\n            140 LOAD_FAST                2 (attention_mask)\n            142 CALL_METHOD              2\n            144 POP_TOP\n            146 JUMP_FORWARD            17 (to 182)\n\n793     >>  148 LOAD_FAST                6 (inputs_embeds)\n            150 LOAD_CONST               1 (None)\n            152 IS_OP                    1\n            154 POP_JUMP_IF_FALSE       87 (to 174)\n\n794         156 LOAD_FAST                6 (inputs_embeds)\n            158 LOAD_METHOD              7 (size)\n            160 CALL_METHOD              0\n            162 LOAD_CONST               1 (None)\n            164 LOAD_CONST               4 (-1)\n            166 BUILD_SLICE              2\n            168 BINARY_SUBSCR\n            170 STORE_FAST              14 (input_shape)\n            172 JUMP_FORWARD             4 (to 182)\n\n796     >>  174 LOAD_GLOBAL              6 (ValueError)\n            176 LOAD_CONST               5 ('You have to specify either input_ids or inputs_embeds')\n            178 CALL_FUNCTION            1\n            180 RAISE_VARARGS            1\n\n798     >>  182 LOAD_FAST               14 (input_shape)\n            184 UNPACK_SEQUENCE          2\n            186 STORE_FAST              15 (batch_size)\n            188 STORE_FAST              16 (seq_length)\n\n799         190 LOAD_FAST                1 (input_ids)\n            192 LOAD_CONST               1 (None)\n            194 IS_OP                    1\n            196 POP_JUMP_IF_FALSE      102 (to 204)\n            198 LOAD_FAST                1 (input_ids)\n            200 LOAD_ATTR                9 (device)\n            202 JUMP_FORWARD             2 (to 208)\n        >>  204 LOAD_FAST                6 (inputs_embeds)\n            206 LOAD_ATTR                9 (device)\n        >>  208 STORE_FAST              17 (device)\n\n802         210 LOAD_FAST                9 (past_key_values)\n            212 LOAD_CONST               1 (None)\n            214 IS_OP                    1\n            216 POP_JUMP_IF_FALSE      118 (to 236)\n            218 LOAD_FAST                9 (past_key_values)\n            220 LOAD_CONST               6 (0)\n            222 BINARY_SUBSCR\n            224 LOAD_CONST               6 (0)\n            226 BINARY_SUBSCR\n            228 LOAD_ATTR               10 (shape)\n            230 LOAD_CONST               7 (2)\n            232 BINARY_SUBSCR\n            234 JUMP_FORWARD             1 (to 238)\n        >>  236 LOAD_CONST               6 (0)\n        >>  238 STORE_FAST              18 (past_key_values_length)\n\n804         240 LOAD_FAST                2 (attention_mask)\n            242 LOAD_CONST               1 (None)\n            244 IS_OP                    0\n            246 POP_JUMP_IF_FALSE      135 (to 270)\n\n805         248 LOAD_GLOBAL             11 (torch)\n            250 LOAD_ATTR               12 (ones)\n            252 LOAD_FAST               15 (batch_size)\n            254 LOAD_FAST               16 (seq_length)\n            256 LOAD_FAST               18 (past_key_values_length)\n            258 BINARY_ADD\n            260 BUILD_TUPLE              2\n            262 LOAD_FAST               17 (device)\n            264 LOAD_CONST               8 (('device',))\n            266 CALL_FUNCTION_KW         2\n            268 STORE_FAST               2 (attention_mask)\n\n807     >>  270 LOAD_FAST                3 (token_type_ids)\n            272 LOAD_CONST               1 (None)\n            274 IS_OP                    0\n            276 POP_JUMP_IF_FALSE      175 (to 350)\n\n808         278 LOAD_GLOBAL             13 (hasattr)\n            280 LOAD_FAST                0 (self)\n            282 LOAD_ATTR               14 (embeddings)\n            284 LOAD_CONST               9 ('token_type_ids')\n            286 CALL_FUNCTION            2\n            288 POP_JUMP_IF_FALSE      166 (to 332)\n\n809         290 LOAD_FAST                0 (self)\n            292 LOAD_ATTR               14 (embeddings)\n            294 LOAD_ATTR               15 (token_type_ids)\n            296 LOAD_CONST               1 (None)\n            298 LOAD_CONST               1 (None)\n            300 BUILD_SLICE              2\n            302 LOAD_CONST               1 (None)\n            304 LOAD_FAST               16 (seq_length)\n            306 BUILD_SLICE              2\n            308 BUILD_TUPLE              2\n            310 BINARY_SUBSCR\n            312 STORE_FAST              19 (buffered_token_type_ids)\n\n810         314 LOAD_FAST               19 (buffered_token_type_ids)\n            316 LOAD_METHOD             16 (expand)\n            318 LOAD_FAST               15 (batch_size)\n            320 LOAD_FAST               16 (seq_length)\n            322 CALL_METHOD              2\n            324 STORE_FAST              20 (buffered_token_type_ids_expanded)\n\n811         326 LOAD_FAST               20 (buffered_token_type_ids_expanded)\n            328 STORE_FAST               3 (token_type_ids)\n            330 JUMP_FORWARD             9 (to 350)\n\n813     >>  332 LOAD_GLOBAL             11 (torch)\n            334 LOAD_ATTR               17 (zeros)\n            336 LOAD_FAST               14 (input_shape)\n            338 LOAD_GLOBAL             11 (torch)\n            340 LOAD_ATTR               18 (long)\n            342 LOAD_FAST               17 (device)\n            344 LOAD_CONST              10 (('dtype', 'device'))\n            346 CALL_FUNCTION_KW         3\n            348 STORE_FAST               3 (token_type_ids)\n\n817     >>  350 LOAD_FAST                0 (self)\n            352 LOAD_METHOD             19 (get_extended_attention_mask)\n            354 LOAD_FAST                2 (attention_mask)\n            356 LOAD_FAST               14 (input_shape)\n            358 CALL_METHOD              2\n            360 STORE_FAST              21 (extended_attention_mask)\n\n821         362 LOAD_FAST                0 (self)\n            364 LOAD_ATTR                0 (config)\n            366 LOAD_ATTR                4 (is_decoder)\n            368 POP_JUMP_IF_FALSE      217 (to 434)\n            370 LOAD_FAST                7 (encoder_hidden_states)\n            372 LOAD_CONST               1 (None)\n            374 IS_OP                    1\n            376 POP_JUMP_IF_FALSE      217 (to 434)\n\n822         378 LOAD_FAST                7 (encoder_hidden_states)\n            380 LOAD_METHOD              7 (size)\n            382 CALL_METHOD              0\n            384 UNPACK_SEQUENCE          3\n            386 STORE_FAST              22 (encoder_batch_size)\n            388 STORE_FAST              23 (encoder_sequence_length)\n            390 STORE_FAST              24 (_)\n\n823         392 LOAD_FAST               22 (encoder_batch_size)\n            394 LOAD_FAST               23 (encoder_sequence_length)\n            396 BUILD_TUPLE              2\n            398 STORE_FAST              25 (encoder_hidden_shape)\n\n824         400 LOAD_FAST                8 (encoder_attention_mask)\n            402 LOAD_CONST               1 (None)\n            404 IS_OP                    0\n            406 POP_JUMP_IF_FALSE      211 (to 422)\n\n825         408 LOAD_GLOBAL             11 (torch)\n            410 LOAD_ATTR               12 (ones)\n            412 LOAD_FAST               25 (encoder_hidden_shape)\n            414 LOAD_FAST               17 (device)\n            416 LOAD_CONST               8 (('device',))\n            418 CALL_FUNCTION_KW         2\n            420 STORE_FAST               8 (encoder_attention_mask)\n\n826     >>  422 LOAD_FAST                0 (self)\n            424 LOAD_METHOD             20 (invert_attention_mask)\n            426 LOAD_FAST                8 (encoder_attention_mask)\n            428 CALL_METHOD              1\n            430 STORE_FAST              26 (encoder_extended_attention_mask)\n            432 JUMP_FORWARD             2 (to 438)\n\n828     >>  434 LOAD_CONST               1 (None)\n            436 STORE_FAST              26 (encoder_extended_attention_mask)\n\n835     >>  438 LOAD_FAST                0 (self)\n            440 LOAD_METHOD             21 (get_head_mask)\n            442 LOAD_FAST                5 (head_mask)\n            444 LOAD_FAST                0 (self)\n            446 LOAD_ATTR                0 (config)\n            448 LOAD_ATTR               22 (num_hidden_layers)\n            450 CALL_METHOD              2\n            452 STORE_FAST               5 (head_mask)\n\n837         454 LOAD_FAST                0 (self)\n            456 LOAD_ATTR               14 (embeddings)\n\n838         458 LOAD_FAST                1 (input_ids)\n\n839         460 LOAD_FAST                4 (position_ids)\n\n840         462 LOAD_FAST                3 (token_type_ids)\n\n841         464 LOAD_FAST                6 (inputs_embeds)\n\n842         466 LOAD_FAST               18 (past_key_values_length)\n\n837         468 LOAD_CONST              11 (('input_ids', 'position_ids', 'token_type_ids', 'inputs_embeds', 'past_key_values_length'))\n            470 CALL_FUNCTION_KW         5\n            472 STORE_FAST              27 (embedding_output)\n\n844         474 LOAD_FAST                0 (self)\n            476 LOAD_ATTR               23 (encoder)\n\n845         478 LOAD_FAST               27 (embedding_output)\n\n846         480 LOAD_FAST               21 (extended_attention_mask)\n\n847         482 LOAD_FAST                5 (head_mask)\n\n848         484 LOAD_FAST                7 (encoder_hidden_states)\n\n849         486 LOAD_FAST               26 (encoder_extended_attention_mask)\n\n850         488 LOAD_FAST                9 (past_key_values)\n\n851         490 LOAD_FAST               10 (use_cache)\n\n852         492 LOAD_FAST               11 (output_attentions)\n\n853         494 LOAD_FAST               12 (output_hidden_states)\n\n854         496 LOAD_FAST               13 (return_dict)\n\n844         498 LOAD_CONST              12 (('attention_mask', 'head_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n            500 CALL_FUNCTION_KW        10\n            502 STORE_FAST              28 (encoder_outputs)\n\n856         504 LOAD_FAST               28 (encoder_outputs)\n            506 LOAD_CONST               6 (0)\n            508 BINARY_SUBSCR\n            510 STORE_FAST              29 (sequence_output)\n\n857         512 LOAD_FAST                0 (self)\n            514 LOAD_ATTR               24 (pooler)\n            516 LOAD_CONST               1 (None)\n            518 IS_OP                    1\n            520 EXTENDED_ARG             1\n            522 POP_JUMP_IF_FALSE      267 (to 534)\n            524 LOAD_FAST                0 (self)\n            526 LOAD_METHOD             24 (pooler)\n            528 LOAD_FAST               29 (sequence_output)\n            530 CALL_METHOD              1\n            532 JUMP_FORWARD             1 (to 536)\n        >>  534 LOAD_CONST               1 (None)\n        >>  536 STORE_FAST              30 (pooled_output)\n\n859         538 LOAD_FAST               13 (return_dict)\n            540 EXTENDED_ARG             1\n            542 POP_JUMP_IF_TRUE       282 (to 564)\n\n860         544 LOAD_FAST               29 (sequence_output)\n            546 LOAD_FAST               30 (pooled_output)\n            548 BUILD_TUPLE              2\n            550 LOAD_FAST               28 (encoder_outputs)\n            552 LOAD_CONST              13 (1)\n            554 LOAD_CONST               1 (None)\n            556 BUILD_SLICE              2\n            558 BINARY_SUBSCR\n            560 BINARY_ADD\n            562 RETURN_VALUE\n\n862     >>  564 LOAD_GLOBAL             25 (BaseModelOutputWithPoolingAndCrossAttentions)\n\n863         566 LOAD_FAST               29 (sequence_output)\n\n864         568 LOAD_FAST               30 (pooled_output)\n\n865         570 LOAD_FAST               28 (encoder_outputs)\n            572 LOAD_ATTR               26 (past_key_values)\n\n866         574 LOAD_FAST               28 (encoder_outputs)\n            576 LOAD_ATTR               27 (hidden_states)\n\n867         578 LOAD_FAST               28 (encoder_outputs)\n            580 LOAD_ATTR               28 (attentions)\n\n868         582 LOAD_FAST               28 (encoder_outputs)\n            584 LOAD_ATTR               29 (cross_attentions)\n\n862         586 LOAD_CONST              14 (('last_hidden_state', 'pooler_output', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n            588 CALL_FUNCTION_KW         6\n            590 RETURN_VALUE\n\n", "ORIGINAL BYTECODE warn_if_padding_and_no_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/modeling_utils.py line 3482 \n3486           0 LOAD_FAST                2 (attention_mask)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_TRUE        10 (to 20)\n               8 LOAD_FAST                0 (self)\n              10 LOAD_ATTR                0 (config)\n              12 LOAD_ATTR                1 (pad_token_id)\n              14 LOAD_CONST               1 (None)\n              16 IS_OP                    0\n              18 POP_JUMP_IF_FALSE       12 (to 24)\n\n3487     >>   20 LOAD_CONST               1 (None)\n              22 RETURN_VALUE\n\n3490     >>   24 LOAD_FAST                0 (self)\n              26 LOAD_ATTR                0 (config)\n              28 LOAD_ATTR                1 (pad_token_id)\n              30 LOAD_FAST                1 (input_ids)\n              32 LOAD_CONST               1 (None)\n              34 LOAD_CONST               1 (None)\n              36 BUILD_SLICE              2\n              38 LOAD_CONST               2 (-1)\n              40 LOAD_CONST               3 (0)\n              42 BUILD_LIST               2\n              44 BUILD_TUPLE              2\n              46 BINARY_SUBSCR\n              48 CONTAINS_OP              0\n              50 POP_JUMP_IF_FALSE      102 (to 204)\n\n3492          52 LOAD_CONST               4 ('We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.')\n\n3491          54 STORE_FAST               3 (warn_string)\n\n3500          56 LOAD_FAST                0 (self)\n              58 LOAD_ATTR                0 (config)\n              60 LOAD_ATTR                2 (bos_token_id)\n              62 LOAD_CONST               1 (None)\n              64 IS_OP                    1\n              66 POP_JUMP_IF_FALSE       42 (to 84)\n              68 LOAD_FAST                0 (self)\n              70 LOAD_ATTR                0 (config)\n              72 LOAD_ATTR                2 (bos_token_id)\n              74 LOAD_FAST                0 (self)\n              76 LOAD_ATTR                0 (config)\n              78 LOAD_ATTR                1 (pad_token_id)\n              80 COMPARE_OP               2 (==)\n              82 POP_JUMP_IF_TRUE        70 (to 140)\n\n3501     >>   84 LOAD_FAST                0 (self)\n              86 LOAD_ATTR                0 (config)\n              88 LOAD_ATTR                3 (eos_token_id)\n              90 LOAD_CONST               1 (None)\n              92 IS_OP                    1\n              94 POP_JUMP_IF_FALSE       56 (to 112)\n              96 LOAD_FAST                0 (self)\n              98 LOAD_ATTR                0 (config)\n             100 LOAD_ATTR                3 (eos_token_id)\n             102 LOAD_FAST                0 (self)\n             104 LOAD_ATTR                0 (config)\n             106 LOAD_ATTR                1 (pad_token_id)\n             108 COMPARE_OP               2 (==)\n             110 POP_JUMP_IF_TRUE        70 (to 140)\n\n3502     >>  112 LOAD_FAST                0 (self)\n             114 LOAD_ATTR                0 (config)\n             116 LOAD_ATTR                4 (sep_token_id)\n             118 LOAD_CONST               1 (None)\n             120 IS_OP                    1\n             122 POP_JUMP_IF_FALSE       95 (to 190)\n             124 LOAD_FAST                0 (self)\n             126 LOAD_ATTR                0 (config)\n             128 LOAD_ATTR                4 (sep_token_id)\n             130 LOAD_FAST                0 (self)\n             132 LOAD_ATTR                0 (config)\n             134 LOAD_ATTR                1 (pad_token_id)\n             136 COMPARE_OP               2 (==)\n             138 POP_JUMP_IF_FALSE       95 (to 190)\n\n3504     >>  140 LOAD_FAST                3 (warn_string)\n\n3505         142 LOAD_CONST               5 ('\\nYou may ignore this warning if your `pad_token_id` (')\n             144 LOAD_FAST                0 (self)\n             146 LOAD_ATTR                0 (config)\n             148 LOAD_ATTR                1 (pad_token_id)\n             150 FORMAT_VALUE             0\n             152 LOAD_CONST               6 (') is identical to the `bos_token_id` (')\n\n3506         154 LOAD_FAST                0 (self)\n             156 LOAD_ATTR                0 (config)\n             158 LOAD_ATTR                2 (bos_token_id)\n\n3505         160 FORMAT_VALUE             0\n             162 LOAD_CONST               7 ('), `eos_token_id` (')\n\n3506         164 LOAD_FAST                0 (self)\n             166 LOAD_ATTR                0 (config)\n             168 LOAD_ATTR                3 (eos_token_id)\n\n3505         170 FORMAT_VALUE             0\n             172 LOAD_CONST               8 ('), or the `sep_token_id` (')\n\n3507         174 LOAD_FAST                0 (self)\n             176 LOAD_ATTR                0 (config)\n             178 LOAD_ATTR                4 (sep_token_id)\n\n3505         180 FORMAT_VALUE             0\n             182 LOAD_CONST               9 ('), and your input is not padded.')\n             184 BUILD_STRING             9\n\n3504         186 INPLACE_ADD\n             188 STORE_FAST               3 (warn_string)\n\n3510     >>  190 LOAD_GLOBAL              5 (logger)\n             192 LOAD_METHOD              6 (warning_once)\n             194 LOAD_FAST                3 (warn_string)\n             196 CALL_METHOD              1\n             198 POP_TOP\n             200 LOAD_CONST               1 (None)\n             202 RETURN_VALUE\n\n3490     >>  204 LOAD_CONST               1 (None)\n             206 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 792 \n792           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           74 (to 148)\n              4 LOAD_FAST               12 (output_attentions)\n              6 LOAD_CONST               1 (None)\n              8 IS_OP                    1\n             10 POP_JUMP_IF_FALSE        8 (to 16)\n             12 LOAD_FAST               12 (output_attentions)\n             14 JUMP_FORWARD             3 (to 22)\n        >>   16 LOAD_FAST                1 (self)\n             18 LOAD_ATTR                0 (config)\n             20 LOAD_ATTR                1 (output_attentions)\n        >>   22 STORE_FAST              12 (output_attentions)\n             24 LOAD_FAST               13 (output_hidden_states)\n             26 LOAD_CONST               1 (None)\n             28 IS_OP                    1\n             30 POP_JUMP_IF_FALSE       18 (to 36)\n             32 LOAD_FAST               13 (output_hidden_states)\n             34 JUMP_FORWARD             3 (to 42)\n        >>   36 LOAD_FAST                1 (self)\n             38 LOAD_ATTR                0 (config)\n             40 LOAD_ATTR                2 (output_hidden_states)\n        >>   42 STORE_FAST              13 (output_hidden_states)\n             44 LOAD_FAST               14 (return_dict)\n             46 LOAD_CONST               1 (None)\n             48 IS_OP                    1\n             50 POP_JUMP_IF_FALSE       28 (to 56)\n             52 LOAD_FAST               14 (return_dict)\n             54 JUMP_FORWARD             3 (to 62)\n        >>   56 LOAD_FAST                1 (self)\n             58 LOAD_ATTR                0 (config)\n             60 LOAD_ATTR                3 (use_return_dict)\n        >>   62 STORE_FAST              14 (return_dict)\n             64 LOAD_FAST                1 (self)\n             66 LOAD_ATTR                0 (config)\n             68 LOAD_ATTR                4 (is_decoder)\n             70 POP_JUMP_IF_FALSE       47 (to 94)\n             72 LOAD_FAST               11 (use_cache)\n             74 LOAD_CONST               1 (None)\n             76 IS_OP                    1\n             78 POP_JUMP_IF_FALSE       42 (to 84)\n             80 LOAD_FAST               11 (use_cache)\n             82 JUMP_FORWARD             3 (to 90)\n        >>   84 LOAD_FAST                1 (self)\n             86 LOAD_ATTR                0 (config)\n             88 LOAD_ATTR                5 (use_cache)\n        >>   90 STORE_FAST              11 (use_cache)\n             92 JUMP_FORWARD             2 (to 98)\n        >>   94 LOAD_CONST               2 (False)\n             96 STORE_FAST              11 (use_cache)\n        >>   98 LOAD_FAST                2 (input_ids)\n            100 LOAD_CONST               1 (None)\n            102 IS_OP                    1\n            104 POP_JUMP_IF_FALSE       61 (to 122)\n            106 LOAD_FAST                7 (inputs_embeds)\n            108 LOAD_CONST               1 (None)\n            110 IS_OP                    1\n            112 POP_JUMP_IF_FALSE       61 (to 122)\n            114 LOAD_GLOBAL              6 (ValueError)\n            116 LOAD_CONST               3 ('You cannot specify both input_ids and inputs_embeds at the same time')\n            118 CALL_FUNCTION            1\n            120 RAISE_VARARGS            1\n        >>  122 LOAD_FAST                2 (input_ids)\n            124 LOAD_CONST               1 (None)\n            126 IS_OP                    1\n            128 POP_JUMP_IF_FALSE       76 (to 152)\n            130 LOAD_FAST                2 (input_ids)\n            132 LOAD_ATTR                7 (size)\n            134 CALL_FUNCTION            0\n            136 STORE_FAST              15 (input_shape)\n            138 LOAD_FAST                1 (self)\n            140 LOAD_ATTR                8 (warn_if_padding_and_no_attention_mask)\n            142 LOAD_FAST                2 (input_ids)\n            144 LOAD_FAST                3 (attention_mask)\n            146 CALL_FUNCTION            2\n        >>  148 POP_TOP\n            150 JUMP_FORWARD            17 (to 186)\n\n793     >>  152 LOAD_FAST                7 (inputs_embeds)\n            154 LOAD_CONST               1 (None)\n            156 IS_OP                    1\n            158 POP_JUMP_IF_FALSE       89 (to 178)\n\n794         160 LOAD_FAST                7 (inputs_embeds)\n            162 LOAD_ATTR                7 (size)\n            164 CALL_FUNCTION            0\n            166 LOAD_CONST               1 (None)\n            168 LOAD_CONST               4 (-1)\n            170 BUILD_SLICE              2\n            172 BINARY_SUBSCR\n            174 STORE_FAST              15 (input_shape)\n            176 JUMP_FORWARD             4 (to 186)\n\n796     >>  178 LOAD_GLOBAL              6 (ValueError)\n            180 LOAD_CONST               5 ('You have to specify either input_ids or inputs_embeds')\n            182 CALL_FUNCTION            1\n            184 RAISE_VARARGS            1\n\n798     >>  186 LOAD_FAST               15 (input_shape)\n            188 UNPACK_SEQUENCE          2\n            190 STORE_FAST              16 (batch_size)\n            192 STORE_FAST              17 (seq_length)\n\n799         194 LOAD_FAST                2 (input_ids)\n            196 LOAD_CONST               1 (None)\n            198 IS_OP                    1\n            200 POP_JUMP_IF_FALSE      104 (to 208)\n            202 LOAD_FAST                2 (input_ids)\n            204 LOAD_ATTR                9 (device)\n            206 JUMP_FORWARD             2 (to 212)\n        >>  208 LOAD_FAST                7 (inputs_embeds)\n            210 LOAD_ATTR                9 (device)\n        >>  212 STORE_FAST              18 (device)\n\n802         214 LOAD_FAST               10 (past_key_values)\n            216 LOAD_CONST               1 (None)\n            218 IS_OP                    1\n            220 POP_JUMP_IF_FALSE      120 (to 240)\n            222 LOAD_FAST               10 (past_key_values)\n            224 LOAD_CONST               6 (0)\n            226 BINARY_SUBSCR\n            228 LOAD_CONST               6 (0)\n            230 BINARY_SUBSCR\n            232 LOAD_ATTR               10 (shape)\n            234 LOAD_CONST               7 (2)\n            236 BINARY_SUBSCR\n            238 JUMP_FORWARD             1 (to 242)\n        >>  240 LOAD_CONST               6 (0)\n        >>  242 STORE_FAST              19 (past_key_values_length)\n\n804         244 LOAD_FAST                3 (attention_mask)\n            246 LOAD_CONST               1 (None)\n            248 IS_OP                    0\n            250 POP_JUMP_IF_FALSE      137 (to 274)\n\n805         252 LOAD_GLOBAL             11 (torch)\n            254 LOAD_ATTR               12 (ones)\n            256 LOAD_FAST               16 (batch_size)\n            258 LOAD_FAST               17 (seq_length)\n            260 LOAD_FAST               19 (past_key_values_length)\n            262 BINARY_ADD\n            264 BUILD_TUPLE              2\n            266 LOAD_FAST               18 (device)\n            268 LOAD_CONST               8 (('device',))\n            270 CALL_FUNCTION_KW         2\n            272 STORE_FAST               3 (attention_mask)\n\n807     >>  274 LOAD_FAST                4 (token_type_ids)\n            276 LOAD_CONST               1 (None)\n            278 IS_OP                    0\n            280 POP_JUMP_IF_FALSE      177 (to 354)\n\n808         282 LOAD_GLOBAL             13 (hasattr)\n            284 LOAD_FAST                1 (self)\n            286 LOAD_ATTR               14 (embeddings)\n            288 LOAD_CONST               9 ('token_type_ids')\n            290 CALL_FUNCTION            2\n            292 POP_JUMP_IF_FALSE      168 (to 336)\n\n809         294 LOAD_FAST                1 (self)\n            296 LOAD_ATTR               14 (embeddings)\n            298 LOAD_ATTR               15 (token_type_ids)\n            300 LOAD_CONST               1 (None)\n            302 LOAD_CONST               1 (None)\n            304 BUILD_SLICE              2\n            306 LOAD_CONST               1 (None)\n            308 LOAD_FAST               17 (seq_length)\n            310 BUILD_SLICE              2\n            312 BUILD_TUPLE              2\n            314 BINARY_SUBSCR\n            316 STORE_FAST              20 (buffered_token_type_ids)\n\n810         318 LOAD_FAST               20 (buffered_token_type_ids)\n            320 LOAD_ATTR               16 (expand)\n            322 LOAD_FAST               16 (batch_size)\n            324 LOAD_FAST               17 (seq_length)\n            326 CALL_FUNCTION            2\n            328 STORE_FAST              21 (buffered_token_type_ids_expanded)\n\n811         330 LOAD_FAST               21 (buffered_token_type_ids_expanded)\n            332 STORE_FAST               4 (token_type_ids)\n            334 JUMP_FORWARD             9 (to 354)\n\n813     >>  336 LOAD_GLOBAL             11 (torch)\n            338 LOAD_ATTR               17 (zeros)\n            340 LOAD_FAST               15 (input_shape)\n            342 LOAD_GLOBAL             11 (torch)\n            344 LOAD_ATTR               18 (long)\n            346 LOAD_FAST               18 (device)\n            348 LOAD_CONST              10 (('dtype', 'device'))\n            350 CALL_FUNCTION_KW         3\n            352 STORE_FAST               4 (token_type_ids)\n\n817     >>  354 LOAD_FAST                1 (self)\n            356 LOAD_ATTR               19 (get_extended_attention_mask)\n            358 LOAD_FAST                3 (attention_mask)\n            360 LOAD_FAST               15 (input_shape)\n            362 CALL_FUNCTION            2\n            364 STORE_FAST              22 (extended_attention_mask)\n\n821         366 LOAD_FAST                1 (self)\n            368 LOAD_ATTR                0 (config)\n            370 LOAD_ATTR                4 (is_decoder)\n            372 POP_JUMP_IF_FALSE      219 (to 438)\n            374 LOAD_FAST                8 (encoder_hidden_states)\n            376 LOAD_CONST               1 (None)\n            378 IS_OP                    1\n            380 POP_JUMP_IF_FALSE      219 (to 438)\n\n822         382 LOAD_FAST                8 (encoder_hidden_states)\n            384 LOAD_ATTR                7 (size)\n            386 CALL_FUNCTION            0\n            388 UNPACK_SEQUENCE          3\n            390 STORE_FAST              23 (encoder_batch_size)\n            392 STORE_FAST              24 (encoder_sequence_length)\n            394 STORE_FAST              25 (_)\n\n823         396 LOAD_FAST               23 (encoder_batch_size)\n            398 LOAD_FAST               24 (encoder_sequence_length)\n            400 BUILD_TUPLE              2\n            402 STORE_FAST              26 (encoder_hidden_shape)\n\n824         404 LOAD_FAST                9 (encoder_attention_mask)\n            406 LOAD_CONST               1 (None)\n            408 IS_OP                    0\n            410 POP_JUMP_IF_FALSE      213 (to 426)\n\n825         412 LOAD_GLOBAL             11 (torch)\n            414 LOAD_ATTR               12 (ones)\n            416 LOAD_FAST               26 (encoder_hidden_shape)\n            418 LOAD_FAST               18 (device)\n            420 LOAD_CONST               8 (('device',))\n            422 CALL_FUNCTION_KW         2\n            424 STORE_FAST               9 (encoder_attention_mask)\n\n826     >>  426 LOAD_FAST                1 (self)\n            428 LOAD_ATTR               20 (invert_attention_mask)\n            430 LOAD_FAST                9 (encoder_attention_mask)\n            432 CALL_FUNCTION            1\n            434 STORE_FAST              27 (encoder_extended_attention_mask)\n            436 JUMP_FORWARD             2 (to 442)\n\n828     >>  438 LOAD_CONST               1 (None)\n            440 STORE_FAST              27 (encoder_extended_attention_mask)\n\n835     >>  442 LOAD_FAST                1 (self)\n            444 LOAD_ATTR               21 (get_head_mask)\n            446 LOAD_FAST                6 (head_mask)\n            448 LOAD_FAST                1 (self)\n            450 LOAD_ATTR                0 (config)\n            452 LOAD_ATTR               22 (num_hidden_layers)\n            454 CALL_FUNCTION            2\n            456 STORE_FAST               6 (head_mask)\n\n837         458 LOAD_FAST                1 (self)\n            460 LOAD_ATTR               14 (embeddings)\n\n838         462 LOAD_FAST                2 (input_ids)\n\n839         464 LOAD_FAST                5 (position_ids)\n\n840         466 LOAD_FAST                4 (token_type_ids)\n\n841         468 LOAD_FAST                7 (inputs_embeds)\n\n842         470 LOAD_FAST               19 (past_key_values_length)\n\n837         472 LOAD_CONST              11 (('input_ids', 'position_ids', 'token_type_ids', 'inputs_embeds', 'past_key_values_length'))\n            474 CALL_FUNCTION_KW         5\n            476 STORE_FAST              28 (embedding_output)\n\n844         478 LOAD_FAST                1 (self)\n            480 LOAD_ATTR               23 (encoder)\n\n845         482 LOAD_FAST               28 (embedding_output)\n\n846         484 LOAD_FAST               22 (extended_attention_mask)\n\n847         486 LOAD_FAST                6 (head_mask)\n\n848         488 LOAD_FAST                8 (encoder_hidden_states)\n\n849         490 LOAD_FAST               27 (encoder_extended_attention_mask)\n\n850         492 LOAD_FAST               10 (past_key_values)\n\n851         494 LOAD_FAST               11 (use_cache)\n\n852         496 LOAD_FAST               12 (output_attentions)\n\n853         498 LOAD_FAST               13 (output_hidden_states)\n\n854         500 LOAD_FAST               14 (return_dict)\n\n844         502 LOAD_CONST              12 (('attention_mask', 'head_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n            504 CALL_FUNCTION_KW        10\n            506 STORE_FAST              29 (encoder_outputs)\n\n856         508 LOAD_FAST               29 (encoder_outputs)\n            510 LOAD_CONST               6 (0)\n            512 BINARY_SUBSCR\n            514 STORE_FAST              30 (sequence_output)\n\n857         516 LOAD_FAST                1 (self)\n            518 LOAD_ATTR               24 (pooler)\n            520 LOAD_CONST               1 (None)\n            522 IS_OP                    1\n            524 EXTENDED_ARG             1\n            526 POP_JUMP_IF_FALSE      269 (to 538)\n            528 LOAD_FAST                1 (self)\n            530 LOAD_ATTR               24 (pooler)\n            532 LOAD_FAST               30 (sequence_output)\n            534 CALL_FUNCTION            1\n            536 JUMP_FORWARD             1 (to 540)\n        >>  538 LOAD_CONST               1 (None)\n        >>  540 STORE_FAST              31 (pooled_output)\n\n859         542 LOAD_FAST               14 (return_dict)\n            544 EXTENDED_ARG             1\n            546 POP_JUMP_IF_TRUE       284 (to 568)\n\n860         548 LOAD_FAST               30 (sequence_output)\n            550 LOAD_FAST               31 (pooled_output)\n            552 BUILD_TUPLE              2\n            554 LOAD_FAST               29 (encoder_outputs)\n            556 LOAD_CONST              13 (1)\n            558 LOAD_CONST               1 (None)\n            560 BUILD_SLICE              2\n            562 BINARY_SUBSCR\n            564 BINARY_ADD\n            566 RETURN_VALUE\n\n862     >>  568 LOAD_GLOBAL             25 (BaseModelOutputWithPoolingAndCrossAttentions)\n\n863         570 LOAD_FAST               30 (sequence_output)\n\n864         572 LOAD_FAST               31 (pooled_output)\n\n865         574 LOAD_FAST               29 (encoder_outputs)\n            576 LOAD_ATTR               26 (past_key_values)\n\n866         578 LOAD_FAST               29 (encoder_outputs)\n            580 LOAD_ATTR               27 (hidden_states)\n\n867         582 LOAD_FAST               29 (encoder_outputs)\n            584 LOAD_ATTR               28 (attentions)\n\n868         586 LOAD_FAST               29 (encoder_outputs)\n            588 LOAD_ATTR               29 (cross_attentions)\n\n862         590 LOAD_CONST              14 (('last_hidden_state', 'pooler_output', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n            592 CALL_FUNCTION_KW         6\n            594 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 962 \n 962           0 LOAD_FAST                0 (___stack0)\n               2 JUMP_ABSOLUTE           35 (to 70)\n               4 LOAD_FAST                3 (return_dict)\n               6 LOAD_CONST               1 (None)\n               8 IS_OP                    1\n              10 POP_JUMP_IF_FALSE        8 (to 16)\n              12 LOAD_FAST                3 (return_dict)\n              14 JUMP_FORWARD             3 (to 22)\n         >>   16 LOAD_FAST                1 (self)\n              18 LOAD_ATTR                0 (config)\n              20 LOAD_ATTR                1 (use_return_dict)\n         >>   22 STORE_FAST               3 (return_dict)\n              24 LOAD_FAST                2 (labels)\n              26 LOAD_CONST               1 (None)\n              28 IS_OP                    1\n              30 POP_JUMP_IF_FALSE       18 (to 36)\n              32 LOAD_CONST               2 (False)\n              34 STORE_FAST              13 (use_cache)\n         >>   36 LOAD_FAST                1 (self)\n              38 LOAD_ATTR                2 (roberta)\n              40 LOAD_FAST                4 (input_ids)\n              42 LOAD_FAST                5 (attention_mask)\n              44 LOAD_FAST                6 (token_type_ids)\n              46 LOAD_FAST                7 (position_ids)\n              48 LOAD_FAST                8 (head_mask)\n              50 LOAD_FAST                9 (inputs_embeds)\n              52 LOAD_FAST               10 (encoder_hidden_states)\n              54 LOAD_FAST               11 (encoder_attention_mask)\n              56 LOAD_FAST               12 (past_key_values)\n              58 LOAD_FAST               13 (use_cache)\n              60 LOAD_FAST               14 (output_attentions)\n              62 LOAD_FAST               15 (output_hidden_states)\n              64 LOAD_FAST                3 (return_dict)\n              66 LOAD_CONST               3 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              68 CALL_FUNCTION_KW        13\n         >>   70 STORE_FAST              16 (outputs)\n\n 978          72 LOAD_FAST               16 (outputs)\n              74 LOAD_CONST               4 (0)\n              76 BINARY_SUBSCR\n              78 STORE_FAST              17 (sequence_output)\n\n 979          80 LOAD_FAST                1 (self)\n              82 LOAD_ATTR                3 (lm_head)\n              84 LOAD_FAST               17 (sequence_output)\n              86 CALL_FUNCTION            1\n              88 STORE_FAST              18 (prediction_scores)\n\n 981          90 LOAD_CONST               1 (None)\n              92 STORE_FAST              19 (lm_loss)\n\n 982          94 LOAD_FAST                2 (labels)\n              96 LOAD_CONST               1 (None)\n              98 IS_OP                    1\n             100 POP_JUMP_IF_FALSE      101 (to 202)\n\n 984         102 LOAD_FAST                2 (labels)\n             104 LOAD_ATTR                4 (to)\n             106 LOAD_FAST               18 (prediction_scores)\n             108 LOAD_ATTR                5 (device)\n             110 CALL_FUNCTION            1\n             112 STORE_FAST               2 (labels)\n\n 986         114 LOAD_FAST               18 (prediction_scores)\n             116 LOAD_CONST               1 (None)\n             118 LOAD_CONST               1 (None)\n             120 BUILD_SLICE              2\n             122 LOAD_CONST               1 (None)\n             124 LOAD_CONST               5 (-1)\n             126 BUILD_SLICE              2\n             128 LOAD_CONST               1 (None)\n             130 LOAD_CONST               1 (None)\n             132 BUILD_SLICE              2\n             134 BUILD_TUPLE              3\n             136 BINARY_SUBSCR\n             138 LOAD_ATTR                6 (contiguous)\n             140 CALL_FUNCTION            0\n             142 STORE_FAST              20 (shifted_prediction_scores)\n\n 987         144 LOAD_FAST                2 (labels)\n             146 LOAD_CONST               1 (None)\n             148 LOAD_CONST               1 (None)\n             150 BUILD_SLICE              2\n             152 LOAD_CONST               6 (1)\n             154 LOAD_CONST               1 (None)\n             156 BUILD_SLICE              2\n             158 BUILD_TUPLE              2\n             160 BINARY_SUBSCR\n             162 LOAD_ATTR                6 (contiguous)\n             164 CALL_FUNCTION            0\n             166 STORE_FAST               2 (labels)\n\n 988         168 LOAD_GLOBAL              7 (CrossEntropyLoss)\n             170 CALL_FUNCTION            0\n             172 STORE_FAST              21 (loss_fct)\n\n 989         174 LOAD_FAST               21 (loss_fct)\n             176 LOAD_FAST               20 (shifted_prediction_scores)\n             178 LOAD_ATTR                8 (view)\n             180 LOAD_CONST               5 (-1)\n             182 LOAD_FAST                1 (self)\n             184 LOAD_ATTR                0 (config)\n             186 LOAD_ATTR                9 (vocab_size)\n             188 CALL_FUNCTION            2\n             190 LOAD_FAST                2 (labels)\n             192 LOAD_ATTR                8 (view)\n             194 LOAD_CONST               5 (-1)\n             196 CALL_FUNCTION            1\n             198 CALL_FUNCTION            2\n             200 STORE_FAST              19 (lm_loss)\n\n 991     >>  202 LOAD_FAST                3 (return_dict)\n             204 POP_JUMP_IF_TRUE       123 (to 246)\n\n 992         206 LOAD_FAST               18 (prediction_scores)\n             208 BUILD_TUPLE              1\n             210 LOAD_FAST               16 (outputs)\n             212 LOAD_CONST               7 (2)\n             214 LOAD_CONST               1 (None)\n             216 BUILD_SLICE              2\n             218 BINARY_SUBSCR\n             220 BINARY_ADD\n             222 STORE_FAST              22 (output)\n\n 993         224 LOAD_FAST               19 (lm_loss)\n             226 LOAD_CONST               1 (None)\n             228 IS_OP                    1\n             230 POP_JUMP_IF_FALSE      121 (to 242)\n             232 LOAD_FAST               19 (lm_loss)\n             234 BUILD_TUPLE              1\n             236 LOAD_FAST               22 (output)\n             238 BINARY_ADD\n             240 RETURN_VALUE\n         >>  242 LOAD_FAST               22 (output)\n             244 RETURN_VALUE\n\n 995     >>  246 LOAD_GLOBAL             10 (CausalLMOutputWithCrossAttentions)\n\n 996         248 LOAD_FAST               19 (lm_loss)\n\n 997         250 LOAD_FAST               18 (prediction_scores)\n\n 998         252 LOAD_FAST               16 (outputs)\n             254 LOAD_ATTR               11 (past_key_values)\n\n 999         256 LOAD_FAST               16 (outputs)\n             258 LOAD_ATTR               12 (hidden_states)\n\n1000         260 LOAD_FAST               16 (outputs)\n             262 LOAD_ATTR               13 (attentions)\n\n1001         264 LOAD_FAST               16 (outputs)\n             266 LOAD_ATTR               14 (cross_attentions)\n\n 995         268 LOAD_CONST               8 (('loss', 'logits', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n             270 CALL_FUNCTION_KW         6\n             272 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 1464 \n1496           0 LOAD_FAST               11 (return_dict)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_FALSE        6 (to 12)\n               8 LOAD_FAST               11 (return_dict)\n              10 JUMP_FORWARD             3 (to 18)\n         >>   12 LOAD_FAST                0 (self)\n              14 LOAD_ATTR                0 (config)\n              16 LOAD_ATTR                1 (use_return_dict)\n         >>   18 STORE_FAST              11 (return_dict)\n\n1498          20 LOAD_FAST                0 (self)\n              22 LOAD_ATTR                2 (roberta)\n\n1499          24 LOAD_FAST                1 (input_ids)\n\n1500          26 LOAD_FAST                2 (attention_mask)\n\n1501          28 LOAD_FAST                3 (token_type_ids)\n\n1502          30 LOAD_FAST                4 (position_ids)\n\n1503          32 LOAD_FAST                5 (head_mask)\n\n1504          34 LOAD_FAST                6 (inputs_embeds)\n\n1505          36 LOAD_FAST                9 (output_attentions)\n\n1506          38 LOAD_FAST               10 (output_hidden_states)\n\n1507          40 LOAD_FAST               11 (return_dict)\n\n1498          42 LOAD_CONST               2 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              44 CALL_FUNCTION_KW         9\n              46 STORE_FAST              12 (outputs)\n\n1510          48 LOAD_FAST               12 (outputs)\n              50 LOAD_CONST               3 (0)\n              52 BINARY_SUBSCR\n              54 STORE_FAST              13 (sequence_output)\n\n1512          56 LOAD_FAST                0 (self)\n              58 LOAD_METHOD              3 (qa_outputs)\n              60 LOAD_FAST               13 (sequence_output)\n              62 CALL_METHOD              1\n              64 STORE_FAST              14 (logits)\n\n1513          66 LOAD_FAST               14 (logits)\n              68 LOAD_ATTR                4 (split)\n              70 LOAD_CONST               4 (1)\n              72 LOAD_CONST               5 (-1)\n              74 LOAD_CONST               6 (('dim',))\n              76 CALL_FUNCTION_KW         2\n              78 UNPACK_SEQUENCE          2\n              80 STORE_FAST              15 (start_logits)\n              82 STORE_FAST              16 (end_logits)\n\n1514          84 LOAD_FAST               15 (start_logits)\n              86 LOAD_METHOD              5 (squeeze)\n              88 LOAD_CONST               5 (-1)\n              90 CALL_METHOD              1\n              92 LOAD_METHOD              6 (contiguous)\n              94 CALL_METHOD              0\n              96 STORE_FAST              15 (start_logits)\n\n1515          98 LOAD_FAST               16 (end_logits)\n             100 LOAD_METHOD              5 (squeeze)\n             102 LOAD_CONST               5 (-1)\n             104 CALL_METHOD              1\n             106 LOAD_METHOD              6 (contiguous)\n             108 CALL_METHOD              0\n             110 STORE_FAST              16 (end_logits)\n\n1517         112 LOAD_CONST               1 (None)\n             114 STORE_FAST              17 (total_loss)\n\n1518         116 LOAD_FAST                7 (start_positions)\n             118 LOAD_CONST               1 (None)\n             120 IS_OP                    1\n             122 POP_JUMP_IF_FALSE      130 (to 260)\n             124 LOAD_FAST                8 (end_positions)\n             126 LOAD_CONST               1 (None)\n             128 IS_OP                    1\n             130 POP_JUMP_IF_FALSE      130 (to 260)\n\n1520         132 LOAD_GLOBAL              7 (len)\n             134 LOAD_FAST                7 (start_positions)\n             136 LOAD_METHOD              8 (size)\n             138 CALL_METHOD              0\n             140 CALL_FUNCTION            1\n             142 LOAD_CONST               4 (1)\n             144 COMPARE_OP               4 (>)\n             146 POP_JUMP_IF_FALSE       79 (to 158)\n\n1521         148 LOAD_FAST                7 (start_positions)\n             150 LOAD_METHOD              5 (squeeze)\n             152 LOAD_CONST               5 (-1)\n             154 CALL_METHOD              1\n             156 STORE_FAST               7 (start_positions)\n\n1522     >>  158 LOAD_GLOBAL              7 (len)\n             160 LOAD_FAST                8 (end_positions)\n             162 LOAD_METHOD              8 (size)\n             164 CALL_METHOD              0\n             166 CALL_FUNCTION            1\n             168 LOAD_CONST               4 (1)\n             170 COMPARE_OP               4 (>)\n             172 POP_JUMP_IF_FALSE       92 (to 184)\n\n1523         174 LOAD_FAST                8 (end_positions)\n             176 LOAD_METHOD              5 (squeeze)\n             178 LOAD_CONST               5 (-1)\n             180 CALL_METHOD              1\n             182 STORE_FAST               8 (end_positions)\n\n1525     >>  184 LOAD_FAST               15 (start_logits)\n             186 LOAD_METHOD              8 (size)\n             188 LOAD_CONST               4 (1)\n             190 CALL_METHOD              1\n             192 STORE_FAST              18 (ignored_index)\n\n1526         194 LOAD_FAST                7 (start_positions)\n             196 LOAD_METHOD              9 (clamp)\n             198 LOAD_CONST               3 (0)\n             200 LOAD_FAST               18 (ignored_index)\n             202 CALL_METHOD              2\n             204 STORE_FAST               7 (start_positions)\n\n1527         206 LOAD_FAST                8 (end_positions)\n             208 LOAD_METHOD              9 (clamp)\n             210 LOAD_CONST               3 (0)\n             212 LOAD_FAST               18 (ignored_index)\n             214 CALL_METHOD              2\n             216 STORE_FAST               8 (end_positions)\n\n1529         218 LOAD_GLOBAL             10 (CrossEntropyLoss)\n             220 LOAD_FAST               18 (ignored_index)\n             222 LOAD_CONST               7 (('ignore_index',))\n             224 CALL_FUNCTION_KW         1\n             226 STORE_FAST              19 (loss_fct)\n\n1530         228 LOAD_FAST               19 (loss_fct)\n             230 LOAD_FAST               15 (start_logits)\n             232 LOAD_FAST                7 (start_positions)\n             234 CALL_FUNCTION            2\n             236 STORE_FAST              20 (start_loss)\n\n1531         238 LOAD_FAST               19 (loss_fct)\n             240 LOAD_FAST               16 (end_logits)\n             242 LOAD_FAST                8 (end_positions)\n             244 CALL_FUNCTION            2\n             246 STORE_FAST              21 (end_loss)\n\n1532         248 LOAD_FAST               20 (start_loss)\n             250 LOAD_FAST               21 (end_loss)\n             252 BINARY_ADD\n             254 LOAD_CONST               8 (2)\n             256 BINARY_TRUE_DIVIDE\n             258 STORE_FAST              17 (total_loss)\n\n1534     >>  260 LOAD_FAST               11 (return_dict)\n             262 POP_JUMP_IF_TRUE       153 (to 306)\n\n1535         264 LOAD_FAST               15 (start_logits)\n             266 LOAD_FAST               16 (end_logits)\n             268 BUILD_TUPLE              2\n             270 LOAD_FAST               12 (outputs)\n             272 LOAD_CONST               8 (2)\n             274 LOAD_CONST               1 (None)\n             276 BUILD_SLICE              2\n             278 BINARY_SUBSCR\n             280 BINARY_ADD\n             282 STORE_FAST              22 (output)\n\n1536         284 LOAD_FAST               17 (total_loss)\n             286 LOAD_CONST               1 (None)\n             288 IS_OP                    1\n             290 POP_JUMP_IF_FALSE      151 (to 302)\n             292 LOAD_FAST               17 (total_loss)\n             294 BUILD_TUPLE              1\n             296 LOAD_FAST               22 (output)\n             298 BINARY_ADD\n             300 RETURN_VALUE\n         >>  302 LOAD_FAST               22 (output)\n             304 RETURN_VALUE\n\n1538     >>  306 LOAD_GLOBAL             11 (QuestionAnsweringModelOutput)\n\n1539         308 LOAD_FAST               17 (total_loss)\n\n1540         310 LOAD_FAST               15 (start_logits)\n\n1541         312 LOAD_FAST               16 (end_logits)\n\n1542         314 LOAD_FAST               12 (outputs)\n             316 LOAD_ATTR               12 (hidden_states)\n\n1543         318 LOAD_FAST               12 (outputs)\n             320 LOAD_ATTR               13 (attentions)\n\n1538         322 LOAD_CONST               9 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n             324 CALL_FUNCTION_KW         5\n             326 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 734 \n777           0 LOAD_FAST               11 (output_attentions)\n              2 LOAD_CONST               1 (None)\n              4 IS_OP                    1\n              6 POP_JUMP_IF_FALSE        6 (to 12)\n              8 LOAD_FAST               11 (output_attentions)\n             10 JUMP_FORWARD             3 (to 18)\n        >>   12 LOAD_FAST                0 (self)\n             14 LOAD_ATTR                0 (config)\n             16 LOAD_ATTR                1 (output_attentions)\n        >>   18 STORE_FAST              11 (output_attentions)\n\n779          20 LOAD_FAST               12 (output_hidden_states)\n             22 LOAD_CONST               1 (None)\n             24 IS_OP                    1\n             26 POP_JUMP_IF_FALSE       16 (to 32)\n             28 LOAD_FAST               12 (output_hidden_states)\n             30 JUMP_FORWARD             3 (to 38)\n        >>   32 LOAD_FAST                0 (self)\n             34 LOAD_ATTR                0 (config)\n             36 LOAD_ATTR                2 (output_hidden_states)\n\n778     >>   38 STORE_FAST              12 (output_hidden_states)\n\n781          40 LOAD_FAST               13 (return_dict)\n             42 LOAD_CONST               1 (None)\n             44 IS_OP                    1\n             46 POP_JUMP_IF_FALSE       26 (to 52)\n             48 LOAD_FAST               13 (return_dict)\n             50 JUMP_FORWARD             3 (to 58)\n        >>   52 LOAD_FAST                0 (self)\n             54 LOAD_ATTR                0 (config)\n             56 LOAD_ATTR                3 (use_return_dict)\n        >>   58 STORE_FAST              13 (return_dict)\n\n783          60 LOAD_FAST                0 (self)\n             62 LOAD_ATTR                0 (config)\n             64 LOAD_ATTR                4 (is_decoder)\n             66 POP_JUMP_IF_FALSE       45 (to 90)\n\n784          68 LOAD_FAST               10 (use_cache)\n             70 LOAD_CONST               1 (None)\n             72 IS_OP                    1\n             74 POP_JUMP_IF_FALSE       40 (to 80)\n             76 LOAD_FAST               10 (use_cache)\n             78 JUMP_FORWARD             3 (to 86)\n        >>   80 LOAD_FAST                0 (self)\n             82 LOAD_ATTR                0 (config)\n             84 LOAD_ATTR                5 (use_cache)\n        >>   86 STORE_FAST              10 (use_cache)\n             88 JUMP_FORWARD             2 (to 94)\n\n786     >>   90 LOAD_CONST               2 (False)\n             92 STORE_FAST              10 (use_cache)\n\n788     >>   94 LOAD_FAST                1 (input_ids)\n             96 LOAD_CONST               1 (None)\n             98 IS_OP                    1\n            100 POP_JUMP_IF_FALSE       59 (to 118)\n            102 LOAD_FAST                6 (inputs_embeds)\n            104 LOAD_CONST               1 (None)\n            106 IS_OP                    1\n            108 POP_JUMP_IF_FALSE       59 (to 118)\n\n789         110 LOAD_GLOBAL              6 (ValueError)\n            112 LOAD_CONST               3 ('You cannot specify both input_ids and inputs_embeds at the same time')\n            114 CALL_FUNCTION            1\n            116 RAISE_VARARGS            1\n\n790     >>  118 LOAD_FAST                1 (input_ids)\n            120 LOAD_CONST               1 (None)\n            122 IS_OP                    1\n            124 POP_JUMP_IF_FALSE       74 (to 148)\n\n791         126 LOAD_FAST                1 (input_ids)\n            128 LOAD_METHOD              7 (size)\n            130 CALL_METHOD              0\n            132 STORE_FAST              14 (input_shape)\n\n792         134 LOAD_FAST                0 (self)\n            136 LOAD_METHOD              8 (warn_if_padding_and_no_attention_mask)\n            138 LOAD_FAST                1 (input_ids)\n            140 LOAD_FAST                2 (attention_mask)\n            142 CALL_METHOD              2\n            144 POP_TOP\n            146 JUMP_FORWARD            17 (to 182)\n\n793     >>  148 LOAD_FAST                6 (inputs_embeds)\n            150 LOAD_CONST               1 (None)\n            152 IS_OP                    1\n            154 POP_JUMP_IF_FALSE       87 (to 174)\n\n794         156 LOAD_FAST                6 (inputs_embeds)\n            158 LOAD_METHOD              7 (size)\n            160 CALL_METHOD              0\n            162 LOAD_CONST               1 (None)\n            164 LOAD_CONST               4 (-1)\n            166 BUILD_SLICE              2\n            168 BINARY_SUBSCR\n            170 STORE_FAST              14 (input_shape)\n            172 JUMP_FORWARD             4 (to 182)\n\n796     >>  174 LOAD_GLOBAL              6 (ValueError)\n            176 LOAD_CONST               5 ('You have to specify either input_ids or inputs_embeds')\n            178 CALL_FUNCTION            1\n            180 RAISE_VARARGS            1\n\n798     >>  182 LOAD_FAST               14 (input_shape)\n            184 UNPACK_SEQUENCE          2\n            186 STORE_FAST              15 (batch_size)\n            188 STORE_FAST              16 (seq_length)\n\n799         190 LOAD_FAST                1 (input_ids)\n            192 LOAD_CONST               1 (None)\n            194 IS_OP                    1\n            196 POP_JUMP_IF_FALSE      102 (to 204)\n            198 LOAD_FAST                1 (input_ids)\n            200 LOAD_ATTR                9 (device)\n            202 JUMP_FORWARD             2 (to 208)\n        >>  204 LOAD_FAST                6 (inputs_embeds)\n            206 LOAD_ATTR                9 (device)\n        >>  208 STORE_FAST              17 (device)\n\n802         210 LOAD_FAST                9 (past_key_values)\n            212 LOAD_CONST               1 (None)\n            214 IS_OP                    1\n            216 POP_JUMP_IF_FALSE      118 (to 236)\n            218 LOAD_FAST                9 (past_key_values)\n            220 LOAD_CONST               6 (0)\n            222 BINARY_SUBSCR\n            224 LOAD_CONST               6 (0)\n            226 BINARY_SUBSCR\n            228 LOAD_ATTR               10 (shape)\n            230 LOAD_CONST               7 (2)\n            232 BINARY_SUBSCR\n            234 JUMP_FORWARD             1 (to 238)\n        >>  236 LOAD_CONST               6 (0)\n        >>  238 STORE_FAST              18 (past_key_values_length)\n\n804         240 LOAD_FAST                2 (attention_mask)\n            242 LOAD_CONST               1 (None)\n            244 IS_OP                    0\n            246 POP_JUMP_IF_FALSE      135 (to 270)\n\n805         248 LOAD_GLOBAL             11 (torch)\n            250 LOAD_ATTR               12 (ones)\n            252 LOAD_FAST               15 (batch_size)\n            254 LOAD_FAST               16 (seq_length)\n            256 LOAD_FAST               18 (past_key_values_length)\n            258 BINARY_ADD\n            260 BUILD_TUPLE              2\n            262 LOAD_FAST               17 (device)\n            264 LOAD_CONST               8 (('device',))\n            266 CALL_FUNCTION_KW         2\n            268 STORE_FAST               2 (attention_mask)\n\n807     >>  270 LOAD_FAST                3 (token_type_ids)\n            272 LOAD_CONST               1 (None)\n            274 IS_OP                    0\n            276 POP_JUMP_IF_FALSE      175 (to 350)\n\n808         278 LOAD_GLOBAL             13 (hasattr)\n            280 LOAD_FAST                0 (self)\n            282 LOAD_ATTR               14 (embeddings)\n            284 LOAD_CONST               9 ('token_type_ids')\n            286 CALL_FUNCTION            2\n            288 POP_JUMP_IF_FALSE      166 (to 332)\n\n809         290 LOAD_FAST                0 (self)\n            292 LOAD_ATTR               14 (embeddings)\n            294 LOAD_ATTR               15 (token_type_ids)\n            296 LOAD_CONST               1 (None)\n            298 LOAD_CONST               1 (None)\n            300 BUILD_SLICE              2\n            302 LOAD_CONST               1 (None)\n            304 LOAD_FAST               16 (seq_length)\n            306 BUILD_SLICE              2\n            308 BUILD_TUPLE              2\n            310 BINARY_SUBSCR\n            312 STORE_FAST              19 (buffered_token_type_ids)\n\n810         314 LOAD_FAST               19 (buffered_token_type_ids)\n            316 LOAD_METHOD             16 (expand)\n            318 LOAD_FAST               15 (batch_size)\n            320 LOAD_FAST               16 (seq_length)\n            322 CALL_METHOD              2\n            324 STORE_FAST              20 (buffered_token_type_ids_expanded)\n\n811         326 LOAD_FAST               20 (buffered_token_type_ids_expanded)\n            328 STORE_FAST               3 (token_type_ids)\n            330 JUMP_FORWARD             9 (to 350)\n\n813     >>  332 LOAD_GLOBAL             11 (torch)\n            334 LOAD_ATTR               17 (zeros)\n            336 LOAD_FAST               14 (input_shape)\n            338 LOAD_GLOBAL             11 (torch)\n            340 LOAD_ATTR               18 (long)\n            342 LOAD_FAST               17 (device)\n            344 LOAD_CONST              10 (('dtype', 'device'))\n            346 CALL_FUNCTION_KW         3\n            348 STORE_FAST               3 (token_type_ids)\n\n817     >>  350 LOAD_FAST                0 (self)\n            352 LOAD_METHOD             19 (get_extended_attention_mask)\n            354 LOAD_FAST                2 (attention_mask)\n            356 LOAD_FAST               14 (input_shape)\n            358 CALL_METHOD              2\n            360 STORE_FAST              21 (extended_attention_mask)\n\n821         362 LOAD_FAST                0 (self)\n            364 LOAD_ATTR                0 (config)\n            366 LOAD_ATTR                4 (is_decoder)\n            368 POP_JUMP_IF_FALSE      217 (to 434)\n            370 LOAD_FAST                7 (encoder_hidden_states)\n            372 LOAD_CONST               1 (None)\n            374 IS_OP                    1\n            376 POP_JUMP_IF_FALSE      217 (to 434)\n\n822         378 LOAD_FAST                7 (encoder_hidden_states)\n            380 LOAD_METHOD              7 (size)\n            382 CALL_METHOD              0\n            384 UNPACK_SEQUENCE          3\n            386 STORE_FAST              22 (encoder_batch_size)\n            388 STORE_FAST              23 (encoder_sequence_length)\n            390 STORE_FAST              24 (_)\n\n823         392 LOAD_FAST               22 (encoder_batch_size)\n            394 LOAD_FAST               23 (encoder_sequence_length)\n            396 BUILD_TUPLE              2\n            398 STORE_FAST              25 (encoder_hidden_shape)\n\n824         400 LOAD_FAST                8 (encoder_attention_mask)\n            402 LOAD_CONST               1 (None)\n            404 IS_OP                    0\n            406 POP_JUMP_IF_FALSE      211 (to 422)\n\n825         408 LOAD_GLOBAL             11 (torch)\n            410 LOAD_ATTR               12 (ones)\n            412 LOAD_FAST               25 (encoder_hidden_shape)\n            414 LOAD_FAST               17 (device)\n            416 LOAD_CONST               8 (('device',))\n            418 CALL_FUNCTION_KW         2\n            420 STORE_FAST               8 (encoder_attention_mask)\n\n826     >>  422 LOAD_FAST                0 (self)\n            424 LOAD_METHOD             20 (invert_attention_mask)\n            426 LOAD_FAST                8 (encoder_attention_mask)\n            428 CALL_METHOD              1\n            430 STORE_FAST              26 (encoder_extended_attention_mask)\n            432 JUMP_FORWARD             2 (to 438)\n\n828     >>  434 LOAD_CONST               1 (None)\n            436 STORE_FAST              26 (encoder_extended_attention_mask)\n\n835     >>  438 LOAD_FAST                0 (self)\n            440 LOAD_METHOD             21 (get_head_mask)\n            442 LOAD_FAST                5 (head_mask)\n            444 LOAD_FAST                0 (self)\n            446 LOAD_ATTR                0 (config)\n            448 LOAD_ATTR               22 (num_hidden_layers)\n            450 CALL_METHOD              2\n            452 STORE_FAST               5 (head_mask)\n\n837         454 LOAD_FAST                0 (self)\n            456 LOAD_ATTR               14 (embeddings)\n\n838         458 LOAD_FAST                1 (input_ids)\n\n839         460 LOAD_FAST                4 (position_ids)\n\n840         462 LOAD_FAST                3 (token_type_ids)\n\n841         464 LOAD_FAST                6 (inputs_embeds)\n\n842         466 LOAD_FAST               18 (past_key_values_length)\n\n837         468 LOAD_CONST              11 (('input_ids', 'position_ids', 'token_type_ids', 'inputs_embeds', 'past_key_values_length'))\n            470 CALL_FUNCTION_KW         5\n            472 STORE_FAST              27 (embedding_output)\n\n844         474 LOAD_FAST                0 (self)\n            476 LOAD_ATTR               23 (encoder)\n\n845         478 LOAD_FAST               27 (embedding_output)\n\n846         480 LOAD_FAST               21 (extended_attention_mask)\n\n847         482 LOAD_FAST                5 (head_mask)\n\n848         484 LOAD_FAST                7 (encoder_hidden_states)\n\n849         486 LOAD_FAST               26 (encoder_extended_attention_mask)\n\n850         488 LOAD_FAST                9 (past_key_values)\n\n851         490 LOAD_FAST               10 (use_cache)\n\n852         492 LOAD_FAST               11 (output_attentions)\n\n853         494 LOAD_FAST               12 (output_hidden_states)\n\n854         496 LOAD_FAST               13 (return_dict)\n\n844         498 LOAD_CONST              12 (('attention_mask', 'head_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n            500 CALL_FUNCTION_KW        10\n            502 STORE_FAST              28 (encoder_outputs)\n\n856         504 LOAD_FAST               28 (encoder_outputs)\n            506 LOAD_CONST               6 (0)\n            508 BINARY_SUBSCR\n            510 STORE_FAST              29 (sequence_output)\n\n857         512 LOAD_FAST                0 (self)\n            514 LOAD_ATTR               24 (pooler)\n            516 LOAD_CONST               1 (None)\n            518 IS_OP                    1\n            520 EXTENDED_ARG             1\n            522 POP_JUMP_IF_FALSE      267 (to 534)\n            524 LOAD_FAST                0 (self)\n            526 LOAD_METHOD             24 (pooler)\n            528 LOAD_FAST               29 (sequence_output)\n            530 CALL_METHOD              1\n            532 JUMP_FORWARD             1 (to 536)\n        >>  534 LOAD_CONST               1 (None)\n        >>  536 STORE_FAST              30 (pooled_output)\n\n859         538 LOAD_FAST               13 (return_dict)\n            540 EXTENDED_ARG             1\n            542 POP_JUMP_IF_TRUE       282 (to 564)\n\n860         544 LOAD_FAST               29 (sequence_output)\n            546 LOAD_FAST               30 (pooled_output)\n            548 BUILD_TUPLE              2\n            550 LOAD_FAST               28 (encoder_outputs)\n            552 LOAD_CONST              13 (1)\n            554 LOAD_CONST               1 (None)\n            556 BUILD_SLICE              2\n            558 BINARY_SUBSCR\n            560 BINARY_ADD\n            562 RETURN_VALUE\n\n862     >>  564 LOAD_GLOBAL             25 (BaseModelOutputWithPoolingAndCrossAttentions)\n\n863         566 LOAD_FAST               29 (sequence_output)\n\n864         568 LOAD_FAST               30 (pooled_output)\n\n865         570 LOAD_FAST               28 (encoder_outputs)\n            572 LOAD_ATTR               26 (past_key_values)\n\n866         574 LOAD_FAST               28 (encoder_outputs)\n            576 LOAD_ATTR               27 (hidden_states)\n\n867         578 LOAD_FAST               28 (encoder_outputs)\n            580 LOAD_ATTR               28 (attentions)\n\n868         582 LOAD_FAST               28 (encoder_outputs)\n            584 LOAD_ATTR               29 (cross_attentions)\n\n862         586 LOAD_CONST              14 (('last_hidden_state', 'pooler_output', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n            588 CALL_FUNCTION_KW         6\n            590 RETURN_VALUE\n\n", "ORIGINAL BYTECODE warn_if_padding_and_no_attention_mask /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/modeling_utils.py line 3482 \n3486           0 LOAD_FAST                2 (attention_mask)\n               2 LOAD_CONST               1 (None)\n               4 IS_OP                    1\n               6 POP_JUMP_IF_TRUE        10 (to 20)\n               8 LOAD_FAST                0 (self)\n              10 LOAD_ATTR                0 (config)\n              12 LOAD_ATTR                1 (pad_token_id)\n              14 LOAD_CONST               1 (None)\n              16 IS_OP                    0\n              18 POP_JUMP_IF_FALSE       12 (to 24)\n\n3487     >>   20 LOAD_CONST               1 (None)\n              22 RETURN_VALUE\n\n3490     >>   24 LOAD_FAST                0 (self)\n              26 LOAD_ATTR                0 (config)\n              28 LOAD_ATTR                1 (pad_token_id)\n              30 LOAD_FAST                1 (input_ids)\n              32 LOAD_CONST               1 (None)\n              34 LOAD_CONST               1 (None)\n              36 BUILD_SLICE              2\n              38 LOAD_CONST               2 (-1)\n              40 LOAD_CONST               3 (0)\n              42 BUILD_LIST               2\n              44 BUILD_TUPLE              2\n              46 BINARY_SUBSCR\n              48 CONTAINS_OP              0\n              50 POP_JUMP_IF_FALSE      102 (to 204)\n\n3492          52 LOAD_CONST               4 ('We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.')\n\n3491          54 STORE_FAST               3 (warn_string)\n\n3500          56 LOAD_FAST                0 (self)\n              58 LOAD_ATTR                0 (config)\n              60 LOAD_ATTR                2 (bos_token_id)\n              62 LOAD_CONST               1 (None)\n              64 IS_OP                    1\n              66 POP_JUMP_IF_FALSE       42 (to 84)\n              68 LOAD_FAST                0 (self)\n              70 LOAD_ATTR                0 (config)\n              72 LOAD_ATTR                2 (bos_token_id)\n              74 LOAD_FAST                0 (self)\n              76 LOAD_ATTR                0 (config)\n              78 LOAD_ATTR                1 (pad_token_id)\n              80 COMPARE_OP               2 (==)\n              82 POP_JUMP_IF_TRUE        70 (to 140)\n\n3501     >>   84 LOAD_FAST                0 (self)\n              86 LOAD_ATTR                0 (config)\n              88 LOAD_ATTR                3 (eos_token_id)\n              90 LOAD_CONST               1 (None)\n              92 IS_OP                    1\n              94 POP_JUMP_IF_FALSE       56 (to 112)\n              96 LOAD_FAST                0 (self)\n              98 LOAD_ATTR                0 (config)\n             100 LOAD_ATTR                3 (eos_token_id)\n             102 LOAD_FAST                0 (self)\n             104 LOAD_ATTR                0 (config)\n             106 LOAD_ATTR                1 (pad_token_id)\n             108 COMPARE_OP               2 (==)\n             110 POP_JUMP_IF_TRUE        70 (to 140)\n\n3502     >>  112 LOAD_FAST                0 (self)\n             114 LOAD_ATTR                0 (config)\n             116 LOAD_ATTR                4 (sep_token_id)\n             118 LOAD_CONST               1 (None)\n             120 IS_OP                    1\n             122 POP_JUMP_IF_FALSE       95 (to 190)\n             124 LOAD_FAST                0 (self)\n             126 LOAD_ATTR                0 (config)\n             128 LOAD_ATTR                4 (sep_token_id)\n             130 LOAD_FAST                0 (self)\n             132 LOAD_ATTR                0 (config)\n             134 LOAD_ATTR                1 (pad_token_id)\n             136 COMPARE_OP               2 (==)\n             138 POP_JUMP_IF_FALSE       95 (to 190)\n\n3504     >>  140 LOAD_FAST                3 (warn_string)\n\n3505         142 LOAD_CONST               5 ('\\nYou may ignore this warning if your `pad_token_id` (')\n             144 LOAD_FAST                0 (self)\n             146 LOAD_ATTR                0 (config)\n             148 LOAD_ATTR                1 (pad_token_id)\n             150 FORMAT_VALUE             0\n             152 LOAD_CONST               6 (') is identical to the `bos_token_id` (')\n\n3506         154 LOAD_FAST                0 (self)\n             156 LOAD_ATTR                0 (config)\n             158 LOAD_ATTR                2 (bos_token_id)\n\n3505         160 FORMAT_VALUE             0\n             162 LOAD_CONST               7 ('), `eos_token_id` (')\n\n3506         164 LOAD_FAST                0 (self)\n             166 LOAD_ATTR                0 (config)\n             168 LOAD_ATTR                3 (eos_token_id)\n\n3505         170 FORMAT_VALUE             0\n             172 LOAD_CONST               8 ('), or the `sep_token_id` (')\n\n3507         174 LOAD_FAST                0 (self)\n             176 LOAD_ATTR                0 (config)\n             178 LOAD_ATTR                4 (sep_token_id)\n\n3505         180 FORMAT_VALUE             0\n             182 LOAD_CONST               9 ('), and your input is not padded.')\n             184 BUILD_STRING             9\n\n3504         186 INPLACE_ADD\n             188 STORE_FAST               3 (warn_string)\n\n3510     >>  190 LOAD_GLOBAL              5 (logger)\n             192 LOAD_METHOD              6 (warning_once)\n             194 LOAD_FAST                3 (warn_string)\n             196 CALL_METHOD              1\n             198 POP_TOP\n             200 LOAD_CONST               1 (None)\n             202 RETURN_VALUE\n\n3490     >>  204 LOAD_CONST               1 (None)\n             206 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 792 \n792           0 LOAD_FAST                0 (___stack0)\n              2 JUMP_ABSOLUTE           74 (to 148)\n              4 LOAD_FAST               12 (output_attentions)\n              6 LOAD_CONST               1 (None)\n              8 IS_OP                    1\n             10 POP_JUMP_IF_FALSE        8 (to 16)\n             12 LOAD_FAST               12 (output_attentions)\n             14 JUMP_FORWARD             3 (to 22)\n        >>   16 LOAD_FAST                1 (self)\n             18 LOAD_ATTR                0 (config)\n             20 LOAD_ATTR                1 (output_attentions)\n        >>   22 STORE_FAST              12 (output_attentions)\n             24 LOAD_FAST               13 (output_hidden_states)\n             26 LOAD_CONST               1 (None)\n             28 IS_OP                    1\n             30 POP_JUMP_IF_FALSE       18 (to 36)\n             32 LOAD_FAST               13 (output_hidden_states)\n             34 JUMP_FORWARD             3 (to 42)\n        >>   36 LOAD_FAST                1 (self)\n             38 LOAD_ATTR                0 (config)\n             40 LOAD_ATTR                2 (output_hidden_states)\n        >>   42 STORE_FAST              13 (output_hidden_states)\n             44 LOAD_FAST               14 (return_dict)\n             46 LOAD_CONST               1 (None)\n             48 IS_OP                    1\n             50 POP_JUMP_IF_FALSE       28 (to 56)\n             52 LOAD_FAST               14 (return_dict)\n             54 JUMP_FORWARD             3 (to 62)\n        >>   56 LOAD_FAST                1 (self)\n             58 LOAD_ATTR                0 (config)\n             60 LOAD_ATTR                3 (use_return_dict)\n        >>   62 STORE_FAST              14 (return_dict)\n             64 LOAD_FAST                1 (self)\n             66 LOAD_ATTR                0 (config)\n             68 LOAD_ATTR                4 (is_decoder)\n             70 POP_JUMP_IF_FALSE       47 (to 94)\n             72 LOAD_FAST               11 (use_cache)\n             74 LOAD_CONST               1 (None)\n             76 IS_OP                    1\n             78 POP_JUMP_IF_FALSE       42 (to 84)\n             80 LOAD_FAST               11 (use_cache)\n             82 JUMP_FORWARD             3 (to 90)\n        >>   84 LOAD_FAST                1 (self)\n             86 LOAD_ATTR                0 (config)\n             88 LOAD_ATTR                5 (use_cache)\n        >>   90 STORE_FAST              11 (use_cache)\n             92 JUMP_FORWARD             2 (to 98)\n        >>   94 LOAD_CONST               2 (False)\n             96 STORE_FAST              11 (use_cache)\n        >>   98 LOAD_FAST                2 (input_ids)\n            100 LOAD_CONST               1 (None)\n            102 IS_OP                    1\n            104 POP_JUMP_IF_FALSE       61 (to 122)\n            106 LOAD_FAST                7 (inputs_embeds)\n            108 LOAD_CONST               1 (None)\n            110 IS_OP                    1\n            112 POP_JUMP_IF_FALSE       61 (to 122)\n            114 LOAD_GLOBAL              6 (ValueError)\n            116 LOAD_CONST               3 ('You cannot specify both input_ids and inputs_embeds at the same time')\n            118 CALL_FUNCTION            1\n            120 RAISE_VARARGS            1\n        >>  122 LOAD_FAST                2 (input_ids)\n            124 LOAD_CONST               1 (None)\n            126 IS_OP                    1\n            128 POP_JUMP_IF_FALSE       76 (to 152)\n            130 LOAD_FAST                2 (input_ids)\n            132 LOAD_ATTR                7 (size)\n            134 CALL_FUNCTION            0\n            136 STORE_FAST              15 (input_shape)\n            138 LOAD_FAST                1 (self)\n            140 LOAD_ATTR                8 (warn_if_padding_and_no_attention_mask)\n            142 LOAD_FAST                2 (input_ids)\n            144 LOAD_FAST                3 (attention_mask)\n            146 CALL_FUNCTION            2\n        >>  148 POP_TOP\n            150 JUMP_FORWARD            17 (to 186)\n\n793     >>  152 LOAD_FAST                7 (inputs_embeds)\n            154 LOAD_CONST               1 (None)\n            156 IS_OP                    1\n            158 POP_JUMP_IF_FALSE       89 (to 178)\n\n794         160 LOAD_FAST                7 (inputs_embeds)\n            162 LOAD_ATTR                7 (size)\n            164 CALL_FUNCTION            0\n            166 LOAD_CONST               1 (None)\n            168 LOAD_CONST               4 (-1)\n            170 BUILD_SLICE              2\n            172 BINARY_SUBSCR\n            174 STORE_FAST              15 (input_shape)\n            176 JUMP_FORWARD             4 (to 186)\n\n796     >>  178 LOAD_GLOBAL              6 (ValueError)\n            180 LOAD_CONST               5 ('You have to specify either input_ids or inputs_embeds')\n            182 CALL_FUNCTION            1\n            184 RAISE_VARARGS            1\n\n798     >>  186 LOAD_FAST               15 (input_shape)\n            188 UNPACK_SEQUENCE          2\n            190 STORE_FAST              16 (batch_size)\n            192 STORE_FAST              17 (seq_length)\n\n799         194 LOAD_FAST                2 (input_ids)\n            196 LOAD_CONST               1 (None)\n            198 IS_OP                    1\n            200 POP_JUMP_IF_FALSE      104 (to 208)\n            202 LOAD_FAST                2 (input_ids)\n            204 LOAD_ATTR                9 (device)\n            206 JUMP_FORWARD             2 (to 212)\n        >>  208 LOAD_FAST                7 (inputs_embeds)\n            210 LOAD_ATTR                9 (device)\n        >>  212 STORE_FAST              18 (device)\n\n802         214 LOAD_FAST               10 (past_key_values)\n            216 LOAD_CONST               1 (None)\n            218 IS_OP                    1\n            220 POP_JUMP_IF_FALSE      120 (to 240)\n            222 LOAD_FAST               10 (past_key_values)\n            224 LOAD_CONST               6 (0)\n            226 BINARY_SUBSCR\n            228 LOAD_CONST               6 (0)\n            230 BINARY_SUBSCR\n            232 LOAD_ATTR               10 (shape)\n            234 LOAD_CONST               7 (2)\n            236 BINARY_SUBSCR\n            238 JUMP_FORWARD             1 (to 242)\n        >>  240 LOAD_CONST               6 (0)\n        >>  242 STORE_FAST              19 (past_key_values_length)\n\n804         244 LOAD_FAST                3 (attention_mask)\n            246 LOAD_CONST               1 (None)\n            248 IS_OP                    0\n            250 POP_JUMP_IF_FALSE      137 (to 274)\n\n805         252 LOAD_GLOBAL             11 (torch)\n            254 LOAD_ATTR               12 (ones)\n            256 LOAD_FAST               16 (batch_size)\n            258 LOAD_FAST               17 (seq_length)\n            260 LOAD_FAST               19 (past_key_values_length)\n            262 BINARY_ADD\n            264 BUILD_TUPLE              2\n            266 LOAD_FAST               18 (device)\n            268 LOAD_CONST               8 (('device',))\n            270 CALL_FUNCTION_KW         2\n            272 STORE_FAST               3 (attention_mask)\n\n807     >>  274 LOAD_FAST                4 (token_type_ids)\n            276 LOAD_CONST               1 (None)\n            278 IS_OP                    0\n            280 POP_JUMP_IF_FALSE      177 (to 354)\n\n808         282 LOAD_GLOBAL             13 (hasattr)\n            284 LOAD_FAST                1 (self)\n            286 LOAD_ATTR               14 (embeddings)\n            288 LOAD_CONST               9 ('token_type_ids')\n            290 CALL_FUNCTION            2\n            292 POP_JUMP_IF_FALSE      168 (to 336)\n\n809         294 LOAD_FAST                1 (self)\n            296 LOAD_ATTR               14 (embeddings)\n            298 LOAD_ATTR               15 (token_type_ids)\n            300 LOAD_CONST               1 (None)\n            302 LOAD_CONST               1 (None)\n            304 BUILD_SLICE              2\n            306 LOAD_CONST               1 (None)\n            308 LOAD_FAST               17 (seq_length)\n            310 BUILD_SLICE              2\n            312 BUILD_TUPLE              2\n            314 BINARY_SUBSCR\n            316 STORE_FAST              20 (buffered_token_type_ids)\n\n810         318 LOAD_FAST               20 (buffered_token_type_ids)\n            320 LOAD_ATTR               16 (expand)\n            322 LOAD_FAST               16 (batch_size)\n            324 LOAD_FAST               17 (seq_length)\n            326 CALL_FUNCTION            2\n            328 STORE_FAST              21 (buffered_token_type_ids_expanded)\n\n811         330 LOAD_FAST               21 (buffered_token_type_ids_expanded)\n            332 STORE_FAST               4 (token_type_ids)\n            334 JUMP_FORWARD             9 (to 354)\n\n813     >>  336 LOAD_GLOBAL             11 (torch)\n            338 LOAD_ATTR               17 (zeros)\n            340 LOAD_FAST               15 (input_shape)\n            342 LOAD_GLOBAL             11 (torch)\n            344 LOAD_ATTR               18 (long)\n            346 LOAD_FAST               18 (device)\n            348 LOAD_CONST              10 (('dtype', 'device'))\n            350 CALL_FUNCTION_KW         3\n            352 STORE_FAST               4 (token_type_ids)\n\n817     >>  354 LOAD_FAST                1 (self)\n            356 LOAD_ATTR               19 (get_extended_attention_mask)\n            358 LOAD_FAST                3 (attention_mask)\n            360 LOAD_FAST               15 (input_shape)\n            362 CALL_FUNCTION            2\n            364 STORE_FAST              22 (extended_attention_mask)\n\n821         366 LOAD_FAST                1 (self)\n            368 LOAD_ATTR                0 (config)\n            370 LOAD_ATTR                4 (is_decoder)\n            372 POP_JUMP_IF_FALSE      219 (to 438)\n            374 LOAD_FAST                8 (encoder_hidden_states)\n            376 LOAD_CONST               1 (None)\n            378 IS_OP                    1\n            380 POP_JUMP_IF_FALSE      219 (to 438)\n\n822         382 LOAD_FAST                8 (encoder_hidden_states)\n            384 LOAD_ATTR                7 (size)\n            386 CALL_FUNCTION            0\n            388 UNPACK_SEQUENCE          3\n            390 STORE_FAST              23 (encoder_batch_size)\n            392 STORE_FAST              24 (encoder_sequence_length)\n            394 STORE_FAST              25 (_)\n\n823         396 LOAD_FAST               23 (encoder_batch_size)\n            398 LOAD_FAST               24 (encoder_sequence_length)\n            400 BUILD_TUPLE              2\n            402 STORE_FAST              26 (encoder_hidden_shape)\n\n824         404 LOAD_FAST                9 (encoder_attention_mask)\n            406 LOAD_CONST               1 (None)\n            408 IS_OP                    0\n            410 POP_JUMP_IF_FALSE      213 (to 426)\n\n825         412 LOAD_GLOBAL             11 (torch)\n            414 LOAD_ATTR               12 (ones)\n            416 LOAD_FAST               26 (encoder_hidden_shape)\n            418 LOAD_FAST               18 (device)\n            420 LOAD_CONST               8 (('device',))\n            422 CALL_FUNCTION_KW         2\n            424 STORE_FAST               9 (encoder_attention_mask)\n\n826     >>  426 LOAD_FAST                1 (self)\n            428 LOAD_ATTR               20 (invert_attention_mask)\n            430 LOAD_FAST                9 (encoder_attention_mask)\n            432 CALL_FUNCTION            1\n            434 STORE_FAST              27 (encoder_extended_attention_mask)\n            436 JUMP_FORWARD             2 (to 442)\n\n828     >>  438 LOAD_CONST               1 (None)\n            440 STORE_FAST              27 (encoder_extended_attention_mask)\n\n835     >>  442 LOAD_FAST                1 (self)\n            444 LOAD_ATTR               21 (get_head_mask)\n            446 LOAD_FAST                6 (head_mask)\n            448 LOAD_FAST                1 (self)\n            450 LOAD_ATTR                0 (config)\n            452 LOAD_ATTR               22 (num_hidden_layers)\n            454 CALL_FUNCTION            2\n            456 STORE_FAST               6 (head_mask)\n\n837         458 LOAD_FAST                1 (self)\n            460 LOAD_ATTR               14 (embeddings)\n\n838         462 LOAD_FAST                2 (input_ids)\n\n839         464 LOAD_FAST                5 (position_ids)\n\n840         466 LOAD_FAST                4 (token_type_ids)\n\n841         468 LOAD_FAST                7 (inputs_embeds)\n\n842         470 LOAD_FAST               19 (past_key_values_length)\n\n837         472 LOAD_CONST              11 (('input_ids', 'position_ids', 'token_type_ids', 'inputs_embeds', 'past_key_values_length'))\n            474 CALL_FUNCTION_KW         5\n            476 STORE_FAST              28 (embedding_output)\n\n844         478 LOAD_FAST                1 (self)\n            480 LOAD_ATTR               23 (encoder)\n\n845         482 LOAD_FAST               28 (embedding_output)\n\n846         484 LOAD_FAST               22 (extended_attention_mask)\n\n847         486 LOAD_FAST                6 (head_mask)\n\n848         488 LOAD_FAST                8 (encoder_hidden_states)\n\n849         490 LOAD_FAST               27 (encoder_extended_attention_mask)\n\n850         492 LOAD_FAST               10 (past_key_values)\n\n851         494 LOAD_FAST               11 (use_cache)\n\n852         496 LOAD_FAST               12 (output_attentions)\n\n853         498 LOAD_FAST               13 (output_hidden_states)\n\n854         500 LOAD_FAST               14 (return_dict)\n\n844         502 LOAD_CONST              12 (('attention_mask', 'head_mask', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'))\n            504 CALL_FUNCTION_KW        10\n            506 STORE_FAST              29 (encoder_outputs)\n\n856         508 LOAD_FAST               29 (encoder_outputs)\n            510 LOAD_CONST               6 (0)\n            512 BINARY_SUBSCR\n            514 STORE_FAST              30 (sequence_output)\n\n857         516 LOAD_FAST                1 (self)\n            518 LOAD_ATTR               24 (pooler)\n            520 LOAD_CONST               1 (None)\n            522 IS_OP                    1\n            524 EXTENDED_ARG             1\n            526 POP_JUMP_IF_FALSE      269 (to 538)\n            528 LOAD_FAST                1 (self)\n            530 LOAD_ATTR               24 (pooler)\n            532 LOAD_FAST               30 (sequence_output)\n            534 CALL_FUNCTION            1\n            536 JUMP_FORWARD             1 (to 540)\n        >>  538 LOAD_CONST               1 (None)\n        >>  540 STORE_FAST              31 (pooled_output)\n\n859         542 LOAD_FAST               14 (return_dict)\n            544 EXTENDED_ARG             1\n            546 POP_JUMP_IF_TRUE       284 (to 568)\n\n860         548 LOAD_FAST               30 (sequence_output)\n            550 LOAD_FAST               31 (pooled_output)\n            552 BUILD_TUPLE              2\n            554 LOAD_FAST               29 (encoder_outputs)\n            556 LOAD_CONST              13 (1)\n            558 LOAD_CONST               1 (None)\n            560 BUILD_SLICE              2\n            562 BINARY_SUBSCR\n            564 BINARY_ADD\n            566 RETURN_VALUE\n\n862     >>  568 LOAD_GLOBAL             25 (BaseModelOutputWithPoolingAndCrossAttentions)\n\n863         570 LOAD_FAST               30 (sequence_output)\n\n864         572 LOAD_FAST               31 (pooled_output)\n\n865         574 LOAD_FAST               29 (encoder_outputs)\n            576 LOAD_ATTR               26 (past_key_values)\n\n866         578 LOAD_FAST               29 (encoder_outputs)\n            580 LOAD_ATTR               27 (hidden_states)\n\n867         582 LOAD_FAST               29 (encoder_outputs)\n            584 LOAD_ATTR               28 (attentions)\n\n868         586 LOAD_FAST               29 (encoder_outputs)\n            588 LOAD_ATTR               29 (cross_attentions)\n\n862         590 LOAD_CONST              14 (('last_hidden_state', 'pooler_output', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'))\n            592 CALL_FUNCTION_KW         6\n            594 RETURN_VALUE\n\n", "ORIGINAL BYTECODE <resume in forward> /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py line 1498 \n1498           0 LOAD_FAST                0 (___stack0)\n               2 JUMP_ABSOLUTE           25 (to 50)\n               4 LOAD_FAST                4 (return_dict)\n               6 LOAD_CONST               1 (None)\n               8 IS_OP                    1\n              10 POP_JUMP_IF_FALSE        8 (to 16)\n              12 LOAD_FAST                4 (return_dict)\n              14 JUMP_FORWARD             3 (to 22)\n         >>   16 LOAD_FAST                1 (self)\n              18 LOAD_ATTR                0 (config)\n              20 LOAD_ATTR                1 (use_return_dict)\n         >>   22 STORE_FAST               4 (return_dict)\n              24 LOAD_FAST                1 (self)\n              26 LOAD_ATTR                2 (roberta)\n              28 LOAD_FAST                5 (input_ids)\n              30 LOAD_FAST                6 (attention_mask)\n              32 LOAD_FAST                7 (token_type_ids)\n              34 LOAD_FAST                8 (position_ids)\n              36 LOAD_FAST                9 (head_mask)\n              38 LOAD_FAST               10 (inputs_embeds)\n              40 LOAD_FAST               11 (output_attentions)\n              42 LOAD_FAST               12 (output_hidden_states)\n              44 LOAD_FAST                4 (return_dict)\n              46 LOAD_CONST               2 (('attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'output_attentions', 'output_hidden_states', 'return_dict'))\n              48 CALL_FUNCTION_KW         9\n         >>   50 STORE_FAST              13 (outputs)\n\n1510          52 LOAD_FAST               13 (outputs)\n              54 LOAD_CONST               3 (0)\n              56 BINARY_SUBSCR\n              58 STORE_FAST              14 (sequence_output)\n\n1512          60 LOAD_FAST                1 (self)\n              62 LOAD_ATTR                3 (qa_outputs)\n              64 LOAD_FAST               14 (sequence_output)\n              66 CALL_FUNCTION            1\n              68 STORE_FAST              15 (logits)\n\n1513          70 LOAD_FAST               15 (logits)\n              72 LOAD_ATTR                4 (split)\n              74 LOAD_CONST               4 (1)\n              76 LOAD_CONST               5 (-1)\n              78 LOAD_CONST               6 (('dim',))\n              80 CALL_FUNCTION_KW         2\n              82 UNPACK_SEQUENCE          2\n              84 STORE_FAST              16 (start_logits)\n              86 STORE_FAST              17 (end_logits)\n\n1514          88 LOAD_FAST               16 (start_logits)\n              90 LOAD_ATTR                5 (squeeze)\n              92 LOAD_CONST               5 (-1)\n              94 CALL_FUNCTION            1\n              96 LOAD_ATTR                6 (contiguous)\n              98 CALL_FUNCTION            0\n             100 STORE_FAST              16 (start_logits)\n\n1515         102 LOAD_FAST               17 (end_logits)\n             104 LOAD_ATTR                5 (squeeze)\n             106 LOAD_CONST               5 (-1)\n             108 CALL_FUNCTION            1\n             110 LOAD_ATTR                6 (contiguous)\n             112 CALL_FUNCTION            0\n             114 STORE_FAST              17 (end_logits)\n\n1517         116 LOAD_CONST               1 (None)\n             118 STORE_FAST              18 (total_loss)\n\n1518         120 LOAD_FAST                2 (start_positions)\n             122 LOAD_CONST               1 (None)\n             124 IS_OP                    1\n             126 POP_JUMP_IF_FALSE      132 (to 264)\n             128 LOAD_FAST                3 (end_positions)\n             130 LOAD_CONST               1 (None)\n             132 IS_OP                    1\n             134 POP_JUMP_IF_FALSE      132 (to 264)\n\n1520         136 LOAD_GLOBAL              7 (len)\n             138 LOAD_FAST                2 (start_positions)\n             140 LOAD_ATTR                8 (size)\n             142 CALL_FUNCTION            0\n             144 CALL_FUNCTION            1\n             146 LOAD_CONST               4 (1)\n             148 COMPARE_OP               4 (>)\n             150 POP_JUMP_IF_FALSE       81 (to 162)\n\n1521         152 LOAD_FAST                2 (start_positions)\n             154 LOAD_ATTR                5 (squeeze)\n             156 LOAD_CONST               5 (-1)\n             158 CALL_FUNCTION            1\n             160 STORE_FAST               2 (start_positions)\n\n1522     >>  162 LOAD_GLOBAL              7 (len)\n             164 LOAD_FAST                3 (end_positions)\n             166 LOAD_ATTR                8 (size)\n             168 CALL_FUNCTION            0\n             170 CALL_FUNCTION            1\n             172 LOAD_CONST               4 (1)\n             174 COMPARE_OP               4 (>)\n             176 POP_JUMP_IF_FALSE       94 (to 188)\n\n1523         178 LOAD_FAST                3 (end_positions)\n             180 LOAD_ATTR                5 (squeeze)\n             182 LOAD_CONST               5 (-1)\n             184 CALL_FUNCTION            1\n             186 STORE_FAST               3 (end_positions)\n\n1525     >>  188 LOAD_FAST               16 (start_logits)\n             190 LOAD_ATTR                8 (size)\n             192 LOAD_CONST               4 (1)\n             194 CALL_FUNCTION            1\n             196 STORE_FAST              19 (ignored_index)\n\n1526         198 LOAD_FAST                2 (start_positions)\n             200 LOAD_ATTR                9 (clamp)\n             202 LOAD_CONST               3 (0)\n             204 LOAD_FAST               19 (ignored_index)\n             206 CALL_FUNCTION            2\n             208 STORE_FAST               2 (start_positions)\n\n1527         210 LOAD_FAST                3 (end_positions)\n             212 LOAD_ATTR                9 (clamp)\n             214 LOAD_CONST               3 (0)\n             216 LOAD_FAST               19 (ignored_index)\n             218 CALL_FUNCTION            2\n             220 STORE_FAST               3 (end_positions)\n\n1529         222 LOAD_GLOBAL             10 (CrossEntropyLoss)\n             224 LOAD_FAST               19 (ignored_index)\n             226 LOAD_CONST               7 (('ignore_index',))\n             228 CALL_FUNCTION_KW         1\n             230 STORE_FAST              20 (loss_fct)\n\n1530         232 LOAD_FAST               20 (loss_fct)\n             234 LOAD_FAST               16 (start_logits)\n             236 LOAD_FAST                2 (start_positions)\n             238 CALL_FUNCTION            2\n             240 STORE_FAST              21 (start_loss)\n\n1531         242 LOAD_FAST               20 (loss_fct)\n             244 LOAD_FAST               17 (end_logits)\n             246 LOAD_FAST                3 (end_positions)\n             248 CALL_FUNCTION            2\n             250 STORE_FAST              22 (end_loss)\n\n1532         252 LOAD_FAST               21 (start_loss)\n             254 LOAD_FAST               22 (end_loss)\n             256 BINARY_ADD\n             258 LOAD_CONST               8 (2)\n             260 BINARY_TRUE_DIVIDE\n             262 STORE_FAST              18 (total_loss)\n\n1534     >>  264 LOAD_FAST                4 (return_dict)\n             266 POP_JUMP_IF_TRUE       155 (to 310)\n\n1535         268 LOAD_FAST               16 (start_logits)\n             270 LOAD_FAST               17 (end_logits)\n             272 BUILD_TUPLE              2\n             274 LOAD_FAST               13 (outputs)\n             276 LOAD_CONST               8 (2)\n             278 LOAD_CONST               1 (None)\n             280 BUILD_SLICE              2\n             282 BINARY_SUBSCR\n             284 BINARY_ADD\n             286 STORE_FAST              23 (output)\n\n1536         288 LOAD_FAST               18 (total_loss)\n             290 LOAD_CONST               1 (None)\n             292 IS_OP                    1\n             294 POP_JUMP_IF_FALSE      153 (to 306)\n             296 LOAD_FAST               18 (total_loss)\n             298 BUILD_TUPLE              1\n             300 LOAD_FAST               23 (output)\n             302 BINARY_ADD\n             304 RETURN_VALUE\n         >>  306 LOAD_FAST               23 (output)\n             308 RETURN_VALUE\n\n1538     >>  310 LOAD_GLOBAL             11 (QuestionAnsweringModelOutput)\n\n1539         312 LOAD_FAST               18 (total_loss)\n\n1540         314 LOAD_FAST               16 (start_logits)\n\n1541         316 LOAD_FAST               17 (end_logits)\n\n1542         318 LOAD_FAST               13 (outputs)\n             320 LOAD_ATTR               12 (hidden_states)\n\n1543         322 LOAD_FAST               13 (outputs)\n             324 LOAD_ATTR               13 (attentions)\n\n1538         326 LOAD_CONST               9 (('loss', 'start_logits', 'end_logits', 'hidden_states', 'attentions'))\n             328 CALL_FUNCTION_KW         5\n             330 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n", "ORIGINAL BYTECODE forward_pass /workspace/youkaichao/code/pytorch/./benchmarks/dynamo/huggingface.py line 549 \n550           0 LOAD_FAST                0 (self)\n              2 LOAD_METHOD              0 (autocast)\n              4 CALL_METHOD              0\n              6 SETUP_WITH              15 (to 38)\n              8 POP_TOP\n\n551          10 LOAD_FAST                1 (mod)\n             12 LOAD_CONST               1 (())\n             14 BUILD_MAP                0\n             16 LOAD_FAST                2 (inputs)\n             18 DICT_MERGE               1\n             20 CALL_FUNCTION_EX         1\n\n550          22 POP_BLOCK\n             24 ROT_TWO\n             26 LOAD_CONST               0 (None)\n             28 DUP_TOP\n             30 DUP_TOP\n             32 CALL_FUNCTION            3\n             34 POP_TOP\n             36 RETURN_VALUE\n        >>   38 WITH_EXCEPT_START\n             40 POP_JUMP_IF_TRUE        22 (to 44)\n             42 RERAISE                  1\n        >>   44 POP_TOP\n             46 POP_TOP\n             48 POP_TOP\n             50 POP_EXCEPT\n             52 POP_TOP\n             54 LOAD_CONST               0 (None)\n             56 RETURN_VALUE\n\n"]